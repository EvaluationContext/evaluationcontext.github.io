{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"about/","title":"About","text":""},{"location":"about/#about","title":"About","text":"<p>Jake Duddy is a Business Intelligence specialist with a deep focus on Power BI and Microsoft Fabric.</p> <ul> <li> <p> Blog</p> <p>Shares experiments, experiences and insights on Power BI and Microsoft Fabric</p> <p> Read Blog</p> </li> <li> <p> Speaker</p> <p>Presents at Fabric and Power BI User Groups and conferences</p> <p> View Presentations</p> </li> <li> <p> Microsoft MVP</p> <p>Recognized as a Microsoft Data Platform MVP - Power BI</p> <p> MVP Profile</p> </li> <li> <p> Community Super User</p> <p>Helps out on the Fabric forums as a Fabric Community Super User</p> <p> Community Profile</p> </li> <li> <p> User Group Organizer</p> <p>Co-organizer of the Devon and Cornwall Microsoft Power BI User Group</p> <p> Join Meetup</p> </li> </ul>"},{"location":"presentations/","title":"Presentations","text":""},{"location":"presentations/#presentations","title":"Presentations","text":""},{"location":"presentations/#2025","title":"2025","text":"<p> Sep Playing With Translytical Task Flows  Devon and Cornwall User Group</p> <p> Aug Visualize Your Connections: A Practical Guide to Power BI Graphs  Power BI and Fabric Manchester User Group</p> <p> Jul Unlocking New Possibilities: Enhancing Power BI with SVGs  Birmingham Power BI Meetup Group</p> <p> Jul Visualize Your Connections: A Practical Guide to Power BI Graphs  Bristol User Group</p> <p> Jun Power Hour  SQLBits</p>"},{"location":"presentations/#2024","title":"2024","text":"<p> Aug PBIP and Fabric API Deployment  Birmingham Power BI Meetup Group</p> <p> Jul Git Branching Strategies and Power BI  Devon and Cornwall User Group</p>"},{"location":"privacy/","title":"Privacy Policy","text":""},{"location":"privacy/#privacy-policy","title":"Privacy Policy","text":"<p>Last Updated: October 19, 2025</p>"},{"location":"privacy/#introduction","title":"Introduction","text":"<p>This privacy policy explains how Evaluation Context (\"we\", \"us\", or \"our\") collects, uses, and protects your information when you visit our website.</p>"},{"location":"privacy/#information-we-collect","title":"Information We Collect","text":""},{"location":"privacy/#analytics-data","title":"Analytics Data","text":"<p>We use Google Analytics to understand how visitors interact with our site. This includes:</p> <ul> <li>Page views: Which pages you visit and how long you spend on them</li> <li>Scroll depth: How far you scroll down pages</li> <li>Referral sources: How you arrived at our site</li> <li>Device information: Browser type, operating system, and screen resolution</li> <li>Geographic location: Approximate location based on IP address (city/country level)</li> </ul> <p>This data is collected through cookies and is anonymized. We do not collect personally identifiable information unless you choose to share it.</p>"},{"location":"privacy/#cookies","title":"Cookies","text":"<p>We use cookies for:</p> <ul> <li>Analytics: Google Analytics cookies to track site usage</li> <li>Preferences: Your cookie consent choices</li> </ul> <p>You can manage your cookie preferences at any time using the cookie settings link in the footer.</p>"},{"location":"privacy/#how-we-use-your-information","title":"How We Use Your Information","text":"<p>We use the collected data to:</p> <ul> <li>Understand which content is most valuable to visitors</li> <li>Improve site navigation and user experience</li> <li>Identify technical issues</li> <li>Analyze traffic patterns and trends</li> </ul>"},{"location":"privacy/#third-party-services","title":"Third-Party Services","text":""},{"location":"privacy/#google-analytics","title":"Google Analytics","text":"<p>Our site uses Google Analytics, a web analytics service provided by Google LLC. Google Analytics uses cookies to help us analyze how users interact with our site.</p> <p>The information generated by the cookie about your use of the website will be transmitted to and stored by Google on servers in the United States. Google will use this information to evaluate your use of the website, compile reports on website activity, and provide other services relating to website activity and internet usage.</p> <p>For more information about Google Analytics, please visit: Google's Privacy Policy</p>"},{"location":"privacy/#external-links","title":"External Links","text":"<p>Our site contains links to external websites (GitHub, YouTube, LinkedIn, Microsoft, etc.). We are not responsible for the privacy practices of these external sites.</p>"},{"location":"privacy/#data-retention","title":"Data Retention","text":"<p>Analytics data is retained according to Google Analytics' default retention settings (typically 26 months). You can request deletion of your data by contacting us.</p>"},{"location":"privacy/#your-rights","title":"Your Rights","text":"<p>You have the right to:</p> <ul> <li>Access: Request information about what data we collect</li> <li>Opt-out: Refuse cookies (though this may limit site functionality)</li> <li>Delete: Request deletion of your analytics data</li> <li>Object: Object to processing of your data</li> </ul> <p>To exercise these rights, please contact us through our social media profiles linked in the footer.</p>"},{"location":"privacy/#childrens-privacy","title":"Children's Privacy","text":"<p>Our website is not directed at children under 13, and we do not knowingly collect information from children.</p>"},{"location":"privacy/#changes-to-this-policy","title":"Changes to This Policy","text":"<p>We may update this privacy policy from time to time. Changes will be posted on this page with an updated revision date.</p>"},{"location":"privacy/#contact","title":"Contact","text":"<p>If you have questions about this privacy policy, you can reach us through:</p> <ul> <li>LinkedIn: Jake Duddy</li> <li>GitHub: EvaluationContext</li> <li>Microsoft Fabric Community: Profile</li> </ul>"},{"location":"privacy/#cookie-settings","title":"Cookie Settings","text":"<p>You can change your cookie preferences at any time: Change cookie settings</p>"},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#daxlib-libraries","title":"DaxLib Libraries","text":"<p>daxlib.org is a website designed to distribute libraries of DAX user-defined functions (UDF). Similar to NuGet, DAX Lib provides a centralized platform where users can discover, share, and download reusable DAX function libraries to enhance their Fabric, Power BI, and Analysis Services semantic models.</p> <ul> <li> <p> EvaluationContext.Colour</p> <p>A DAX UDF library for manipulation of hex colours for Power BI</p> <p></p> <p> Docs  Docs  Package</p> </li> <li> <p> DaxLib.SVG</p> <p>A DAX UDF library designed to make creating SVG visuals in Power BI easier</p> <p></p> <p> Docs  Docs  Package  Package</p> </li> </ul>"},{"location":"posts/Syntax-Highlight-DAX/","title":"DAX Highlighting in Github Pages","text":"<p>This blog is hosted on Github Pages. Github pages uses Jekyll to create static webpages from Markdown. I want to able to blog on Power BI context, and it would be nice to have Syntax Highlighting for DAX code, but this is not natively supported. This article describes the development and implementation to make this possible.</p>"},{"location":"posts/Syntax-Highlight-DAX/#syntax-highlighting","title":"Syntax Highlighting","text":"<p>In markdown, Fenced Code Blocks are created by placing triple backticks <code>```</code>  before and after a code snippets, causing text to render inside a box.</p> <pre><code>SELECT *\nFROM tbl\nWHERE foo = 'bar'\n</code></pre> <p>If you add a language identifier after the opening backticks <code>```sql```</code> you can apply Syntax Highlighting.</p> <pre><code>SELECT *\nFROM tbl\nWHERE foo = 'bar'\n</code></pre>"},{"location":"posts/Syntax-Highlight-DAX/#tokenization","title":"Tokenization","text":"<p>When we think of a programming language we have various different tokens (keywords, operators, data types, comments etc). In order to apply the correct styling, we need to parse the raw text to list of tokens. The patterns of the various tokens is described by a Lexer. For example a single line comment in SQL starts with two dashes <code>--</code> followed by any length of text.</p> <pre><code>-- a single-line SQL comment\n</code></pre> <p>This pattern can be described via a regex expression <code>%r/--.*/</code>. A theme can then be applied to colour and style tokens according to their token type.</p> <p></p>"},{"location":"posts/Syntax-Highlight-DAX/#dax-lexer","title":"DAX Lexer","text":"<p>Jekyll's default Syntax Highlighter is Rouge. Rouge doesn't have a DAX lexer, so we'll have to develop one. Rouge provides a Lexer Development Guide that we can follow. </p>"},{"location":"posts/Syntax-Highlight-DAX/#rouge-development-environment","title":"Rouge Development Environment","text":"<p>Rouge is a Ruby application, and therefore development and testing of the lexer requires a Linux Development Environment running Ruby with the required gems. To keep the development environment self-contained, Rouge suggests using a Docker Development Environment. Rouge offers a automated tested suite (rake) and a visual testing website (rackup), to test and validate your lexer.</p> <p>Warning</p> <p>You can run Ruby locally on WSL but I would advise against it. I ran into issues with the mapping of folder/file permission between Windows and Linux when installing gems.</p> <pre><code>#Install gemfile dependencies to /tmp/vendor with bundler\ndocker run -t -v $PWD:/app -v /tmp/vendor:/vendor -w /app -e BUNDLE_PATH=/vendor ruby bundle\n</code></pre> <pre><code>#Run test suite\ndocker run -t -v $PWD:/app -v /tmp/vendor:/vendor -w /app -e BUNDLE_PATH=/vendor ruby bundle exec rake\n</code></pre> <pre><code>#Run a web app on localhost with highlighted code snippets http://localhost:9292\ndocker run -t -v $PWD:/app -v /tmp/vendor:/vendor -w /app -e BUNDLE_PATH=/vendor -p 9292:9292 ruby bundle exec rackup --host 0.0.0.0\n</code></pre>"},{"location":"posts/Syntax-Highlight-DAX/#rouge-dax-lexer-development","title":"Rouge DAX Lexer Development","text":"<p>Once we have the development environment setup we can start developing our DAX lexer. There are few existing DAX lexer in other frameworks that I used for inspiration (Tabular Editor, SQLBI, and Microsoft learn), resulting in this Rouge DAX Lexer. With the addition some code snippets (Demo &amp; Sample) we are ready to perform a visual check of the lexer.</p> <p>Tip</p> <p>If you have the local website running on rackup. Any changes saved to your files are reflected on the web page, without having to restart the server; just refresh your browser.</p> <p>Following validation via visual inspection we can perform automated testing with rake. Once this passes we are ready to push to our fork and submit a Pull Request (PR) to get our files added to the main Rouge repo. </p> <p>At the time of publishing, this PR is still under review. So in the meantime I turned to highlight.js which allows you selfhost a lexer, but you need to jump through a few extra hoops. </p>"},{"location":"posts/Syntax-Highlight-DAX/#highlightjs-development-environment","title":"Highlight.js Development Environment","text":"<p>Highlight.js provides some guides on contributing and how to setup a Docker Development Environment. Unlike Rouge, where you mount a directory and save files to your local machine, highlight.js creates a self contained build.</p> <p>Info</p> <p>An update of the base docker image from node:12-slim to node:21-bullseye-slim was required to get container to build successfully</p> <pre><code>#### Create Docker build\ndocker build -t highlight-js .\n</code></pre> <pre><code># Run a web app on localhost with highlighted code snippets http://127.0.0.1/tools/developer.html\ndocker run -d --name highlight-js --volume $PWD/src:/var/www/html/src --rm -p 80:80 highlight-js\n</code></pre> <pre><code># Rebuilds based on local changes\ndocker exec highlight-js node tools/build.js -n dax\n</code></pre>"},{"location":"posts/Syntax-Highlight-DAX/#highlightjs-dax-lexer-development","title":"Highlight.js DAX Lexer Development","text":"<p>To translate our lexer we can reference the Language Definition Guide.  Highlight.js gives us a great development environment. We provide sample code, and the webpage show how our text is being tokenized and styled.</p> <p></p> <p>Once we have finished developing our Lexer, there are some extra steps required to Contribute a Language. I decided to skip this process at this time, as ideally I want to use Rouge in the longer term, but I might come back to this.</p> <p>In order to self host highlight.js:</p> <ul> <li>Update _config.yml to turn off Rouge</li> </ul> <pre><code>kramdown:\n  syntax_highlighter: None\n  syntax_highlighter_opts:\n    disable : true\n</code></pre> <ul> <li> <p>Downloaded highlight.js with languages of interest</p> </li> <li> <p>Save the files in assets/js/highlight</p> </li> <li> <p>Update _includes/head.html to point to highlight.js and theme</p> </li> </ul> <pre><code>&lt;link rel=\"stylesheet\" href=\"/assets/js/highlight/styles/base16/material.css\"&gt;\n&lt;script src=\"/assets/js/highlight/highlight.js\"&gt;&lt;/script&gt;\n&lt;script&gt;hljs.highlightAll();&lt;/script&gt;\n</code></pre> <ul> <li>To host a custom language, copy the lexer code into the highlight.js file</li> </ul> <p>And NOW we are finally able to apply DAX highlighting with <code>dax</code></p> <pre><code>DEFINE MEASURE 'foo'[measure1] = \n    VAR variable = 4.5\n    RETURN\n    SUMX( VALUES( 'bar'[col] ), 'bar'[col] + variable ) \n\nEVALUATE\n    ADDCOLUMNS(\n        VALUES( 'Date'[Year Month] )\n        ,\"@sumBar\"\n        ,CALCULATE(\n            [measure1]\n            ,REMOVEFILTERS()\n            ,USERELATIONSHIP( 'Date'[Date], 'bar'[Order Date] )\n        )\n    )\n</code></pre> <p>As an end note, in addition to the more statically defined token, DAX also has the concept of variables. To highlight these would require Semantic Highlighting, which is beyond the scope of this project.</p>"},{"location":"posts/Trash-Compactor/","title":"Considering Cardinality of Column Combinations in Junk Dimension Design","text":"<p>I have been musing on the process of designing of Junk Dimensions for Power BI Semantic Models that employ User Defined Aggregations.</p>"},{"location":"posts/Trash-Compactor/#junk-dimensions","title":"Junk Dimensions","text":"<p>Kimball - Junk Dimensions</p> <p>Transactional business processes typically produce a number of miscellaneous, low-cardinality \ufb02ags and indicators. Rather than making separate dimensions for each \ufb02ag and attribute, you can create a single junk dimension combining them together. -- Kimball </p>"},{"location":"posts/Trash-Compactor/#why-would-we-want-a-junk-dimension","title":"Why would we want a Junk Dimension?","text":"<ul> <li> <p>Simpler DAX: Easier to remove filters. Avoids hard to \"incorrect\" results due to Auto-exist; well covered by SQLBI.</p> </li> <li> <p>Simplify User Defined Aggregations: With a Junk Dimension, a single field can be used to define the criteria for a cache hit. Otherwise you would potentially need a large number of single field dimension, defined by relationship. You can keep the fields in the fact table and use GROUP BY, but cross-filtering between other fact tables is not possible.</p> </li> </ul>"},{"location":"posts/Trash-Compactor/#qualities-of-a-good-junk-dimension","title":"Qualities of a Good Junk Dimension?","text":"<p>Kimball's definition makes a Junk Dimension a dumping ground, consisting of a jumble of fields. If we were to improve a Junk Dimension, what qualities would we desire?</p> <ol> <li>Minimal number of fields</li> <li>Low cardinality fields</li> <li>Fields are all well related as to avoid the full cartesian product</li> </ol> <p>These point all relate to minimize the size of the relationship, and avoiding sub-optimal sort order, to maximize run-length encoding and ensuring good query performance.</p>"},{"location":"posts/Trash-Compactor/#methodology-for-evaluating-junk-dimension","title":"Methodology For Evaluating Junk Dimension","text":"<p>To address the above we can compare each field to each other field and calculate the distinct count of results. These are then plotted on a heatmap. Those combinations with high cardinality will be highlighted and we can make informed architectural decision on whether fields are included in the dimension. This is performed by the script below. </p> <p>Warning</p> <p>This can potentially be a rather expensive operation. The cost grows the more fields considered. Number of rows has less effect.</p> VisualCode <p></p> <pre><code>import itertools as it\nfrom pyspark.sql import functions as F\nimport pandas as pd\nimport seaborn as sns\n\n# filter to potential junk dimension, excluding primary key\ndf = spark.sql(\"SELECT * FROM foo\")\n\n# Iterate all combinations of columns, calculate cardinality combinations\ncombo_dict={}\nfor combo in list(it.combinations(df.columns, 2)): \n    combo_dict['__'.join(perm)] = df.select(list(combo)).distinct().count()\n\n# Pivot data\npd_df = pd.DataFrame.from_dict(combo_dict.items())\npd_df.rename(columns={0: 'column_combo', 1: 'cardinality'}, inplace=True)\npd_df[['X', 'Y']] = pd_df['column_combo'].str.split('__', expand=True)\n\n# Generate Heatmap\ndata = pd_df.pivot(index = \"Y\", columns='X',values='cardinality')\nsns.set(font_scale=0.5)\nsns.heatmap(data, cmap=\"viridis\", square=True)\n</code></pre> <p>Combination of fields that result in high cardinality , are coloured Yellow. You should think very carefully whether a field or fields that result in a large cardinality should exist in a candidate Junk Dimension. If the junk dimension is small in relation to the fact table, probably not. But if the junk dimension is large, and there one or more highlighted fields, you should carefully consider whether these should remain in the fact table or group well together in a secondary junk table. </p> <p>Every Semantic Model is different, and careful thought and testing are required when making design decisions. This approach can be another tool in you arsenal during this process.</p>"},{"location":"posts/Window-Function/","title":"TOPN vs INDEX","text":"<p>Now I have DAX Syntax Highlighting working on my blog I wanted to revisit some exploration I did with window function when they first came out. This was based on some DAX written by Phil Seamark that Counts numbers of last known state. The measure counts, for each day, the most recent State for each TestID.</p>"},{"location":"posts/Window-Function/#semantic-model","title":"Semantic Model","text":""},{"location":"posts/Window-Function/#measure","title":"Measure","text":"<p>Phil's blog pre-dated window functions so I thought it would be interesting to refactor this measure, swapping TOPN with INDEX and observe the differences.</p> TOPNINDEX <pre><code>// Using TOPN\nVAR currentState = SELECTEDVALUE(States[State])\nVAR currentDate = SELECTEDVALUE('Calendar'[Date])+1\nRETURN\n    COUNTROWS(\n        FILTER(\n            ALL( data[TestID] ),\n            SELECTCOLUMNS(\n                TOPN(\n                    1,\n                    CALCULATETABLE( \n                        FILTER( \n                            'data', \n                            [DateTime] &lt; currentDate\n                        )\n                        ,REMOVEFILTERS('Calendar'[Date])\n                        ,REMOVEFILTERS('States'[State])\n                        ),\n                    'data'[DateTime], \n                    DESC\n                ),\n                \"Last Value\", [State]\n                )\n            = currentState\n            )\n        )       \n</code></pre> <pre><code>// Using INDEX\nVAR currentState = SELECTEDVALUE(States[State])\nVAR currentDate = SELECTEDVALUE('Calendar'[Date])+1\nRETURN\n    COUNTROWS(\n        FILTER(\n            ALL( data[TestID] ),\n            SELECTCOLUMNS(\n                INDEX( \n                    1, \n                    CALCULATETABLE(\n                        FILTER(\n                            data, \n                            data[DateTime] &lt; currentDate\n                        )\n                        ,REMOVEFILTERS( 'Calendar'[Date] )\n                        ,REMOVEFILTERS( 'States'[State] )\n                    )\n                    ,ORDERBY( data[DateTime], DESC)\n                    , \n                    ,MATCHBY( data[DateTime] )\n                ),\n                \"Last Value\", [State]\n                )\n            = currentState\n            )\n        )\n</code></pre>"},{"location":"posts/Window-Function/#measure-comparison","title":"Measure Comparison","text":"<p>I ran both measures in DAX studio, with a cleared cache and Server Timing turned on.</p> <pre><code>EVALUATE\nSUMMARIZECOLUMNS(\n    'Calendar'[Date],\n    'States'[State],\n    \"Using_TOPN\", [Using TopN]\n    \"Using_INDEX\", [Using INDEX]\n)\n</code></pre> TOPN PerformanceINDEX Performance <p></p> <p></p>"},{"location":"posts/Window-Function/#comparison","title":"Comparison","text":"<p>Both of these measures create the same Storage Engine (SE) queries.</p> <pre><code>-- 1\nSELECT 'data'[TestID] FROM 'data';\n\n-- 2\nSELECT 'States'[State] FROM 'States';\n\n-- 3\nSELECT 'Calendar'[Date] FROM 'Calendar'\n\n-- 4 \nSELECT\n    'data'[RowNumber],\n    'data'[DateTime],\n    'data'[TestID],\n    'data'[State]\nFROM 'data';\n</code></pre> <p>The dataset is very small so we can ignore the actual performance timings. To test the difference in performance I extended the data to 20,000 rows.</p> TOPN 20,000 Rows PerformanceINDEX 20,000 Rows Performance <p></p> <p></p> <p>TOPN quickly performs it's SE queries, followed by a substantial period of Formula Engine (FE). Interestingly INDEX takes twice the time to execute. </p> <p>If we look at the Query Plans, the Logical Query Plans are almost identical. But on the Physical Query Plans we see some differences. Both queries end up at the same point, at a CrossApply EqualTo <code>'(Calendar'[Date])</code>, <code>('States'[State])</code>, <code>('data'[TestID])</code> on <code>('Calendar'[Date], 'States'[State], 'data'[TestID])</code>, where the latter is the most recent State of the given TestID on a given Date. The main difference is INDEX joins <code>('Calendar'[Date], 'data'[TestID], 'data'[State])</code> with the ordered list of <code>('Calendar'[Date], 'data'[TestID], 'data'[RowNumber-2662979B-1795-4F74-8F37-6A1BA8059B61], 'data'[DateTime], 'data'[State])</code> to determine n<sup>th</sup> row. </p>"},{"location":"posts/Window-Function/#conclusion","title":"Conclusion","text":"<p>While the semantics of TOPN and INDEX measures are similar, the underlying algorithm and therefore query plans differ, resulting in differences in query performance. If you want to return the n<sup>th</sup> item, INDEX can be a good solution. TOPN only works out for the first or last item. Another option would be to use RANK/RANX, but the measure might be less readable, and the performance would need to be quantified. When trying to develop or optimize a measure you should try to experiment with a few variations to check the characteristics of each before landing on a final design.</p>"},{"location":"posts/IslandsAndGaps/","title":"Solving the Islands Problem in DAX","text":"<p>A while ago the SQLBI guys release a blog providing a solution for Displaying a list of selected months. The input is a selection of filter month, and the output list these months, with contiguous sequences grouped together. This was done by looking at leading and lagging values to detect edges of continuous sequences. This problem falls into the classic Island Problem, and there is another nice way to solve this.</p>"},{"location":"posts/IslandsAndGaps/#simple-solution","title":"Simple Solution","text":"<p>The Island problem states that we want to group ranges of continuous sequences. There is also a similar but opposite Gaps problem, which detects missing values in a sequence. To solve the Island problem we can add a extra column with sequential integers, which increases at the same cadence. </p> <p>Sequence considerations</p> <p>If we have daily the sequential sequence would need to increase by 1 for each row. If we have weekly data the sequence would need to increase by 7. </p> <p>We can calculate the difference between the two sequences. If there is no gap then the difference will be constant. If there is a gap the difference will change, and denoting a new island. This is demonstrated with this example.</p> CodeResult <pre><code>DEFINE \nVAR DateFilter = \n    TREATAS(\n        {\n            DATE(2021, 11, 01),\n            DATE(2022, 11, 02),\n            DATE(2022, 11, 03),\n            DATE(2022, 11, 06),\n            DATE(2022, 11, 07),\n            DATE(2022, 11, 08),\n            DATE(2022, 11, 11),\n            DATE(2022, 11, 12)  \n        },\n        'Dates'[Date]\n    )\n\nEVALUATE\nCALCULATETABLE(\n    ADDCOLUMNS(\n        VALUES( Dates[Date] )\n        ,\"@RowNumber\"\n            , RANK( DENSE, VALUES( Dates[Date] ), ORDERBY( Dates[Date], ASC ) )\n        ,\"@Islands\"\n            ,DATEDIFF(\n                RANK( DENSE, VALUES( Dates[Date] ), ORDERBY( Dates[Date], ASC ) )\n                ,Dates[Date]\n                ,DAY \n            )\n    )\n    ,DateFilter\n)\n</code></pre> Date <code>@RowNumber</code> <code>@Islands</code> 02/11/2022 00:00:00 1 44866 03/11/2022 00:00:00 2 44866 06/11/2022 00:00:00 3 44868 07/11/2022 00:00:00 4 44868 08/11/2022 00:00:00 5 44868 11/11/2022 00:00:00 6 44870 12/11/2022 00:00:00 7 44870 <p>As you can see there are 3 separate continuous sequences, and each group gets a distinct <code>[@Island]</code> value. </p> <p>We can use this as a base for the final solution. </p> VisualCode <p></p> <pre><code>// Islands\nVAR DataSequence = \n    ADDCOLUMNS(\n        VALUES( Dates[Date] )\n        , \"@Islands\"\n        ,DATEDIFF(\n            RANK( DENSE, VALUES( Dates[Date] ), ORDERBY( Dates[Date], ASC ) )\n            ,Dates[Date]\n            ,DAY \n        )\n    )\nVAR Islands =   \n    SUMMARIZE(\n        DataSequence\n        ,[@Islands]\n        ,\"IslandString\"\n            ,VAR MinDate = FORMAT( MIN( Dates[Date] ), \"mmm-yy\" )\n            VAR MaxDate = FORMAT( MAX( Dates[Date] ), \"mmm-yy\" )\n            RETURN\n            IF( MinDate &lt;&gt; MaxDate, MinDate &amp; \" - \" &amp; MaxDate, MaxDate )\n    )\nVAR result = CONCATENATEX ( islands, [IslandString], \" | \" )\nRETURN\nresult\n</code></pre>"},{"location":"posts/IslandsAndGaps/#extended-solution","title":"Extended Solution","text":"<p>This works well for a given granularity, but what about if we want to show a mixture of selected days, months and years. We can extend our solution as follows.</p> VisualCode <p></p> <pre><code>// Island Extended\nIslands = \nVAR DataSequence = \n    ADDCOLUMNS(\n        SUMMARIZE(\n            Dates\n            ,Dates[Year]\n            ,Dates[Month]\n            ,Dates[Date]\n        )\n        , \"@Islands\"\n        ,DATEDIFF(\n            RANK( DENSE, VALUES( Dates[Date] ), ORDERBY( Dates[Date], ASC ) )\n            ,Dates[Date]\n            ,DAY \n        )\n    )\nVAR Islands =   \n    SUMMARIZE(\n        DataSequence\n        ,[@Islands]\n        ,\"IslandString\"\n            ,VAR MinDate =  MIN ( Dates[Date] )\n            VAR MaxDate =   MAX ( Dates[Date] )\n            VAR MinYear =   CALCULATE( MIN( Dates[Date] ), ALLEXCEPT( Dates, Dates[Year] ))\n            VAR MaxYear =   CALCULATE( MAX( Dates[Date] ), ALLEXCEPT( Dates, Dates[Year] ))\n            VAR MinMonth =  CALCULATE( MIN( Dates[Date] ), ALLEXCEPT( Dates, Dates[Month] ))\n            VAR MaxMonth =  CALCULATE( MAX( Dates[Date] ), ALLEXCEPT( Dates, Dates[Month] ))\n            VAR GranularityCheckStart = \n            SWITCH(\n                true\n                ,MinDate = MinYear &amp;&amp; MaxDate = MaxYear\n                    , CALCULATE( MAX( Dates[Year] ), Dates[Date] = MinDate )\n                ,MinDate = MinMonth &amp;&amp; MaxDate = MaxMonth\n                    , FORMAT( CALCULATE( MAX( Dates[Month] ), Dates[Date] = MinDate ), \"mmm-yy\" )\n                ,MinDate\n            )\n            VAR GranularityCheckEnd = \n                SWITCH(\n                    true\n                    ,MinDate = MinYear &amp;&amp; MaxDate = MaxYear\n                        , CALCULATE( MAX( Dates[Year] ), Dates[Date] = MaxDate )\n                    ,MinDate = MinMonth &amp;&amp; MaxDate = MaxMonth\n                        , FORMAT( CALCULATE( MAX( Dates[Month] ), Dates[Date] = MaxDate ), \"mmm-yy\" )\n                    ,MaxDate\n                )\n            return\n            IF( GranularityCheckStart = GranularityCheckEnd, GranularityCheckStart, GranularityCheckStart  &amp; \" - \" &amp; GranularityCheckEnd )\n    )\nVAR result = CONCATENATEX ( islands, [IslandString], UNICHAR( 10 ), [@Islands], ASC )\nRETURN\nresult\n</code></pre>"},{"location":"posts/VSCode-Extensions/","title":"VS Code Extensions For Power BI Development and Administration","text":"<p>VS Code is a simple, powerful and highly customizable IDE. Features include syntax highlighting, intellisense, git version control, integrated terminal, and more. The Extension Marketplace offers additional languages and editor features. You can make your environment your own, by applying themes and setting up editor shortcuts and preferences.</p> <p>Because of this, VS Code is the IDE most used by developers; as per the Stack Overflow 2023 Developer Survey.</p>"},{"location":"posts/VSCode-Extensions/#extensions","title":"Extensions","text":"<p>Here are subset of extensions I use for Power BI development, administration and automation.</p>"},{"location":"posts/VSCode-Extensions/#languages","title":"Languages","text":"Language VS Code Extension ID Syntax Highlighting Intellisense TMDL analysis-services.TMDL \u2714\ufe0f \u274c DAX jianfajun.dax-language \u2714\ufe0f \u274c M PowerQuery.vscode-powerquery \u2714\ufe0f \u2714\ufe0f C# ms-dotnettools.csharp \u2714\ufe0f \u2714\ufe0f Powershell ms-vscode.powershell \u2714\ufe0f \u2714\ufe0f Python ms-python.python \u2714\ufe0f \u2714\ufe0f YAML redhat.vscode-yaml \u2714\ufe0f \u274c <p>TMDL, DAX and M are used to review and update tabular models that have been decompiled to files (.pbip, pbitools, tabular editor). Powershell and C# for scripting using the analysis services libraries and cmdlets. Python and Powershell for scripting using the Power BI and Fabric REST APIs. YAML is used for developing GitHub/ADO pipelines.</p>"},{"location":"posts/VSCode-Extensions/#version-control","title":"Version Control","text":"Extension VS Code Extension ID Gitlens eamodio.gitlens Git Graph mhutchie.git-graph <p>These extensions extend VS Code's inbuilt git version control functionality. The features I use most are Inline blame annotation on rows, and On hover pop-up to show the details of commit.</p> <p> GitLens: In-line Blame</p> <p> GitLens: On Hover Blame Details</p> <p>Commit graph and GitGraph. Commit graph, gives you a graph of commits, branches and merges over time, although I prefer Git Graph for this as it is simpler and more compact.</p> <p> GitLens: Commit Graph</p> <p> Git Graph: Git Graph</p>"},{"location":"posts/DeltaParquet/","title":"The Delta Between a Lake and Lakehouse","text":"<p>Data Lakes offer flexible low cost file storage, but how do they differ from Data Lakehouse's? Data Lakehouse's leverage existing Data Lake architectures, and add a metadata layer (Table Format) above the data files to confer the additional reliability, resilience and performance associated with Data Warehouses. There are a number of open-source Table Formats: Delta Lake, Apache Hudi and Apache Iceberg. Microsoft Fabric and Databricks use Delta Lake, so I will focus on that.</p>"},{"location":"posts/DeltaParquet/#data-lakehouse-components","title":"Data Lakehouse Components","text":"<p>A Data Lakehouse uses the following components:</p> Component Example Storage Azure Data Lake Storage Gen2 File Format Apache Parquet Table Format Delta Lake Catalog Hive Metastore Query Engine Apache Spark <p>The Storage, File Formats, Catalog and Query Engine can all be present in the architecture of a classical Data Lake. The Table Format uplifts this architecture to a Data Lakehouse, adding a range of features including ACID transactions, file skipping, time travel, and schema enforcement and evolution.</p>"},{"location":"posts/DeltaParquet/#delta-lake","title":"Delta Lake","text":"<p>Delta Lake tables are defined by a directory, which contain the Delta Transaction Log and Apache Parquet files. The Transaction Log is a ordered record of every transaction commit against the table, and the Parquet files stores the data committed to the table.</p> <p>The best way to understand Delta Tables is see them in action. Lets start with a simple example:</p> <pre><code>+\ud83d\udcc1 delta_table\n+\u2515\u2501\u2501 \ud83d\udcc1 _delta_log        # Transaction Log\n+\u2502   \u2515\u2501\u2501 \ud83d\udcc4 00000.json    # Add 1.parquet\n+\u2515\u2501\u2501 \ud83d\udcc4 1.parquet         # Data File\n</code></pre> <p>Data is saved as a parquet file(s) and changes are committed to the transactions log.</p> <p>ACID</p> <p>If a operation against a Data Lake table fails, it could left be in a compromised state. With Delta Lake operations against table are Atomic transactions, if there is a failure anywhere in a transaction, then no commit is made to Transaction Log and the Table is still valid.</p> <p>Lets append some more data. </p> <pre><code> \ud83d\udcc1 delta_table\n \u2515\u2501\u2501 \ud83d\udcc1 _delta_log\n \u2502   \u2515\u2501\u2501 \ud83d\udcc4 00000.json  # Add 1.parquet\n+\u2502   \u2515\u2501\u2501 \ud83d\udcc4 00001.json  # Add 2.parquet\n \u2515\u2501\u2501 \ud83d\udcc4 1.parquet\n+\u2515\u2501\u2501 \ud83d\udcc4 2.parquet\n</code></pre> <p>A new file was added with a new commit.</p> <p>Lets delete some rows from the table.</p> <pre><code> \ud83d\udcc1 delta_table\n \u2515\u2501\u2501 \ud83d\udcc1 _delta_log\n \u2502   \u2515\u2501\u2501 \ud83d\udcc4 00000.json  # Add 1.parquet\n \u2502   \u2515\u2501\u2501 \ud83d\udcc4 00001.json  # Add 2.parquet\n+\u2502   \u2515\u2501\u2501 \ud83d\udcc4 00002.json  # Remove 2.parquet, Add 3.parquet\n \u2515\u2501\u2501 \ud83d\udcc4 1.parquet\n \u2515\u2501\u2501 \ud83d\udcc4 2.parquet\n+\u2515\u2501\u2501 \ud83d\udcc4 3.parquet\n</code></pre> <p>Parquet files can be considered to immutable, therefore For DML commands UPDATE, DELETE, and MERGE, existing parquet files are not altered. Instead new version of files are created, and the old versions \"deleted\". We see that the Transaction Log notes that <code>2.parquet</code> was removed and <code>3.parquet</code> was added. <code>3.parquet</code> was added to the folder but <code>2.parquet</code> still exists. To ensure Isolation of transactions files are not deleted straight away, instead the remove operation is a soft-delete and is files are tombstoned. This gives us the ability to time travel in the table and view previous versions, as we can traverse the Transaction Log up to a specific commit to determine which files formed the Table at that point in time.</p> <p>VACCUM</p> <p>Tombstoned files can be fully deleted with VACCUM command. \"Deleted\" file can be removed from the Data Lake via the VACCUM command, at which point the ability to Time Travel to a commit that relies on that file is lost.</p> <p>If we continue to perform actions on the table, after every ten commits a checkpoint file is created. This combines all the small JSON commit files into a single parquet file that is more easily parsed.</p> <pre><code> \ud83d\udcc1 delta_table\n \u2515\u2501\u2501 \ud83d\udcc1 _delta_log\n \u2502   \u2515\u2501\u2501 \ud83d\udcc4 00000.json  # Add 1.parquet\n \u2502   \u2515\u2501\u2501 \ud83d\udcc4 00001.json  # Add 2.parquet\n \u2502   \u2515\u2501\u2501 \ud83d\udcc4 00002.json  # Remove 2.parquet, Add 3.parquet\n+\u2502   \u2515\u2501\u2501 ...\n+\u2502   \u2515\u2501\u2501 \ud83d\udcc4 00010.json\n+\u2502   \u2515\u2501\u2501 \ud83d\udcc4 00010.checkpoint.parquet\n \u2515\u2501\u2501 \ud83d\udcc4 1.parquet\n \u2515\u2501\u2501 \ud83d\udcc4 2.parquet\n \u2515\u2501\u2501 \ud83d\udcc4 3.parquet\n+\u2515 ...\n</code></pre> <p>In the case where there are multiple concurrent transactions, each will try to commit. One will win, the loser will check the new current state of the table and attempt another commit. This provides transaction's Isolation.</p> <p>It's worth mentioning that the Transaction log also stores the Table's Schema. This is important as this allows for the protection of the Tables via on-write Schema Enforcement. Conversely that is also the idea of Schema Evolution that allows for schema merging. These topics are covered here.</p> <p>If you want read about the Transaction Log in more detail, then look to other blogs or the Delta Transaction Log Protocol.</p> <p>Delta Lake uses Parquet files to store the Table's data. Parquet is a open-source columnar storage format that employs efficient compression and encoding techniques. It is very cool and worth understanding but beyond the scope of this blog post.</p> <p> Parquet Structure</p>"},{"location":"posts/DeltaParquet/#resources","title":"Resources","text":""},{"location":"posts/DeltaParquet/#delta-lake_1","title":"Delta Lake","text":"<ul> <li>Delta Lake</li> <li>Databricks: Diving Into Delta Lake: Unpacking The Transaction Log</li> <li>Databricks: Diving Into Delta Lake: Schema Enforcement &amp; Evolution</li> <li>Diving Into Delta Lake: DML Internals</li> <li>DENNY LEE: Understanding the Delta Lake transaction log at the file level</li> <li>DENNY LEE: A peek into the Delta Lake Transaction Log</li> </ul>"},{"location":"posts/DeltaParquet/#parquet","title":"Parquet","text":"<ul> <li>Parquet Github</li> <li>Querying Parquet with Millisecond Latency</li> <li>Spark + Parquet In Depth: Spark Summit East talk by: Emily Curtin and Robbie Strickland</li> <li>The columnar roadmap: Apache Parquet and Apache Arrow</li> </ul>"},{"location":"posts/Git-Submodule-Report-Theme/","title":"Using Git Submodules to Distribute a Common Theme to Power BI Reports","text":"<p>As of a couple of months ago PBIR has been added to PBIP. This new format brings a bunch of benefits. As a chance to explore the format more I've explored the concept for injecting a report Theme from a Donor Report, defined in a Git Submodule, into Recipient Reports.</p>"},{"location":"posts/Git-Submodule-Report-Theme/#recipient","title":"Recipient","text":"<p>Lets start by creating a Recipient folder. We create our report and save the report in the PBIP format with PBIR enabled. We enable git and commit our changes.</p> <pre><code>cd Recipient\ngit init\ngit add .\ngit commit -m \"init\"\n</code></pre> <pre><code>+\ud83d\udcc1 Recipient\n+\u251c\u2500\u2500 \ud83d\udcc1 recipient.Report\n+\u2502    \u251c\u2500\u2500 \ud83d\udcc1 .pbi\n+\u2502 \u00a0 \u00a0\u251c\u2500\u2500 \ud83d\udcc1 definition\n+\u2502 \u00a0 \u00a0\u2502   \u251c\u2500\u2500 \ud83d\udcc1 pages\n+\u2502 \u00a0 \u00a0\u2502   \u251c\u2500\u2500 \ud83d\udcc4report.json\n+\u2502 \u00a0 \u00a0\u2502   \u2514\u2500\u2500 \ud83d\udcc4version.json\n+\u2502 \u00a0 \u00a0\u251c\u2500\u2500 \ud83d\udcc1 StaticResources\n+\u2502 \u00a0 \u00a0\u2502   \u2514\u2500\u2500 \ud83d\udcc1 SharedResources\n+\u2502 \u00a0 \u00a0\u2502       \u2514\u2500\u2500 \ud83d\udcc1 BaseThemes\n+\u2502 \u00a0 \u00a0\u2502           \u2514\u2500\u2500 \ud83d\udcc4CY24SU06.json\n+\u2502 \u00a0 \u00a0\u251c\u2500\u2500 \ud83d\udcc4 .platform\n+\u2502 \u00a0 \u00a0\u2514\u2500\u2500 \ud83d\udcc4 definition.pbir\n+\u251c\u2500\u2500 \ud83d\udcc1 recipient.SemanticModel\n+\u2514 .gitignore\n</code></pre>"},{"location":"posts/Git-Submodule-Report-Theme/#donor","title":"Donor","text":"<p>Now we'll create a Donor folder to host our donor report. We create a blank report, define a custom theme, and save the report in the PBIP format with PBIR enabled. I defined a full PBIP here rather than individual files to allow for easy updates via PBI Desktop.</p> <pre><code>cd Donor\ngit init\ngit add .\ngit commit -m \"init\"\n</code></pre> <pre><code>+\ud83d\udcc1 Donor\n+\u251c\u2500\u2500 \ud83d\udcc1 donor.Report\n+\u2502    \u251c\u2500\u2500 \ud83d\udcc1 .pbi\n+\u2502 \u00a0 \u00a0\u251c\u2500\u2500 \ud83d\udcc1 definition\n+\u2502 \u00a0 \u00a0\u2502   \u251c\u2500\u2500 \ud83d\udcc1 pages\n+\u2502 \u00a0 \u00a0\u2502   \u251c\u2500\u2500 \ud83d\udcc4report.json\n+\u2502 \u00a0 \u00a0\u2502   \u2514\u2500\u2500 \ud83d\udcc4version.json\n+\u2502 \u00a0 \u00a0\u251c\u2500\u2500 \ud83d\udcc1 StaticResources\n+\u2502 \u00a0 \u00a0\u2502   \u2514\u2500\u2500 \ud83d\udcc1 RegisteredResources\n+\u2502 \u00a0 \u00a0\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 donorTheme.json\n+\u2502 \u00a0 \u00a0\u2502   \u2514\u2500\u2500 \ud83d\udcc1 SharedResources\n+\u2502 \u00a0 \u00a0\u2502       \u251c\u2500\u2500 \ud83d\udcc1 BaseThemes\n+\u2502 \u00a0 \u00a0\u2502       \u2502   \u2514\u2500\u2500 \ud83d\udcc4 CY24SU06.json\n+\u2502 \u00a0 \u00a0\u2502       \u2514\u2500\u2500 \ud83d\udcc1 BaseThemes\n+\u2502 \u00a0 \u00a0\u251c\u2500\u2500 \ud83d\udcc4 .platform\n+\u2502 \u00a0 \u00a0\u2514\u2500\u2500 \ud83d\udcc4 definition.pbir\n+\u251c\u2500\u2500 \ud83d\udcc1 donor.SemanticModel\n+\u2514 .gitignore\n</code></pre> <p>I then pushed this repo to GitHub.</p> <pre><code>git remote add origin https://github.com/EvaluationContext/Donor.git\ngit branch -M main\ngit push -u origin main\n</code></pre>"},{"location":"posts/Git-Submodule-Report-Theme/#git-submodule","title":"Git Submodule","text":"<p>We now need to navigate back to our local Recipient folder and add register our remote Donor repo as a submodule. </p> <pre><code>cd Recipient\ngit submodule add https://github.com/EvaluationContext/Donor\n</code></pre> <pre><code> \ud83d\udcc1 Recipient\n \u251c\u2500\u2500 \ud83d\udcc1 recipient.Report\n \u251c\u2500\u2500 \ud83d\udcc1 recipient.SemanticModel\n+\u251c\u2500\u2500 \ud83d\udcc1 Donor\n+\u2502   \u251c\u2500\u2500 \ud83d\udcc1 donor.Report\n+\u2502   \u251c\u2500\u2500 \ud83d\udcc1 donor.SemanticModel\n+\u2502   \u2514 .gitignore\n+\u251c .gitmodules\n \u2514 .gitignore\n</code></pre> <p>Above you can see the Donor repo is nested within Recipient repo, plus a new .gitmodules file. This means the Recipient repo now has access to files in the Donor Repo. The point being any arbitrary number of Recipient repos can access the files defined once in Donor repo.</p>"},{"location":"posts/Git-Submodule-Report-Theme/#script-to-donate-theme","title":"Script to Donate Theme","text":"<p>We now need to add and update files in the Recipient Report, so that the Donor theme is applied.</p>"},{"location":"posts/Git-Submodule-Report-Theme/#required-changes","title":"Required Changes","text":"<p>In order for the theme to be applied we need to:</p> <ul> <li>Copy <code>Donor/Recipient/recipient.Report/StaticResources/RegisteredResources/donorTheme.json</code> to <code>Donor/donor.Report/StaticResources/RegisteredResources/</code></li> </ul> <pre><code>{\n    \"name\": \"donorTheme\",\n    \"textClasses\": {\n        \"label\": {\n            \"color\": \"#0D9BDD\",\n            \"fontFace\": \"'Segoe UI Light', wf_segoe-ui_light, helvetica, arial, sans-serif\"\n        }\n    },\n    \"dataColors\": [\n        \"#BF1212\",\n        \"#B34545\",\n        \"#4B1818\",\n        \"#6B007B\",\n        \"#E044A7\",\n        \"#D9B300\",\n        \"#D63550\"\n    ]\n}\n</code></pre> <ul> <li>Register the custom theme in <code>Donor/donor.Report/definition/report.json</code></li> </ul> <pre><code>{\n    \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/item/report/definition/report/1.0.0/schema.json\"\n    ,\"ThemeCollection\": {\n        \"baseTheme\": {},\n+        \"customTheme\": {\n+            \"name\": \"donorTheme\",\n+            \"reportVersionAtImport\": \"5.55\",\n+            \"type\": \"RegisteredResources\"\n+        }\n    },\n    ...\n    \"resourcePackages\": [\n        {\n            \"name\": \"SharedResources\",\n            ...\n        },\n+        {\n+            \"name\": \"RegisteredResources\",\n+            \"type\": \"RegisteredResources\",\n+            \"items\": [\n+                {\n+                    \"name\": \"donorTheme.json\",\n+                    \"path\": \"donorTheme.json\",\n+                    \"type\": \"CustomTheme\"\n+                }\n+            ]\n+        }\n    ]\n}\n</code></pre>"},{"location":"posts/Git-Submodule-Report-Theme/#manifest","title":"Manifest","text":"<p>We have hosted the entire Donor Report, and we might want to define the donation of other visuals assets in the Recipient repo. Therefore we want to create a file to specifies what assets we want to donate. I have a manifest file (<code>Recipient/.deploymentManifest.json</code>) that I am using for deployments, I extended it to add allow configuration of the required operation.</p> <pre><code>{\n    \"repo\": {},\n    \"items\": {\n        \"semanticModels\" : {},\n        \"reports\": {\n            \"recipient.report\": {\n                \"path\": \"recipient.report\",\n                \"addItems\": {\n                    \"path\": \"Donor/donor.report\",\n                    \"visuals\": {},\n                    \"images\": {},\n                    \"theme\": \"Recipient/recipient.Report/StaticResources/RegisteredResources/donorTheme.json\"\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>We can save this to the repo.</p> <pre><code> \ud83d\udcc1 Recipient\n \u251c\u2500\u2500 \ud83d\udcc1 recipient.Report\n \u251c\u2500\u2500 \ud83d\udcc1 recipient.SemanticModel\n \u251c\u2500\u2500 \ud83d\udcc1 Donor\n+\u251c .deploymentManifest.json\n \u251c .gitmodules\n \u2514 .gitignore\n</code></pre>"},{"location":"posts/Git-Submodule-Report-Theme/#script","title":"Script","text":"<p>We now need to read <code>.deploymentManifest.json</code> detect if a custom theme is specified and update the definition of the Recipient Report. As a proof of concept I'll assume there is no custom theme currently applied in the Recipient Report.</p> <p>script</p> <p>I apologize in advance for this Powershell script, I'm sure there is a nicer way of writing this</p> <pre><code>$deploymentManifest = Get-Content '.deploymentManifest.json' | Out-String | ConvertFrom-Json -AsHashtable\n\nforeach ($recipientReport in $deploymentManifest.items.reports.GetEnumerator()) {\n    foreach($donorReport in $recipientReport.Value.addItems.GetEnumerator()) {\n\n        $recipientPath = $recipientReport.Value.path\n        $donorPath = $donorReport.Value.path\n\n        Write-Host \"Donating Files\"\n        $theme = $donorReport.Value.theme\n        $donorPath = \"$pwd/$donorPath/StaticResources/RegisteredResources/$theme\"\n        $recipientFolderPath = \"$pwd/$recipientPath/StaticResources/RegisteredResources\"\n        $recipientPath = \"$recipientFolderPath/$theme\"\n        if(-Not (Test-Path $recipientFolderPath)) {New-Item -ItemType \"directory\" -Path $recipientFolderPath}\n        Copy-Item -Path $donorPath -Destination $recipientPath\n\n        Write-Host \"Registering Files\"\n        $recipientReportjson = Get-Content -Path \"$pwd/$recipientPath/definition/report.json\" | ConvertFrom-Json -AsHashtable\n        $themeCollection = @{\n            name = $theme;\n            reportVersionAtImport = \"5.55\";\n            type = \"RegisteredResources\"\n        }\n\n        $resourcePackages = @{\n            name  = \"RegisteredResources\";\n            type  = \"RegisteredResources\";\n            items = @(\n                @{\n                    name = $theme;\n                    path = $theme;\n                    type = \"CustomTheme\"\n                }\n            )\n        }\n\n        $recipientReportjson.themeCollection[\"customTheme\"] = $themeCollection\n        $recipientReportjson.resourcePackages += $resourcePackages\n        $updatedFile = $recipientReportjson | ConvertTo-Json -Depth 10\n        Set-Content -Path \"$pwd/$recipientPath/definition/report.json\" -Value $updatedFile\n</code></pre> <p>Running the script results in the following results in the following.</p> <pre><code> \ud83d\udcc1 Recipient\n \u251c\u2500\u2500 \ud83d\udcc1 recipient.Report\n \u2502    \u251c\u2500\u2500 \ud83d\udcc1 .pbi\n \u2502 \u00a0 \u00a0\u251c\u2500\u2500 \ud83d\udcc1 definition\n \u2502 \u00a0 \u00a0\u2502   \u251c\u2500\u2500 \ud83d\udcc1 pages\n-\u2502 \u00a0 \u00a0\u2502   \u251c\u2500\u2500 \ud83d\udcc4report.json\n+\u2502 \u00a0 \u00a0\u2502   \u251c\u2500\u2500 \ud83d\udcc4report.json\n \u2502 \u00a0 \u00a0\u2502   \u2514\u2500\u2500 \ud83d\udcc4version.json\n \u2502 \u00a0 \u00a0\u251c\u2500\u2500 \ud83d\udcc1 StaticResources\n+\u2502 \u00a0 \u00a0\u2502   \u2514\u2500\u2500 \ud83d\udcc1 RegisteredResources\n+\u2502 \u00a0 \u00a0\u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4 donorTheme.json\n \u2502 \u00a0 \u00a0\u2502   \u2514\u2500\u2500 \ud83d\udcc1 SharedResources\n \u2502 \u00a0 \u00a0\u251c\u2500\u2500 \ud83d\udcc4 .platform\n \u2502 \u00a0 \u00a0\u2514\u2500\u2500 \ud83d\udcc4 definition.pbir\n \u251c\u2500\u2500 \ud83d\udcc1 recipient.SemanticModel\n \u251c\u2500\u2500 \ud83d\udcc1 Donor\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 donor.Report\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 donor.SemanticModel\n \u2502   \u2514 .gitignore\n \u251c .deploymentManifest.json\n \u251c .gitmodules\n \u2514 .gitignore\n</code></pre> <p>When we open the file we can see the theme has changed.</p> <p></p>"},{"location":"posts/Git-Submodule-Report-Theme/#conclusion","title":"Conclusion","text":"<p>In regards to resources it would be nice if their presence would register them as to use to avoid having to register them in report.json. Regardless, this pattern could be quite useful in defining a theme, allow propagation of a standard from a single repo to many reports. This version while rough introduces the concept.</p>"},{"location":"posts/SVG-RAG-Dumbell/","title":"SVG Dumbbell Chart in Power BI","text":"<p>I recently got inspired by post by Kurt Buhler: Creating custom visuals in Power BI with DAX. I have seen many of Kerry Kolosko's SVG examples in the past but have not got round to playing with them. I have a report that visualizes KPIs over time in a matrix. Each of these KPIs have a status of Green, Yellow or Red, depending on whether the value is within a range. The most simple way to visualize this is just to conditional colour the cell to show the status, but that option doesn't provide much visual context to how close to the threshold the value is. I decided to take Kurt's post and adjust it to create a visual to suit my needs.</p> <p>I was happy to discovery the process of creating SVGs to be quite easy. You define elements and their properties such as their coordinates and colours. You define the SVG definition as string using DAX, set the measures to Image URL Type and throw it in a matrix. This is what I ended up with:</p> VisualCode <p></p> <pre><code>Bar Bell SVG =\nVAR _SvgWidth =                 75\nVAR _SvgHeight =                20\n\n// Values\nVAR _ActualValue =              [Max Value]\nVAR _ActualColour=              MAX( 'Fact'[Colour] )\nVAR _ActualValueFormatted =     IF( MAX( Metrics[format] ) = \"Percent\", FORMAT( _ActualValue, \"0%\"), FORMAT( _ActualValue, \"0.00\") )\nVAR _RedValue =                 MAX( Metrics[red#] )\nVAR _GreenValue =               MAX( Metrics[green#] )\nVAR _SmallValue =               MIN( _RedValue, _GreenValue)\nVAR _LargeValue =               MAX( _RedValue, _GreenValue) \nVAR _SmallestValue =            MIN( _SmallValue, _ActualValue )\nVAR _LargestValue =             MAX( _LargeValue, _ActualValue )\n\n// Mapping from values to svg scale\nVAR _Diff =                     _LargestValue - _SmallestValue\nVAR _InputStart =               _SmallestValue - _Diff / 2          // The lowest number of the range input\nVAR _InputEnd =                 _LargestValue + _Diff / 2           // The largest number of the range input\nVAR _OutputStart =              0                                   // The lowest number of the range output\nVAR _OutputEnd =                _SvgWidth                           // The largest number of the range output\n\nVAR _ActualPosition =           _OutputStart + ((_OutputEnd - _OutputStart) / (_InputEnd - _InputStart)) * (_ActualValue - _InputStart)\nVAR _SmallPosition =            _OutputStart + ((_OutputEnd - _OutputStart) / (_InputEnd - _InputStart)) * (_SmallValue - _InputStart)\nVAR _LargePosition =            _OutputStart + ((_OutputEnd - _OutputStart) / (_InputEnd - _InputStart)) * (_LargeValue - _InputStart)\n\n// Colours\nVAR _Opacity =                  \"80\" // 50%\nVAR _ActualHex =                MAX( 'Fact'[Colour Hex] )\nVAR _RedHex =                   \"#D2222D\"\nVAR _AmberHex =                 \"#FFBF00\"\nVAR _GreenHex =                 \"#238823\"\nVAR _GreyHex =                  \"#A3A3A3\"\nVAR _WhiteHex =                 \"#FFFFFF\"\nVAR _BlackHex =                 \"#000000\"\nVAR _SmallHex =                 IF( _GreenValue = _SmallValue, _GreenHex, _RedHex )\nVAR _LargeHex =                 IF( _GreenValue = _LargeValue, _GreenHex, _RedHex )\nVAR _CallOutHex =              \n    SWITCH(\n        _ActualColour\n        ,\"Green\", _GreyHex\n        ,\"Red\", _BlackHex   \n        ,\"Amber\",\n            // https://dax.tips/2019/10/02/dax-base-conversions/\n            VAR StartNumber = 163 //_GreyHex\n            VAR ConvertMe =\n                MIN(\n                    StartNumber +  // distance to red\n                    SWITCH(\n                        true\n                        ,_RedValue = _SmallValue,       INT( ( 1 - ( _ActualPosition - _SmallPosition ) / ( _LargePosition - _SmallPosition ) ) * 100 )\n                        ,_RedValue = _LargestValue,     INT( ( 1 - ( _LargePosition - _ActualPosition ) / ( _LargePosition - _SmallPosition ) ) * 100 )\n                    )\n                    ,255 - StartNumber\n                )\n            VAR Base = 16\n            VAR BitTable = GENERATESERIES ( 1, 8 )\n            VAR DEC2HEX =\n                CONCATENATEX(\n                    BitTable,\n                    VAR c = MOD ( TRUNC ( ConvertMe / POWER ( base, [value] - 1 ) ),base )\n                    RETURN SWITCH(c,10,\"A\",11,\"B\",12,\"C\",13,\"D\",14,\"E\",15,\"F\",c),\n                    ,[Value],Desc\n                )\n            VAR SubHex = RIGHT( DEC2HEX, 2 )\n            RETURN\n                \"#\" &amp; REPT( SubHex, 3 )\n    )\n\n// Vectors\nVAR _Offset = 5\nVAR _linePosition =             _SvgHeight - (_SvgHeight / 4)\nVAR _MiddleLine =               \"&lt;line x1=\"\"\" &amp; _SmallPosition &amp; \"\"\"\" &amp; UNICHAR(10) &amp; \"y1=\"\"\" &amp; _linePosition &amp; \"\"\" x2=\"\"\" &amp; _LargePosition &amp; \"\"\" y2=\"\"\" &amp; _linePosition &amp; \"\"\" stroke=\"\"\" &amp; _GreyHex &amp; \"\"\"/&gt;\"\nVAR _ActualCircle =             \"&lt;circle cx=\"\"\" &amp; _ActualPosition &amp;\"\"\" cy=\"\"\" &amp; _linePosition &amp; \"\"\" r=\"\"3\"\" fill=\"\"\" &amp; _ActualHex &amp; \"\"\" stroke= '\" &amp; _BlackHex &amp; \"'/&gt;\"\nVAR _SmallCircle =              \"&lt;circle cx=\"\"\" &amp; _SmallPosition &amp;\"\"\" cy=\"\"\" &amp; _linePosition &amp; \"\"\" r=\"\"2\"\" fill=\"\"\" &amp; _SmallHex &amp; _Opacity &amp; \"\"\"/&gt;\"\nVAR _LargeCircle =              \"&lt;circle cx=\"\"\" &amp; _LargePosition &amp;\"\"\" cy=\"\"\" &amp; _linePosition &amp; \"\"\" r=\"\"2\"\" fill=\"\"\" &amp; _LargeHex &amp; _Opacity &amp; \"\"\"/&gt;\"\nVAR _Callout =                  \"&lt;text x='\" &amp; _ActualPosition - _Offset &amp; \"' y='\"&amp; _SvgHeight / 3 &amp; \"' fill='\" &amp; _CallOutHex  &amp; \"' font-size='8' font-family='Segoe UI, sans-serif' &gt;\"&amp; _ActualValueFormatted &amp;\"&lt;/text&gt;\"\n\nVAR _Svg =\n    \"data:image/svg+xml;utf8, &lt;svg width=\"\"\" &amp; _SvgWidth &amp; \"\"\" height=\"\"\" &amp; _SvgHeight &amp;\"\"\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\" &amp;\n    _MiddleLine &amp;\n    _SmallCircle &amp;\n    _LargeCircle &amp;\n    _ActualCircle &amp;\n    _Callout &amp;\n    \"&lt;/svg&gt;\"\n\nRETURN\nIF( not ISBLANK( _ActualValue ) &amp;&amp;  not ISBLANK( MAX( Metrics[RAG] )), _Svg )\n</code></pre>"},{"location":"posts/SVG-RAG-Trend/","title":"Further Adventures in SVGs","text":"<p>In a previous post I created a SVG Barbell visual. </p> <p>While this was a good start I wanted to expand upon this concept by adding an extra dimension of time to see trends.</p> VisualCode <p></p> <pre><code>Trend SVG =\nVAR _SvgWidth =                 100\nVAR _SvgHeight =                20\n\n// values\nVAR _ActualValue =              [Max Value]\nVAR _ActualColour=              [Colour Hex]\nVAR _ActualValueFormatted =     IF( MAX( Metrics[format] ) = \"Percent\", FORMAT( _ActualValue, \"0.0%\"), FORMAT( _ActualValue, \"0.0\") )\nVAR _RedValue =                 MAX( Metrics[red#] )\nVAR _GreenValue =               MAX( Metrics[green#] )\n\n// y axis\nVAR _Actual =\n    ADDCOLUMNS(\n        CALCULATETABLE( VALUES( Periods[Reporting Period] ), ALLSELECTED( Periods ) )\n        ,\"@Actual\", CALCULATE( [Max Value] )\n    )\n\nVAR _MinActual =                MINX( _Actual, [@Actual] )\nVAR _MaxActual =                MAXX( _Actual, [@Actual] )\nVAR _SmallValue =               MIN( _RedValue, _GreenValue)\nVAR _LargeValue =               MAX( _RedValue, _GreenValue) \nVAR _SmallestValue =            MIN( _SmallValue, _MinActual )\nVAR _LargestValue =             MAX( _LargeValue, _MaxActual )\n\nVAR _YOffset =                  5\nVAR _YInputStart =              _SmallestValue                     // The lowest number of the range input\nVAR _YInputEnd =                _LargestValue                      // The largest number of the range input\nVAR _YOutputStart =             _SvgHeight -  _YOffset             // The lowest number of the range output\nVAR _YOutputEnd =               0 + _YOffset                        // The largest number of the range output\n\n// x axis\nVAR _DateMin =                  CALCULATE( MINX( SUMMARIZE( 'Fact', Periods[End Date] ), Periods[End Date] ), ALLSELECTED( Periods ), ALLSELECTED( Metrics ) )\nVAR _DateMax =                  CALCULATE( MAXX( SUMMARIZE( 'Fact', Periods[End Date] ), Periods[End Date] ), ALLSELECTED( Periods ), ALLSELECTED( Metrics ) )\nVAR _MaxDateWithActual =        CALCULATE( MAXX( SUMMARIZE( 'Fact', Periods[End Date] ), Periods[End Date] ) )\n\nVAR _XOffset =                   5\nVAR _XInputStart =               _DateMin                           // The lowest number of the range input\nVAR _XInputEnd =                 _DateMax                           // The largest number of the range input\nVAR _XOutputStart =              0 + _XOffset                       // The lowest number of the range output\nVAR _XOutputEnd =                _SvgWidth - _XOffset - 20          // The largest number of the range output\n\n// Colours\nVAR _Opacity =                  \"73\" // 45%\nVAR _RedHex =                   \"#A9000A\"\nVAR _AmberHex =                 \"#E49F16\"\nVAR _GreenHex =                 \"#00847E\"\nVAR _GreyHex =                  \"#A3A3A3\"\nVAR _BlackHex =                 \"#000000\"\nVAR _SmallHex =                 IF( _GreenValue = _SmallValue, _GreenHex, _RedHex )\nVAR _LargeHex =                 IF( _GreenValue = _LargeValue, _GreenHex, _RedHex )\nVAR _CallOutHex =               _BlackHex\n\n// Vectors\nVAR _TopPosition =             _YOutputStart + ((_YOutputEnd - _YOutputStart) / (_YInputEnd - _YInputStart)) * (_LargeValue  - _YInputStart)\nVAR _BottomPosition =          _YOutputStart + ((_YOutputEnd - _YOutputStart) / (_YInputEnd - _YInputStart)) * (_SmallValue  - _YInputStart)\nVAR _TopLine =                  \"&lt;line x1=\"\"\" &amp; _XOutputStart &amp; \"\"\"\" &amp; UNICHAR(10) &amp; \"y1=\"\"\" &amp; _TopPosition &amp; \"\"\" x2=\"\"\" &amp; _XOutputEnd &amp; \"\"\" y2=\"\"\" &amp; _TopPosition &amp; \"\"\" stroke=\"\"\" &amp; _LargeHex &amp; _Opacity &amp; \"\"\"/&gt;\"\nVAR _BottomLine =               \"&lt;line x1=\"\"\" &amp; _XOutputStart &amp; \"\"\"\" &amp; UNICHAR(10) &amp; \"y1=\"\"\" &amp; _BottomPosition &amp; \"\"\" x2=\"\"\" &amp; _XOutputEnd &amp; \"\"\" y2=\"\"\" &amp; _BottomPosition &amp; \"\"\" stroke=\"\"\" &amp; _SmallHex &amp; _Opacity &amp; \"\"\"/&gt;\"\n\n// Circles\nvar _SmallCircleSize =          1.\nvar _LargeCircleSize =          3\nvar _Circles =\n  ADDCOLUMNS(\n      CALCULATETABLE( SUMMARIZE( 'Fact', Periods[End Date] ) ,REMOVEFILTERS( Periods ), DATESBETWEEN( Dates[Date], _DateMin, _MaxDateWithActual ) )\n      ,\"@circles\"\n          ,var xVal = Periods[End Date]\n          var yVal =  CALCULATE( [Max Value], REMOVEFILTERS( Periods ), TREATAS( { xVal }, Periods[End Date] ))\n          var hex =   CALCULATE( [Colour Hex], REMOVEFILTERS( Periods ), TREATAS( { xVal }, Periods[End Date] ))\n          var x =     _XOutputStart + ((_XOutputEnd - _XOutputStart) / (_XInputEnd - _XInputStart)) * (xVal - _XInputStart)\n          var y =     _YOutputStart + ((_YOutputEnd - _YOutputStart) / (_YInputEnd - _YInputStart)) * (yVal  - _YInputStart)\n          var size =  IF( Periods[End Date] = _MaxDateWithActual, _LargeCircleSize, _SmallCircleSize )\n          return\n          IF( not ISBLANK( xVal ), \"&lt;circle cx=\"\"\" &amp; x &amp;\"\"\" cy=\"\"\" &amp; y &amp; \"\"\" r=\"\"\" &amp; size &amp; \"\"\" fill=\"\"\" &amp; IF( Periods[End Date] = _MaxDateWithActual, hex, hex &amp; _Opacity ) &amp; \"\"\" stroke= '\" &amp; IF( Periods[End Date] = _MaxDateWithActual, _BlackHex, _BlackHex &amp; _Opacity )  &amp; \"'/&gt;\", \"\")\n          &amp; IF( Periods[End Date] = _MaxDateWithActual  , \"&lt;text x='\" &amp; _SvgWidth - 20 &amp; \"' y='\"&amp; y + _LargeCircleSize &amp; \"' fill='\" &amp; _CallOutHex  &amp; \"' font-size='8' font-family='Segoe UI, sans-serif' &gt;\"&amp; _ActualValueFormatted &amp;\"&lt;/text&gt;\", \"\")\n  )               \n\nVAR _Svg =\n    \"data:image/svg+xml;utf8, &lt;svg width=\"\"\" &amp; _SvgWidth &amp; \"\"\" height=\"\"\" &amp; _SvgHeight &amp;\"\"\" xmlns=\"\"http://www.w3.org/2000/svg\"\"&gt;\" &amp;\n    _TopLine &amp;\n    _BottomLine &amp;\n    CONCATENATEX( _Circles, [@circles] ) &amp;\n    \"&lt;/svg&gt;\"\n\nRETURN\n    IF( not ISBLANK( _ActualValue ) &amp;&amp;  not ISBLANK( MAX( Metrics[RAG] )), _Svg )\n</code></pre> <p>I have seen some interesting examples of SVGs that utilize css and classes. SVG's still have plenty of depth that I have yet to dive into.</p>"},{"location":"posts/SVG-Heatmap/","title":"SVG Heatmap","text":"<p>I have been playing with the Fabric Log Analytics for Analysis Services Engine report template in order to get some insights in query and refresh performance. The data is derived from Analysis Services engine trace events that are collected as part of the Power BI Log Analytics integration. The template provides a a data model and some template visuals to help you understand and manage your Power BI environment. In order to help understand the distribution of data I developed a heatmap SVG.</p> <p>Tip</p> <p>The code from this post has been optimized in a another post</p>"},{"location":"posts/SVG-Heatmap/#heatmap-svg","title":"Heatmap SVG","text":"<p>I wanted to visualize the query and command CPU and Durations per Semantic Model. My first thought was a box-plot. The problem being is most queries are short, but we really want to identify the longer running queries. With a box plot you get an idea of max, but the IQR and mean/median are all on the low end. A jitter plot allows you to visualize the data points, but there are too many in a small space to be viable. The next thought is a violin plot, as it shows the shape of the entire distribution, but this requires quite a bit of processing to generate. My final thought was to split the distribution into boxes and apply a heatmap to the count of values within each box.</p>"},{"location":"posts/SVG-Heatmap/#colour-gradient","title":"Colour Gradient","text":"<p>To colour the boxes we want to apply a colour gradient. This simple enough if we only care about showing shades of gray. If we take the value to want to convert to gray scale, we can map it from a min-max range to a 0-255 range (255 is largest hex value). We then convert the output value to the corresponding hex value.</p> <pre><code>// Mapping values range 0 -&gt; 255\nVAR _inputStart =    0          // The lowest number of the range input\nVAR _inputEnd =      _maxVal    // The largest number of the range input\nVAR _outputStart =   255        // The lowest number of the range output\nVAR _outputEnd =     0          // The largest number of the range output         \nVAR _outputVal =     _outputStart + (( _outputEnd - _outputStart ) / ( _inputEnd - _inputStart )) * ( _val - _inputStart )\n\n// https://dax.tips/2019/10/02/dax-base-conversions/\nVAR ConvertMe = IFERROR( _outputVal, 255 )\nVAR Base = 16\nVAR BitTable = GENERATESERIES ( 1, 8 )\nVAR DEC2HEX =\n    CONCATENATEX(\n        BitTable,\n        VAR c = MOD( TRUNC ( ConvertMe / POWER ( base, [value] - 1 ) ), base )\n        RETURN SWITCH(c,10,\"A\",11,\"B\",12,\"C\",13,\"D\",14,\"E\",15,\"F\",c),\n        ,[Value],Desc\n    )\nVAR HEX = \"#\" &amp; REPT( RIGHT( DEC2HEX, 2 ), 3 ) &amp; IF( ConvertMe = 255, \"00\", \"\" )\nRETURN\nHEX\n</code></pre> <p>I found a great article on how to interpolate from gray scale a colour gradient using SVG filters.</p> <p>We can go from this.</p> <p></p> <p>To this.</p> <p></p> <p>By defining and applying the following filter in our SVG definition. You can set the output scale to any colour by taking the RGB values and dividing them by 255. For example, if you have a R value of 50, 50 / 255 = 0.196.</p> <pre><code>&lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"0\" height=\"0\"&gt;\n    &lt;filter id=\"blue-red\" color-interpolation-filters=\"sRGB\"&gt;\n    &lt;feComponentTransfer&gt;\n        &lt;feFuncR type=\"table\" tableValues=\"0 1\" /&gt;\n        &lt;feFuncG type=\"table\" tableValues=\"0 0\" /&gt;\n        &lt;feFuncB type=\"table\" tableValues=\"1 0\" /&gt;\n    &lt;/feComponentTransfer&gt;\n    &lt;/filter&gt;\n&lt;/svg&gt;\n</code></pre>"},{"location":"posts/SVG-Heatmap/#heatmap","title":"Heatmap","text":"<p>My first approach was to define the number of box, iterate each with <code>GENERATESERIES()</code> and count of the number of values per box, but this resulting in multiple iterations of the fact table. A much faster approach was to iterate the main table once and divide the value by the max value and truncate the decimal, to define the box. You can just group by the calculated box number to get the count per box, which can then be converted to the required hex.</p> <p>The visual and dax are given below. As a side note I applied a log scale to help show boxes with smaller counts.</p> VisualCode <p></p> <pre><code>Query Duration Heatmap SVG =\nVAR _SvgWidth = 150\nVAR _SvgHeight = 20\n\nVAR _numBoxes = 40\nVAR _boxWidth = _SvgWidth / _numBoxes\n\nVAR _range =\n    MAXX(\n        ALLSELECTED( ExecutionMetrics[XmlaRequestId] )\n        ,CALCULATE(\n            SUM( ExecutionMetrics[durationMs] ) / 1000\n            ,ExecutionMetrics[LogAnalyticsCategory]= \"Query\"\n            ,ALLSELECTED( Artifact )\n        )\n    )\nVAR _values =\n    ADDCOLUMNS(\n        VALUES( ExecutionMetrics[XmlaRequestId] )\n        ,\"@Val\"\n        ,CALCULATE(\n            SUM( ExecutionMetrics[durationMs] ) / 1000\n            ,ExecutionMetrics[LogAnalyticsCategory]= \"Query\"\n        )\n    )\nVAR _minVal =           MINX( _values, [@val] )\nVAR _maxVal =           MAXX( _values, [@val] )\nVAR _medianVal =        MEDIANX( _values, [@val] )\nVAR _avgVal =           AVERAGEX( _values, [@val] )\nVAR _assignBoxes =      ADDCOLUMNS( _values , \"@box\" , IF( not ISBLANK( [@val] ), INT( ([@val] / _range) * (_numBoxes - 1 )) + 1 ) )\nVAR _countPerBox =\n    ADDCOLUMNS(\n        SUMMARIZE( _assignBoxes, [@box] )\n        ,\"@x\", ( [@box] * _boxWidth ) - _boxWidth\n        ,\"@cnt\",\n            VAR _box =  [@box]\n            RETURN\n            CALCULATE( COUNTX ( _assignBoxes, IF( [@box] = _box &amp;&amp; _box &lt;&gt; 0, 1 ) ) )\n    )\nVAR _cntRange = MAXX( _countPerBox, [@cnt] )\nVAR _boxes =\n    CONCATENATEX(\n        ADDCOLUMNS(\n            _countPerBox\n            ,\"@Boxes\"\n            ,// Mapping values range 0 -&gt; 255\n            VAR _inputStart =           0                           // The lowest number of the range input\n            VAR _inputEnd =             LOG( _cntRange, 10 )        // The largest number of the range input\n            VAR _outputStart =          255                         // The lowest number of the range output\n            VAR _outputEnd =            0                           // The largest number of the range output         \n            VAR _outputVal =            _outputStart + ((_outputEnd - _outputStart) / (_inputEnd - _inputStart)) * ( LOG( [@cnt], 10 ) - _inputStart)\n\n            // https://dax.tips/2019/10/02/dax-base-conversions/\n            VAR ConvertMe = IFERROR( _outputVal, 255 )\n            VAR Base = 16\n            VAR BitTable = GENERATESERIES ( 1, 8 )\n            VAR DEC2HEX =\n                CONCATENATEX(\n                    BitTable,\n                    VAR c = MOD( TRUNC ( ConvertMe / POWER ( base, [value] - 1 ) ), base )\n                    RETURN SWITCH(c,10,\"A\",11,\"B\",12,\"C\",13,\"D\",14,\"E\",15,\"F\",c),\n                    ,[Value],Desc\n                )\n            VAR HEX = \"#\" &amp; REPT( RIGHT( DEC2HEX, 2 ), 3 ) &amp; IF( ConvertMe = 255, \"00\", \"\" )\n            RETURN\n            \"&lt;rect id='box' x='\" &amp; [@x] &amp; \"' y='\" &amp; _SvgHeight / 2 &amp; \"' width='\" &amp; _boxWidth &amp; \"' height='\" &amp; _SvgHeight / 2 &amp; \"' fill='\" &amp; HEX &amp; \"' filter='url(#gradient)'/&gt;\"\n        )\n        ,[@Boxes]\n    )\n\n// Trends\nvar dt = MAX( Dates[Date] )\nvar _greenHex = \"#37A794\"\nvar _redHex = \"#DD6B7F\" \n\n// Avg Trend\nvar _MonthAverageAvg =\n    CALCULATE(\n        AVERAGEX( VALUES( ExecutionMetrics[XmlaRequestId] ), CALCULATE( SUM( ExecutionMetrics[durationMs] ) ) / 1000 )\n        ,REMOVEFILTERS( Dates )\n        ,DATESINPERIOD( Dates[Date], dt, -1, MONTH )\n        ,ExecutionMetrics[LogAnalyticsCategory] = \"Query\"\n    )\n\nvar _WeekAverageAvg =\n    CALCULATE(\n        AVERAGEX( VALUES( ExecutionMetrics[XmlaRequestId] ), CALCULATE( SUM( ExecutionMetrics[durationMs] ) ) / 1000 )\n        ,REMOVEFILTERS( Dates )\n        ,DATESINPERIOD( Dates[Date], dt, -7, DAY )\n        ,ExecutionMetrics[LogAnalyticsCategory] = \"Query\"\n    )\nVAR _trendAvg = IF( not ISBLANK( _MonthAverageAvg ), (1 - ( _WeekAverageAvg / _MonthAverageAvg )) * - 1 )\nVAR _trendAvgHex = IF( _trendAvg &gt; 0, _redHex, _greenHex )\n\n// 90 Percentile Trend\nvar _MonthAverage90th =\n    CALCULATE(\n        PERCENTILEX.INC( VALUES( ExecutionMetrics[XmlaRequestId] ), CALCULATE( SUM( ExecutionMetrics[durationMs] ) ) / 1000, 0.9)\n        ,REMOVEFILTERS( Dates )\n        ,DATESINPERIOD( Dates[Date], dt, -1, MONTH )\n        ,ExecutionMetrics[LogAnalyticsCategory] = \"Query\"\n    )\nvar _WeekAverage90th =\n    CALCULATE(\n        PERCENTILEX.INC( VALUES( ExecutionMetrics[XmlaRequestId] ), CALCULATE( SUM( ExecutionMetrics[durationMs] ) ) / 1000, 0.9)\n        ,REMOVEFILTERS( Dates )\n        ,DATESINPERIOD( Dates[Date], dt, -7, DAY )\n        ,ExecutionMetrics[LogAnalyticsCategory] = \"Query\"\n    )\nVAR _trend90th = IF( not ISBLANK( _MonthAverage90th ), (1 - ( _WeekAverage90th / _MonthAverage90th )) * - 1 )\nVAR _trend90thHex = IF( _trend90th &gt; 0, _redHex, _greenHex ) \n\n// https://expensive.toys/blog/svg-filter-heat-map \nVAR _svg =\n    \"data:image/svg+xml;utf8, &lt;svg width=\"\"\" &amp; _SvgWidth &amp; \"\"\" height=\"\"\" &amp; _SvgHeight &amp;\"\"\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\" &amp;\n    \"&lt;defs&gt;\n        &lt;filter id=\"\"gradient\"\" color-interpolation-filters=\"\"sRGB\"\"&gt;\n            &lt;feComponentTransfer&gt;\n                &lt;feFuncR type=\"\"table\"\" tableValues=\"\"0.020 0.975\"\" /&gt;\n                &lt;feFuncG type=\"\"table\"\" tableValues=\"\" 0.776 0.975\"\" /&gt;\n                &lt;feFuncB type=\"\"table\"\" tableValues=\"\"0.733 0.975\"\" /&gt;\n            &lt;/feComponentTransfer&gt;\n        &lt;/filter&gt;\n    &lt;/defs&gt;\" &amp;\n    _boxes &amp;\n    \"&lt;text x='0' y='7' fill='black' font-size='6' font-family='Segoe UI, sans-serif'&gt;Max&lt;/text&gt;\" &amp;\n    \"&lt;text x='15' y='7' fill='black' font-size='7' font-family='Segoe UI, sans-serif' font-weight='bold'&gt;\" &amp; FORMAT(_maxVal, \"0.0\") &amp; \"&lt;/text&gt;\" &amp;\n    \"&lt;text x='50' y='7' fill='black' font-size='6' font-family='Segoe UI, sans-serif'&gt;Avg Trend&lt;/text&gt;\" &amp;\n    \"&lt;text x='80' y='7' fill='\" &amp; _trendAvgHex &amp; \"' font-size='7' font-family='Segoe UI, sans-serif' font-weight='bold'&gt;\" &amp; FORMAT(_trendAvg, \"0%\") &amp; \"&lt;/text&gt;\" &amp;\n    \"&lt;text x='105' y='7' fill='black' font-size='6' font-family='Segoe UI, sans-serif'&gt;90th Trend&lt;/text&gt;\" &amp;\n    \"&lt;text x='140' y='7' fill='\" &amp; _trend90thHex &amp; \"' font-size='7' font-family='Segoe UI, sans-serif' font-weight='bold'&gt;\" &amp; FORMAT(_trend90th, \"0%\") &amp; \"&lt;/text&gt;\" &amp;\n    // \"&lt;line x1='\" &amp; ( _avgVal / _range ) * _SvgWidth  &amp; \"' y1='13' x2='\" &amp; ( _avgVal / _range ) * _SvgWidth  &amp; \"' y2='18' style='stroke:red;stroke-width:2' /&gt;\" &amp;\n    \"&lt;/svg&gt;\"\n\nRETURN\nIF( not ISBLANK( _maxVal ), _svg )\n</code></pre>"},{"location":"posts/SVG-Violin/","title":"SVG Violin Plot","text":"<p>In my previous post I created a heat map SVG visual to visualize data distributions. In that post I mentioned Violin plots, in this post I describe how to create one.</p>"},{"location":"posts/SVG-Violin/#violin-plots","title":"Violin Plots","text":"<p>Firstly, what are Violin Plots? Violin plot show the distribution of data points, with the width of the curve estimating the density of points in a region. This allows the visualization multimodal data (more than one peak). These tend to be accompanied with a Box Plot to provide addition information and context. The curves of the Violin Plot are calculated using Kernel Density Estimation (KDE).</p> <p> Wikipedia</p>"},{"location":"posts/SVG-Violin/#kernel-density-estimation-kde","title":"Kernel Density Estimation (KDE)","text":"<p>In KDE data points are converted into kernels, where each point is represented by a distribution (normal, uniform, gaussian etc.). In the case of the normal distribution each point represents a mean (center of the curve), and the standard deviation can be used to expand the width of the distribution, allowing for smoothing. The data is sampled at uniform points across the range of the data, and the contribution each point is summed. I found this video to provide a good description.</p> <p> Wikipedia</p>"},{"location":"posts/SVG-Violin/#paths-and-bezier-curves","title":"Paths and B\u00e9zier Curves","text":"<p>Once we have calculated our KDE, we need to plot the curves. This is done using SVG paths, using B\u00e9zier Curves. B\u00e9zier Curves are defined by start and end point, plus 1 or more control points, which act like gravity pulling the curve towards them. Quadratic B\u00e9zier Curves have a single control point, Cubic have two, and so on. </p> <p> Link</p> <p>In SVG paths the cubic curve is specified by <code>C x1 y1, x2 y2, x y</code>or a short-hand <code>S x2 y2, x y</code>which assumes the first control point is a reflection of the one used previously in the path.</p>"},{"location":"posts/SVG-Violin/#end-result","title":"End Result","text":"<p>Taking all the concept above, we can now generate a Violin Plot. The level of sampling and bandwidth will have to be adjusted to fit your data.</p> VisualCode <p></p> <pre><code>Command Duration Violin SVG =\nVAR _SvgWidth = 150\nVAR _SvgHeight = 20\n\nVAR _samples = 50\nVAR _bandwidth = 60 // Kernal width, adjust to avoid over or under fitting\n\nVAR _range =\n    MAXX(\n        ALLSELECTED( ExecutionMetrics[XmlaRequestId] )\n        ,CALCULATE(\n            SUM( ExecutionMetrics[durationMs] ) / 1000\n            ,ExecutionMetrics[LogAnalyticsCategory]= \"Command\"\n            ,ALLSELECTED( Artifact )\n        )\n    )\nVAR _rangePerSample = _range / _samples\nVAR _values =\n    ADDCOLUMNS(\n        CALCULATETABLE( \n            VALUES( ExecutionMetrics[XmlaRequestId] )\n            , not ISBLANK( ExecutionMetrics[durationMs] )\n            , ExecutionMetrics[LogAnalyticsCategory]= \"Command\" \n        )\n        ,\"@Val\", CALCULATE( SUM( ExecutionMetrics[durationMs] ) / 1000, ExecutionMetrics[LogAnalyticsCategory]= \"Command\" )\n    )\nVAR _numValues = COUNTAX( _values, [@Val] )\nVAR _KDE = // Kernal Density Estimation\n    ADDCOLUMNS(\n        GENERATESERIES( 0, _samples + 1, 1 )\n        ,\"@inputX\", _rangePerSample * [Value]\n        ,\"@KDE\", ( 1 / _numValues ) * SUMX( _values, NORM.DIST( _rangePerSample * [Value], [@val], _bandwidth, false ) ) // Normal distribution for Kernal (Gaussian, Uniform, Triangular etc.)\n    )\nVAR _maxKDE = MAXX( _KDE, [@KDE] )\nVAR _points =\n    ADDCOLUMNS(\n        _KDE\n        ,\"@x\",\n            VAR _xInputStart =           0                         // The lowest number of the range input\n            VAR _xInputEnd =             _range                    // The largest number of the range input\n            VAR _xOutputStart =          0                         // The lowest number of the range output\n            VAR _xOutputEnd =            _SvgWidth                 // The largest number of the range output         \n            VAR _xOutputVal =            _xOutputStart + ((_xOutputEnd - _xOutputStart) / (_xInputEnd - _xInputStart)) * ( [@inputX] - _xInputStart)\n            RETURN\n            _xOutputVal\n        ,\"@y\",\n            VAR _yInputStart =           0                         // The lowest number of the range input\n            VAR _yInputEnd =             _maxKDE                   // The largest number of the range input\n            VAR _yOutputStart =          _SvgHeight / 2            // The lowest number of the range output\n            VAR _yOutputEnd =            0                         // The largest number of the range output         \n            VAR _yOutputVal =            _yOutputStart + ((_yOutputEnd - _yOutputStart) / (_yInputEnd - _yInputStart)) * ([@KDE] - _yInputStart)\n            RETURN\n            _yOutputVal\n    )\nVAR _pointsAndPreviousPoint = \n    NATURALLEFTOUTERJOIN(\n        _points\n        ,SELECTCOLUMNS(\n            _points\n            ,\"Value\", [Value] + 1\n            ,\"@prevX\", [@x]\n            ,\"@prevY\", [@y]\n        )\n    )\nVAR _plusControlPoint = // Calculate control point for cubic B\u00e9zier curve. Assumes they half way between points, at the same height as the current point\n    ADDCOLUMNS(\n        _pointsAndPreviousPoint\n        ,\"@cx\", [@prevX] + (([@x] - [@prevX]) / 2)\n        ,\"@cy\", [@y]\n    )\nVAR _topCurves =\n    CONCATENATEX(\n        _plusControlPoint\n        ,\"S \" &amp; [@cx] &amp; \" \" &amp; [@cy] &amp; \", \" &amp; [@x] &amp; \" \" &amp; [@y] // cubic B\u00e9zier curve\n        , \" \"\n        , [Value]\n)\nVAR _bottomCurves = // top curve inverted\n    CONCATENATEX(\n        _plusControlPoint\n        ,\"S \" &amp; [@cx] &amp; \" \" &amp; (_SvgHeight / 2) - ([@cy] - (_SvgHeight / 2)) &amp; \", \" &amp; [@x] &amp; \" \" &amp; (_SvgHeight / 2) - ([@y] - (_SvgHeight / 2))\n        , \" \"\n        , [Value]\n)\nVAR _svg =\n    \"data:image/svg+xml;utf8, &lt;svg width=\"\"\" &amp; _SvgWidth &amp; \"\"\" height=\"\"\" &amp; _SvgHeight &amp;\"\"\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\" &amp;\n    \"&lt;path d=' M 0 \" &amp; (_SvgHeight / 2) &amp; \" \" &amp; _topCurves &amp; \"M \" &amp; _SvgWidth &amp; \" \" &amp; (_SvgHeight / 2) &amp; \"' stroke='gray' fill='gray' fill-opacity='50%'/&gt;\" &amp;\n    \"&lt;path d=' M 0 \" &amp; (_SvgHeight / 2) &amp; \" \" &amp; _bottomCurves &amp; \"M \" &amp; _SvgWidth &amp; \" \" &amp; (_SvgHeight / 2) &amp; \"' stroke='gray' fill='gray' fill-opacity='50%'/&gt;\" &amp;\n    \"&lt;/svg&gt;\"\nRETURN\nIF( not ISBLANK( _maxKDE ), _svg )\n</code></pre>"},{"location":"posts/SVG-Violin/#conclusion","title":"Conclusion","text":"<p>This visual is more well known than the previous heatmap, so might be more parsable by end users. This comes at the cost of some extra computation. In any case the generation of this visuals involves some interesting concepts that are worth finding more about.</p>"},{"location":"posts/SVG-Sparkline/","title":"Stealing Performance from Sparklines","text":"<p>In this blog post I walk through my discovery of the implementation of Power BI sparklines, and what can be learned in aid of optimizing SVG measures.</p>"},{"location":"posts/SVG-Sparkline/#discovery","title":"Discovery","text":"<p>I was expanding on the Fabric Log Analytics for Analysis Services Engine report template, by creating a barcode SVG visual to visualize refresh successes and failures. </p> <p></p> <p>The measure was working fine locally so I pushed it to the service to a fully hydrated model, and the performance was terrible. So fired up DAX studio to perform some optimization, and ran the visual's DAX query (see below). By chance the visual had a couple of sparklines. I was surprised to see that the sparklines were measures defined in the query, and produced a output similar to that of a SVG, with list x,y coordinates. I investigated further and found that SQLBI's Alberto has a video exploring sparkline measures and they seem close to optimal, so I decided to steal and adjust the code for my purposes.</p> <pre><code>DEFINE MEASURE 'Progress Report'[Sparkline] =\n// USER DAX BEGIN\nVAR __Categories =\n    VALUES('DateTimes'[Date])\nVAR __Data =\n    ADDCOLUMNS(\n        KEEPFILTERS(\n            FILTER(\n                KEEPFILTERS(__Categories),\n                NOT(ISBLANK('Progress Report'[Avg Cmd Duration (s) Refresh]))\n            )\n        ),\n        \"ScalarKey\", 'DateTimes'[Date],\n        \"Value\", 'Progress Report'[Avg Cmd Duration (s) Refresh]\n    )\nRETURN\n    IF(\n        ISEMPTY(__Data),\n        BLANK(),\n        VAR __All_Categories =\n            SELECTCOLUMNS(\n                KEEPFILTERS(\n                    CALCULATETABLE(\n                        ADDCOLUMNS(\n                            KEEPFILTERS(\n                                FILTER(\n                                    KEEPFILTERS(VALUES('DateTimes'[Date])),\n                                    AND(\n                                        NOT(ISBLANK('Progress Report'[Avg Cmd Duration (s) Refresh])),\n                                        NOT(ISBLANK('DateTimes'[Date]))\n                                    )\n                                 )\n                            ),\n                            \"ScalarKey\", 'DateTimes'[Date]\n                        ),\n                        ALLSELECTED()\n                    )\n                ),\n                \"ScalarKey\", [ScalarKey]\n            )\n        VAR __Sample_Categories =\n            SAMPLE(53, __All_Categories, [ScalarKey], 1)\n        VAR __Min_Interval =\n            MINX(\n                KEEPFILTERS(__Sample_Categories),\n                VAR __Previous =\n                    MAXX(\n                        KEEPFILTERS(\n                            FILTER(KEEPFILTERS(__Sample_Categories), [ScalarKey] &lt; EARLIER([ScalarKey], 1))\n                        ),\n                        [ScalarKey]\n                    )\n                    RETURN IF(ISBLANK(__Previous), BLANK(), [ScalarKey] - __Previous)\n            )\n        VAR __Sync_Data =\n            NATURALINNERJOIN(\n                __Sample_Categories,\n                __Data\n            )\n        VAR __MinY_Value = MINX(KEEPFILTERS(__Sync_Data), [Value])\n        VAR __MaxY_Value = MAXX(KEEPFILTERS(__Sync_Data), [Value])\n        VAR __RangeY = (__MaxY_Value - __MinY_Value)\n        VAR __MinX_Value = MINX(KEEPFILTERS(__Sample_Categories), [ScalarKey])\n        VAR __MaxX_Value = MAXX(KEEPFILTERS(__Sample_Categories), [ScalarKey])\n        VAR __Non_Blank_Sync_Data =\n            FILTER(\n                KEEPFILTERS(__Sync_Data),\n                    NOT(ISBLANK([Value]))\n                )\n        VAR __Result =\n            (\"{\"\"p\"\":[\" &amp;\n                CONCATENATEX(\n                    __Sync_Data,\n                    (\"[\" &amp;\n                    FORMAT([ScalarKey], \"General Number\", \"en-US\") &amp;\n                    IF(\n                        ISBLANK([Value]),\n                        \"\",\n                        CONCATENATE(\",\", FORMAT(DIVIDE([Value] - __MinY_Value, __RangeY, 0) * 100, \"0.0\", \"en-US\"))\n                    ) &amp;\n                     \"]\"),\n                    \",\",\n                    [ScalarKey],\n                    ASC\n                ) &amp;\n            \"],\"\"yl\"\":\" &amp;\n            FORMAT(__MinY_Value, \"General Number\", \"en-US\") &amp;\n            \",\"\"yh\"\":\" &amp;\n            FORMAT(__MaxY_Value, \"General Number\", \"en-US\") &amp;\n            \",\"\"xl\"\":\" &amp;\n             FORMAT(__MinX_Value, \"General Number\", \"en-US\") &amp;\n            \",\"\"xh\"\":\" &amp;\n            FORMAT(__MaxX_Value, \"General Number\", \"en-US\") &amp;\n            IF(\n                ISBLANK(__Min_Interval),\n                \"\",\n                CONCATENATE(\",\"\"md\"\":\", FORMAT(__Min_Interval, \"General Number\", \"en-US\"))\n            ) &amp;\n            \"}\")\n        RETURN IF( ISEMPTY(__Non_Blank_Sync_Data), BLANK(), __Result )\n    )\n// USER DAX END\n\nVAR __DS0Core =\n    SUMMARIZECOLUMNS(\n        'Artifact'[ArtifactName],\n        \"Refresh_Barcode_SVG\", 'Progress Report'[Refresh Barcode SVG],\n        \"Sparkline\", 'Progress Report'[Sparkline]\n    )\nVAR __DS0PrimaryWindowed =\n    TOPN(101, __DS0Core, 'Artifact'[ArtifactName], 1)\n\nEVALUATE\n    __DS0PrimaryWindowed\nORDER BY\n    'Artifact'[ArtifactName]\n</code></pre>"},{"location":"posts/SVG-Sparkline/#creating-the-measure","title":"Creating the measure","text":"<p>There seem to be a couple of forms of the sparkline measures, the one above that uses ScalarKey and <code>CROSSJOIN()</code>, and another that uses a GroupIndex and <code>SUBSTITUTEWITHINDEX</code>. The latter is used when more than one value is used for the categories on the Y axis if the sort order of a field depends on another field. I used the latter one, resulting in the following:</p> <pre><code>Refresh SVG Barcode =\nVAR __svgHeight = 20\nVAR __svgWidth = 150\nVAR __Categories = SUMMARIZE( 'Progress Report', 'DateTimes'[Date], 'DateTimes'[DateTime], 'Progress Report'[XmlaRequestId] )\nVAR __Data =\n    ADDCOLUMNS(\n        KEEPFILTERS(\n            FILTER(\n                KEEPFILTERS( __Categories ),\n                not ISBLANK( CALCULATE ( MAX( 'ExecutionMetrics'[Status] ) ) )\n            )\n        ),\n        \"Value\", CALCULATE ( MAX( 'ExecutionMetrics'[Status] ) )\n    )\nVAR __All_Categories =\n    CALCULATETABLE(\n        FILTER(\n            KEEPFILTERS( SUMMARIZE( 'Progress Report', 'DateTimes'[Date], 'DateTimes'[DateTime] ) ),\n            not ISBLANK( CALCULATE ( MAX( 'ExecutionMetrics'[Status] ) ) ) &amp;&amp; not ISBLANK( 'DateTimes'[Date] )\n        ),\n        ALLSELECTED()\n    )\nVAR __Sync_Data =\n    SUBSTITUTEWITHINDEX(\n        __Data\n        ,\"GroupIndex\", __All_Categories\n        ,'DateTimes'[DateTime], ASC\n    )\nVAR __Non_Blank_Sync_Data = FILTER( KEEPFILTERS( __Sync_Data ), not ISBLANK( [Value] ) )        \nVAR __MinX_Value = 0\nVAR __MaxX_Value = COUNTROWS( __All_Categories ) - 1\nVAR __RangeX = __MaxX_Value - __MinX_Value\nVAR __Lines =\n    CONCATENATEX(\n        __Sync_Data,\n        VAR __Value =\n            SWITCH(\n                [Value]\n                ,\"Started\", 1\n                ,\"Succeeded\", 0.7\n                ,\"Failed\", 0.85\n                ,0\n            )\n        VAR _Hex =\n            SWITCH(\n                [Value]\n                ,\"Started\",\"#FFB900\"       // Orange\n                ,\"Failed\", \"#DD6B7F\"       // Red \n                ,\"Succeeded\", \"#37A794\"    // Green\n                ,\"gray\"\n            )\n        VAR _x = FORMAT( DIVIDE( [GroupIndex] - __MinX_Value, __RangeX, 0 ) * 100, \"0.0\", \"en-US\" )\n        RETURN\n        \"&lt;line x1='\" &amp; _x &amp; \"' y1='\" &amp; __svgHeight * __Value &amp; \"' x2='\" &amp; _x &amp; \"' y2='\" &amp; __svgHeight - (__svgHeight * __Value) &amp; \"' stroke='\" &amp; _Hex &amp; \"' stroke-width='2' /&gt;\"\n        ,\",\", [value], ASC\n    )\nVAR __Svg =\n    \"data:image/svg+xml;utf8, &lt;svg width=\"\"\" &amp; __svgWidth &amp; \"\"\" height=\"\"\" &amp; __svgHeight &amp;\"\"\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\" &amp;\n    __Lines &amp;\n    \"&lt;/svg&gt;\"\nRETURN\nIF( not ISEMPTY(__Data) &amp;&amp; not ISEMPTY( __Non_Blank_Sync_Data ), __Svg )\n</code></pre>"},{"location":"posts/SVG-Sparkline/#performance","title":"Performance","text":"<p>All that was left was to test performance, you can see my first attempt was not great, using a lot of formula engine and creating large data cache. The version using the sparkline backbone had fantastic performance.</p> Old PerformanceNew Performance <p></p> <p></p>"},{"location":"posts/SVG-Sparkline/#conclusion","title":"Conclusion","text":"<p>This backbone query used by sparklines is well designed and can and should be used for the generation of SVG visuals.</p>"},{"location":"posts/SVG-Heatmap-Optimized/","title":"Optimizing the SVG Heatmap","text":"<p>A quick post that is the cumulation of a couple of previous posts, optimizing the heatmap SVG using the Power BI Sparklines pattern. </p>"},{"location":"posts/SVG-Heatmap-Optimized/#performance","title":"Performance","text":"<p>Running on the Fabric Log Analytics for Analysis Services Engine report template hydrated with a months worth of logs. I captured the query that generated the following matrix.</p> VisualOptimized Code <p></p> <pre><code>Command CPU (s) Heatmap SVG =\nVAR __svgHeight = 20\nVAR __svgWidth = 150\n\nVAR _numBoxes = 40\nVAR _boxWidth = __svgWidth / _numBoxes\n\nVAR __Categories = VALUES( ExecutionMetrics[XmlaRequestId] )\nVAR __Range =\n    MAXX(\n        ALLSELECTED( ExecutionMetrics[XmlaRequestId] ),\n        CALCULATE(\n            SUM( ExecutionMetrics[totalCpuTimeMs] ) / 1000\n            ,ExecutionMetrics[LogAnalyticsCategory]= \"Command\"\n            ,REMOVEFILTERS( Artifact )\n        )\n    )\n\nVAR __Data =\n    ADDCOLUMNS(\n        KEEPFILTERS(\n            FILTER(\n                KEEPFILTERS( __Categories ),\n                not ISBLANK(\n                    CALCULATE(\n                        SUM( ExecutionMetrics[totalCpuTimeMs] ) / 1000\n                        ,ExecutionMetrics[LogAnalyticsCategory]= \"Command\"\n                    )\n                )\n            )\n        ),\n        \"@box\",\n            INT(\n                (\n                    CALCULATE(\n                        SUM( ExecutionMetrics[totalCpuTimeMs] ) / 1000\n                        ,ExecutionMetrics[LogAnalyticsCategory]= \"Command\"\n                    ) /   __Range\n                ) * (_numBoxes - 1 )\n            ) + 1\n    )\nVAR _countPerBox =\n    ADDCOLUMNS(\n        SUMMARIZE( __Data, [@box] )\n        ,\"@x\", ( [@box] * _boxWidth ) - _boxWidth // could do this elsewhere\n        ,\"@cnt\",\n            VAR _box =  [@box]\n            RETURN\n            CALCULATE( COUNTX ( __Data, IF( [@box] = _box &amp;&amp; _box &lt;&gt; 0, 1 ) ) )\n    )\nVAR _cntRange = MAXX( _countPerBox, [@cnt] )\nVAR __Boxes =\n    CONCATENATEX(\n        ADDCOLUMNS(\n            _countPerBox\n            ,\"@Boxes\"\n            ,// Mapping values range 0 -&gt; 255\n            VAR _inputStart =           0                   // The lowest number of the range input\n            VAR _inputEnd =             LOG( _cntRange, 10 )    // The largest number of the range input\n            VAR _outputStart =          255                            // The lowest number of the range output\n            VAR _outputEnd =            0                     // The largest number of the range output         \n            VAR _outputVal =            _outputStart + ((_outputEnd - _outputStart) / (_inputEnd - _inputStart)) * ( LOG( [@cnt], 10 ) - _inputStart)\n            // https://dax.tips/2019/10/02/dax-base-conversions/\n            VAR ConvertMe = IFERROR( _outputVal, 255 )\n            VAR Base = 16\n            VAR BitTable = GENERATESERIES ( 1, 8 )\n            VAR DEC2HEX =\n                CONCATENATEX(\n                    BitTable,\n                    VAR c = MOD( TRUNC ( ConvertMe / POWER ( base, [value] - 1 ) ), base )\n                    RETURN SWITCH(c,10,\"A\",11,\"B\",12,\"C\",13,\"D\",14,\"E\",15,\"F\",c),\n                    ,[Value],Desc\n                )\n            VAR HEX = \"#\" &amp; REPT( RIGHT( DEC2HEX, 2 ), 3 ) &amp; IF( ConvertMe = 255, \"00\", \"\" )\n            RETURN\n            \"&lt;rect id='box' x='\" &amp; [@x] &amp; \"' y='\" &amp; __svgHeight / 2 &amp; \"' width='\" &amp; _boxWidth &amp; \"' height='\" &amp; __svgHeight / 2 &amp; \"' fill='\" &amp; HEX &amp; \"' filter='url(#gradient)'/&gt;\"\n        )\n        ,[@Boxes]\n        ,\" \"\n        ,[@x]\n    )\n\n// Trends\nvar dt = MAX( Dates[Date] )\nvar _greenHex = \"#37A794\"\nvar _redHex = \"#DD6B7F\"\n\nVAR __MaxVal =\n    MAXX(\n        VALUES( ExecutionMetrics[XmlaRequestId] ),\n        CALCULATE(\n            SUM( ExecutionMetrics[totalCpuTimeMs] ) / 1000\n            ,ExecutionMetrics[LogAnalyticsCategory]= \"Command\"\n        )\n    )\n\n// Avg Trend\nvar _MonthAverageAvg =\n    CALCULATE(\n        AVERAGEX( VALUES( ExecutionMetrics[XmlaRequestId] ), CALCULATE( SUM( ExecutionMetrics[totalCpuTimeMs] ) ) / 1000 )\n        ,REMOVEFILTERS( Dates )\n        ,DATESINPERIOD( Dates[Date], dt, -1, MONTH )\n        ,ExecutionMetrics[LogAnalyticsCategory] = \"Command\"\n    )\nvar _WeekAverageAvg =\n    CALCULATE(\n        AVERAGEX( VALUES( ExecutionMetrics[XmlaRequestId] ), CALCULATE( SUM( ExecutionMetrics[totalCpuTimeMs] ) ) / 1000 )\n        ,REMOVEFILTERS( Dates )\n        ,DATESINPERIOD( Dates[Date], dt, -7, DAY )\n        ,ExecutionMetrics[LogAnalyticsCategory] = \"Command\"\n    )\nVar _trendAvg = IF( not ISBLANK( _MonthAverageAvg ), (1 - ( _WeekAverageAvg / _MonthAverageAvg )) * - 1 )\nVar _trendAvgHex = IF( _trendAvg &gt; 0, _redHex, _greenHex )\n\n// 90 Percentile Trend\nvar _MonthAverage90th =\n    CALCULATE(\n        PERCENTILEX.INC( VALUES( ExecutionMetrics[XmlaRequestId] ), CALCULATE( SUM( ExecutionMetrics[totalCpuTimeMs] ) ) / 1000, 0.9)\n        ,REMOVEFILTERS( Dates )\n        ,DATESINPERIOD( Dates[Date], dt, -1, MONTH )\n        ,ExecutionMetrics[LogAnalyticsCategory] = \"Command\"\n    )\nvar _WeekAverage90th =\n    CALCULATE(\n        PERCENTILEX.INC( VALUES( ExecutionMetrics[XmlaRequestId] ), CALCULATE( SUM( ExecutionMetrics[totalCpuTimeMs] ) ) / 1000, 0.9)\n        ,REMOVEFILTERS( Dates )\n        ,DATESINPERIOD( Dates[Date], dt, -7, DAY )\n        ,ExecutionMetrics[LogAnalyticsCategory] = \"Command\"\n    )\nVar _trend90th = IF( not ISBLANK( _MonthAverage90th ), (1 - ( _WeekAverage90th / _MonthAverage90th )) * - 1 )\nVar _trend90thHex = IF( _trend90th &gt; 0, _redHex, _greenHex )\n\nVAR __Svg =\n    \"data:image/svg+xml;utf8, &lt;svg width=\"\"\" &amp; __svgWidth &amp; \"\"\" height=\"\"\" &amp; __svgHeight &amp;\"\"\" xmlns=\"\"http://www.w3.org/2000/svg\"\"&gt;\" &amp;\n    \"&lt;defs&gt;\n        &lt;filter id=\"\"gradient\"\" color-interpolation-filters=\"\"sRGB\"\"&gt;\n            &lt;feComponentTransfer&gt;\n                &lt;feFuncR type=\"\"table\"\" tableValues=\"\"1 0.975\"\" /&gt;\n                &lt;feFuncG type=\"\"table\"\" tableValues=\"\" 0.78 0.975\"\" /&gt;\n                &lt;feFuncB type=\"\"table\"\" tableValues=\"\"0.2 0.975\"\" /&gt;\n            &lt;/feComponentTransfer&gt;\n        &lt;/filter&gt;\n    &lt;/defs&gt;\" &amp;\n    \"&lt;text x='0' y='7' fill='black' font-size='6' font-family='Segoe UI, sans-serif'&gt;Max&lt;/text&gt;\" &amp;\n    \"&lt;text x='15' y='7' fill='black' font-size='7' font-family='Segoe UI, sans-serif' font-weight='bold'&gt;\" &amp; FORMAT(__maxVal, \"0.0\") &amp; \"&lt;/text&gt;\" &amp;\n    \"&lt;text x='45' y='7' fill='black' font-size='6' font-family='Segoe UI, sans-serif'&gt;Avg Trend&lt;/text&gt;\" &amp;\n    \"&lt;text x='75' y='7' fill='\" &amp; _trendAvgHex &amp; \"' font-size='7' font-family='Segoe UI, sans-serif' font-weight='bold'&gt;\" &amp; FORMAT(_trendAvg, \"0%\") &amp; \"&lt;/text&gt;\" &amp;\n    \"&lt;text x='100' y='7' fill='black' font-size='6' font-family='Segoe UI, sans-serif'&gt;90th Trend&lt;/text&gt;\" &amp;\n    \"&lt;text x='130' y='7' fill='\" &amp; _trend90thHex &amp; \"' font-size='7' font-family='Segoe UI, sans-serif' font-weight='bold'&gt;\" &amp; FORMAT(_trend90th, \"0%\") &amp; \"&lt;/text&gt;\" &amp;\n    __Boxes &amp;\n    \"&lt;/svg&gt;\"\n\nRETURN\nIF( not ISEMPTY(__Data), __Svg )\n</code></pre> <p>Running server timing on DAX Studio you can see that the old code causes the materialization of a large data cache and uses a large amount of formula engine, taking nearly 3 seconds. Adjusting the code to a similar pattern to sparklines we reduce this by 90% to 347ms, with no large materialization.</p> Old PerformanceNew Performance <p></p> <p></p>"},{"location":"posts/SVG-Heatmap-Optimized/#conclusions","title":"Conclusions","text":"<p>This is a fantastic pattern, that is reasonably easy to implement and obtain good results. Of note I test the performance of the Violin plot, and it already very good, and doesn't require any improvement.</p>"},{"location":"posts/SVG-Page-Heatmap/","title":"Visualizing Power BI Visual Performance","text":"<p>While using the Fabric Log Analytics for Analysis Services Engine report template I ran into an annoyance. You can see queries created by visuals, and the logs have a visualId and reportId associated with them, but it is a bit of a pain to figure out what visual that Id relates to.</p>"},{"location":"posts/SVG-Page-Heatmap/#obtaining-visualid","title":"Obtaining VisualId","text":"<p>Firstly VisualId isn't immediately available in Power BI desktop. It is possible to unzip the binary and get the id from the layout json file. With Power BI Developer mode the underlying metadata is now even easier to access. If we want to obtain all the visualId for a entire tenant these options are not appealing. Thankfully there is now a Get Report Definition Fabric API. Additionally Semantic Link, acts as a wrapping making calling the same API a bit simpler.</p>"},{"location":"posts/SVG-Page-Heatmap/#visual-properties","title":"Visual Properties","text":"<p>If we look at the PBIR format, we can see each visual is listed under each page, with all the metadata. </p> <p></p> <p>In addition to the visualId the report metadata also offers a range of other useful properties. Top of the list is x, y, z, height and width. Therefore with a SVG we can draw each visual on a page as a rectangle and colour to display interesting information. For example we could show # user, # errors, # queries CPU usage, or query duration.</p>"},{"location":"posts/SVG-Page-Heatmap/#proof-of-concept","title":"Proof of Concept","text":"<p>To create a quick proof of concept, I pulled the visual metadata from a local pbip folder and created a composite model to the Fabric Log Analytics for Analysis Services Engine report. </p> <p></p> <p>I created a report page and SVG measure. I then added the SVG measure to a tooltip to get the following result.</p> VisualCode <p></p> <pre><code>Page SVG =\nVAR __pageHeight = SELECTEDVALUE( 'Visual Properties'[Page height] )\nVAR __pageWidth = SELECTEDVALUE( 'Visual Properties'[Page width] )\nVAR __svgHeight = 200\nVAR __svgWidth = __pageWidth * ( __svgHeight / __pageHeight )\nVAR __selectedVisual = SELECTEDVALUE( 'Visual Properties'[Visual name]  )\nVAR __Range =\n    MAXX(\n        ALLSELECTED( 'Execution Metrics'[XmlaRequestId] ),\n        CALCULATE(\n            SUM( 'Execution Metrics'[totalCpuTimeMs] ) / 1000\n            ,'Execution Metrics'[LogAnalyticsCategory]= \"Query\"\n            ,REMOVEFILTERS( Artifact )\n        )\n    )\nVAR __page = \"&lt;rect id='Page' width='\" &amp; __svgWidth &amp; \"' height='\" &amp; __svgHeight &amp; \"' fill='none' stroke='black' /&gt;\"\nVAR _visualFill =\n    CONCATENATEX(\n        ADDCOLUMNS(\n            ALLSELECTED( 'Visual Properties'[Visual name], 'Visual Properties'[z] )\n            ,\"@visual\"\n            ,var x =            CALCULATE( SELECTEDVALUE( 'Visual Properties'[x] ) * ( __svgWidth / __pageWidth ), ALLSELECTED( 'Visual Properties'[Visual name] ) )\n            var y =             CALCULATE( SELECTEDVALUE( 'Visual Properties'[y] ) * ( __svgHeight / __pageHeight ), ALLSELECTED( 'Visual Properties'[Visual name] ) )\n            var width =         CALCULATE( SELECTEDVALUE( 'Visual Properties'[width] ) * ( __svgWidth / __pageWidth ), ALLSELECTED( 'Visual Properties'[Visual name] ) )\n            var height =        CALCULATE( SELECTEDVALUE( 'Visual Properties'[height] ) * ( __svgHeight / __pageHeight ), ALLSELECTED( 'Visual Properties'[Visual name] ) )\n            VAR _inputStart =           0                       // The lowest number of the range input\n            VAR _inputEnd =             __Range                 // The largest number of the range input\n            VAR _outputStart =          255                     // The lowest number of the range output\n            VAR _outputEnd =            0                       // The largest number of the range output        \n            VAR _outputVal =            _outputStart + ((_outputEnd - _outputStart) / (_inputEnd - _inputStart)) * ( [Avg Duration (s)] - _inputStart)\n            // https://dax.tips/2019/10/02/dax-base-conversions/\n            VAR ConvertMe = IFERROR( _outputVal, 255 )\n            VAR Base = 16\n            VAR BitTable = GENERATESERIES ( 1, 8 )\n            VAR DEC2HEX =\n                CONCATENATEX(\n                    BitTable,\n                    VAR c = MOD( TRUNC ( ConvertMe / POWER ( base, [value] - 1 ) ), base )\n                    RETURN SWITCH(c,10,\"A\",11,\"B\",12,\"C\",13,\"D\",14,\"E\",15,\"F\",c),\n                    ,[Value],Desc\n                )\n            VAR fillHex =\n                IF(\n                    'Visual Properties'[Visual name] = __selectedVisual\n                    || not ISINSCOPE( 'Visual Properties'[Visual name] )\n                    , \"#\" &amp; REPT( RIGHT( DEC2HEX, 2 ), 3 ) &amp; IF( ConvertMe = 255, \"00\", \"\" )\n                    , \"none\"\n                )\n            return\n            \"&lt;rect id='Page' x = '\" &amp; x &amp; \"' y = '\" &amp; y &amp; \"' width='\" &amp; width &amp; \"' height='\" &amp; height &amp; \"' fill='\" &amp; fillHex &amp; \"' filter='url(#gradient)' /&gt;\"\n        )\n        ,[@visual]\n        ,\" \"\n        ,'Visual Properties'[z]\n        ,ASC\n    )\nVAR _visualoutline =\n    CONCATENATEX(\n        ADDCOLUMNS(\n            ALLSELECTED( 'Visual Properties'[Visual name], 'Visual Properties'[z] )\n            ,\"@visual\"\n            ,var x =            CALCULATE( SELECTEDVALUE( 'Visual Properties'[x] ) * ( __svgWidth / __pageWidth ), ALLSELECTED( 'Visual Properties'[Visual name] ) )\n            var y =             CALCULATE( SELECTEDVALUE( 'Visual Properties'[y] ) * ( __svgHeight / __pageHeight ), ALLSELECTED( 'Visual Properties'[Visual name] ) )\n            var width =         CALCULATE( SELECTEDVALUE( 'Visual Properties'[width] ) * ( __svgWidth / __pageWidth ), ALLSELECTED( 'Visual Properties'[Visual name] ) )\n            var height =        CALCULATE( SELECTEDVALUE( 'Visual Properties'[height] ) * ( __svgHeight / __pageHeight ), ALLSELECTED( 'Visual Properties'[Visual name] ) )\n            var strokeHex =     IF( 'Visual Properties'[Visual name] = __selectedVisual, \"green\", \"gray\" )\n            var strokeWidth =   IF( 'Visual Properties'[Visual name] = __selectedVisual, \"2\", \"1\" )\n            return\n            \"&lt;rect id='Page' x = '\" &amp; x &amp; \"' y = '\" &amp; y &amp; \"' width='\" &amp; width &amp; \"' height='\" &amp; height &amp; \"' fill='none' stroke='\" &amp; strokeHex &amp; \"' stroke-width='\" &amp; strokeWidth &amp; \"'/&gt;\"\n        )\n        ,[@visual]\n        ,\" \"\n        ,'Visual Properties'[z]\n        ,ASC\n    )\nRETURN\n\"data:image/svg+xml;utf8, &lt;svg width=\"\"\" &amp; __svgWidth &amp; \"\"\" height=\"\"\" &amp; __svgHeight &amp;\"\"\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\" &amp;\n\"&lt;defs&gt;\n    &lt;filter id=\"\"gradient\"\" color-interpolation-filters=\"\"sRGB\"\"&gt;\n        &lt;feComponentTransfer&gt;\n            &lt;feFuncR type=\"\"table\"\" tableValues=\"\"0.020 0.975\"\" /&gt;\n            &lt;feFuncG type=\"\"table\"\" tableValues=\"\" 0.776 0.975\"\" /&gt;\n            &lt;feFuncB type=\"\"table\"\" tableValues=\"\"0.733 0.975\"\" /&gt;\n        &lt;/feComponentTransfer&gt;\n    &lt;/filter&gt;\n&lt;/defs&gt;\" &amp;\n__Page &amp;\n_visualFill &amp;\n_visualoutline &amp;\n\"&lt;/svg&gt;\n</code></pre>"},{"location":"posts/SVG-Page-Heatmap/#next-steps","title":"Next Steps","text":"<p>To fully deploy this solution you'd likely want field parameters or calculations groups allow the SVG to show different measures. Additionally you'd want to leverage the Fabric API to pull the metadata for the entire tenant. One thing you'd want to think about is what you want to happen if a visual is deleted.</p>"},{"location":"posts/IncrementalRefersh-Parameters/","title":"Keeping Power BI Incremental Refresh Up To Date","text":"<p>I was inspired to write this blog post after seeing a linkedin post from Nikolaos Christoforidis. </p> <p></p> <p>He was trying to address a problem when developing models with Incremental Refresh, whereby the parameter required for Incremental Refresh can be a headache when developing locally, as you have to keep redefining <code>RangeStart</code> and <code>RangeEnd</code> to get up-to-date data. He suggested using dynamic M parameters, Pavel A. mentioned these may fail in some scenarios, e.g. Dataflows GEN1. This post proposes another method to solve this problem.</p>"},{"location":"posts/IncrementalRefersh-Parameters/#power-bi-incremental-refresh","title":"Power BI Incremental Refresh","text":"<p>A SSAS Tabular table is made up of one or more partitions. When you first create a table in Power BI it will have one partition. You can create additional partitions with TMSL and the XMLA endpoint, or Microsoft.AnalysisServices.Tabular client library, or tool such as Tabular Editor and SQL Server Management Studio (SSMS) that leverage these. These additional partitions will have the same schema, but be filtered to include different data (i.e. 2020, 2021, 2022). Each partition can then be processed (Refreshed) independent of other partitions and in parallel.</p> <p>Normally with partitions you are responsible for creating and managing them. Incremental Refresh is a feature in the Power BI service that automates partition creation and management. It is configured by the creation of a Refresh Policy for a table, defining rolling time windows for hot and cold data, and data eviction. Tabular Editor has some very good docs on configuration and gotchas for this feature.</p>"},{"location":"posts/IncrementalRefersh-Parameters/#problemsolution","title":"Problem/Solution","text":"<p>When you are working on a Power BI file locally you will have static <code>RangeStart</code> and <code>RangeEnd</code> parameters filtering a table to a given data range.</p> RangeStartRangeEndTable <pre><code>#datetime(2022, 12, 01, 0, 0, 0) meta [IsParameterQuery = true, IsParameterQueryRequired = true, Type = type datetime]\n</code></pre> <pre><code>#datetime(2022, 12, 31, 0, 0, 0) meta [IsParameterQuery = true, IsParameterQueryRequired = true, Type = type datetime]\n</code></pre> <pre><code>let\n    source = .....,\n    incrementalRefresh = Table.SelectRows(source, each [date] &gt;= #\"RangeStart\" and [date] &lt; #\"RangeEnd\")\nin \n    incrementalRefresh\n</code></pre> <p>If you haven't worked on the Report in a while, these dates might be from months ago, but you want to look at new data. You can update <code>RangeStart</code> and <code>RangeEnd</code> with new dates, but there is another solution. Nikolaos Christoforidis's solution is to have dynamic parameters that provides the previous 3 month of data. </p> RangeStartRangeEnd <pre><code>DateTime.From(Date.StartOfMonth(Date.AddMonths(DateTime.LocalNow(), -3))) meta [IsParameterQuery = true, IsParameterQueryRequired = true, Type = type datetime]\n</code></pre> <pre><code>DateTime.From(Date.EndOfMonth(DateTime.LocalNow())) meta [IsParameterQuery = true, IsParameterQueryRequired = true, Type = type datetime]\n</code></pre> <p>But this has the issues with dataflow Dataflows GEN1 mentioned before. Instead we are able keep <code>RangeStart</code> and <code>RangeEnd</code> static, and move the logic to return the most recent 3 month to the table. We can leverage the fact that the Power BI service will hijack the <code>RangeStart</code> and <code>RangeEnd</code> parameters to filter the data for a data range for each partition, and that we only want the most recent 3 months locally. Firstly we want to identify a date that precedes the window of our Incremental Refresh window. For example if we are archiving 2 years of data we could pick <code>01-01-2020</code>. In the table definition we can look out for this date and filter to the last 3 month, else enact the regular incremental refresh pattern.</p> <pre><code>// table\nlet\n    source = .....,\n    threeMonthsAgo = DateTime.From(Date.StartOfMonth(Date.AddMonths(DateTime.LocalNow(), -3))),\n    now = DateTime.From(Date.EndOfMonth(DateTime.LocalNow())),\n    lastThreemonths = Table.SelectRows(data, each [date] &gt;= threeMonthsAgo and [date] &lt; now),\n    incrementalRefresh = Table.SelectRows(#\"Changed Type\", each [Date] &gt;= #\"RangeStart\" and [Date] &lt; #\"RangeEnd\"),\n    selectPath = if #\"RangeStart\" = DateTime.FromText(\"01/01/2020 00:00:00\") then lastThreemonths else incrementalRefresh\nin \n    selectPath\n</code></pre> <p>Now if we set <code>RangeStart</code> to <code>01/01/2020 00:00:00</code>, this will mean we will return <code>lastThreemonths</code>. In the service based on our Refresh Policy, this value will not be injected by the service and the incrementalRefresh path will be taken instead.</p>"},{"location":"posts/IncrementalRefersh-Parameters/#conclusion","title":"Conclusion","text":"<p>This pattern of using a value in parameter to provide a behavior quite a useful pattern in the DevOps space as well. For example, we can define a <code>TopN</code> parameter, this could be set locally to 1000, this means locally locally or in a deployment to dev environment we to test the schema and connectivity without performing a large refresh. As part of the DevOps process when deploying to to a UAT or Prod environment we can use the Update Parameters In Group REST API, or script the update the PBIP files in the deployment to overwrite the parameters to include all the data.</p>"},{"location":"posts/Deneb-Gantt/","title":"Deneb Gantt Chart","text":"<p>The Fabric Log Analytics for Analysis Services Engine report template has Gantt chart presumably inspired by Phil Seamark blog. The problem is the template uses Craydec timeline visual which is not certified and therefore I can't use it at work. Annoyingly, all the other Gantt charts on the App store seem only work down to a day level, not the hours, minutes and seconds that would represent a refresh operation. Fortunately Deneb is a certified visual with a alot of flexibility and possibility, so I had a stab at creating a replica. My first though was to modify David Bacci incredible Gantt Chart, but as I only just starting out with Deneb, it was a little too overwhelming and complicated for my simple use case, so I started from scratch with Vega-lite.</p> <p>Firstly why? Power BI has two part, the report and the semantic model. The semantic model is a instance of SSAS Tabular, an in-memory columnar database. Unlike a transitional database that are row-oriented, aiming for rapid transaction on individuals rows, SSAS Tabular is instead column-oriented, aiming to performance aggregations over large number of rows to answer analytical queries. A Semantic Model has 1 or more tables and those each have 1 or more partition. During a refresh: </p> <ol> <li>Data is read from the data source</li> <li>Data is compressed (row-length encoding) and encoded (hash/value encoding) into columns</li> <li>Metadata is calculated, such as relationships between tables and calculated tables/columns</li> </ol> <p>Partitions can be refresh in parallel, and the amount of parallelism can be set see Chris Webb.</p> <p>By looking at all the operations on the partitions of the semantic model as a gantt chart we are able to determine if there are any bottlenecks in a slow refresh, which informs any optimization work.</p> <p>The Craytec timelines Gantt looks like this for a refresh:</p> <p></p> <p>And this is what I managed in a afternoon with Deneb.</p> <p>I did run into some quirks of the framework</p> <ol> <li>White space in a label is trimmed therefore I used a NO-BREAK SPACE instead of regular space to pad text</li> <li>In the refresh several operations occur at the same time for the partition, to have the bars overlap you first need to set <code>stack = null</code>and in order to get the appropriate z-order, avoiding a large bar obscuring others I had to set <code>\"order\": {\"aggregate\": \"max\", \"field\": \"startTime\"}</code>, to bubble the hidden bars to the top</li> </ol> VisualSpecConfig <p></p> <pre><code>{\n  \"data\": {\n    \"name\": \"dataset\"\n  },\n  \"transform\": [\n    {\n      \"calculate\": \"truncate(datum.TableName, 25) + ' | ' + pad(truncate(datum.ObjectName, 25), 25, ' ', 'left') \",\n      \"as\": \"Table|Partition\"\n    }\n  ],\n  \"height\": {\n    \"step\": 8\n  },\n  \"mark\": \"bar\",\n  \"encoding\": {\n    \"y\": {\n      \"field\": \"Table|Partition\",\n      \"axis\": {\n        \"labelLimit\": 400,\n        \"labelFont\": \"Courier New\"\n      },\n      \"type\": \"nominal\",\n      \"sort\": \"x\"\n    },\n    \"x\": {\n      \"field\": \"startTime\",\n      \"type\": \"temporal\",\n      \"stack\": null,\n      \"axis\": {\n        \"orient\": \"top\",\n        \"format\": \"%H:%M:%S\",\n        \"formatType\": \"time\"\n      }\n    },\n    \"x2\": {\n      \"field\": \"endTime\"\n    },\n    \"color\": {\n      \"field\": \"OperationDetailName\",\n      \"type\": \"ordinal\",\n      \"scale\": {\n        \"scheme\": \"pbiColorNominal\"\n      }\n    },\n    \"order\": {\n      \"aggregate\": \"max\",\n      \"field\": \"startTime\"\n    },\n    \"tooltip\": [\n      {\n        \"field\": \"OperationDetailName\",\n        \"title\": \"Operation\"\n      },\n      {\n        \"field\": \"TableName\",\n        \"title\": \"Table\"\n      },\n      {\n        \"field\": \"ObjectName\",\n        \"title\": \"Partition\"\n      },\n      {\n        \"field\": \"startTime\",\n        \"type\": \"temporal\",\n        \"timeUnit\": \"yearmonthdate\",\n        \"title\": \"Start\"\n      },\n      {\n        \"field\": \"endTime\",\n        \"type\": \"temporal\",\n        \"timeUnit\": \"hoursminutesseconds\",\n        \"title\": \"End\"\n      }\n    ]\n  }\n}\n</code></pre> <pre><code>{\n  \"autosize\": {\n    \"contains\": \"padding\",\n    \"type\": \"fit\"\n  },\n  \"view\": {\n    \"stroke\": \"transparent\"\n  },\n  \"font\": \"Segoe UI\",\n  \"arc\": {},\n  \"area\": {\n    \"line\": true,\n    \"opacity\": 0.6\n  },\n  \"bar\": {},\n  \"line\": {\n    \"strokeWidth\": 3,\n    \"strokeCap\": \"round\",\n    \"strokeJoin\": \"round\"\n  },\n  \"point\": {\n    \"filled\": true,\n    \"size\": 75\n  },\n  \"rect\": {},\n  \"text\": {\n    \"font\": \"Segoe UI\",\n    \"fontSize\": 12,\n    \"fill\": \"#605E5C\"\n  },\n  \"axis\": {\n    \"ticks\": false,\n    \"grid\": false,\n    \"domain\": false,\n    \"labelColor\": \"#605E5C\",\n    \"labelFontSize\": 8,\n    \"title\": null,\n    \"titleFont\": \"din\",\n    \"titleColor\": \"#252423\",\n    \"titleFontSize\": 10,\n    \"titleFontWeight\": \"normal\"\n  },\n  \"axisQuantitative\": {\n    \"tickCount\": 3,\n    \"grid\": true,\n    \"gridColor\": \"#C8C6C4\",\n    \"gridDash\": [\n      1,\n      5\n    ],\n    \"labelFlush\": false\n  },\n  \"axisX\": {\n    \"labelPadding\": 5\n  },\n  \"axisY\": {\n    \"labelPadding\": 10\n  },\n  \"header\": {\n    \"titleFont\": \"din\",\n    \"titleFontSize\": 16,\n    \"titleColor\": \"#252423\",\n    \"labelFont\": \"Segoe UI\",\n    \"labelFontSize\": 13.333333333333332,\n    \"labelColor\": \"#605E5C\"\n  },\n  \"legend\": {\n    \"titleFont\": \"Segoe UI\",\n    \"titleFontWeight\": 200,\n    \"title\": null,\n    \"titleColor\": \"#605E5C\",\n    \"labelFont\": \"Segoe UI\",\n    \"labelFontSize\": 9,\n    \"labelColor\": \"#605E5C\",\n    \"symbolType\": \"circle\",\n    \"symbolSize\": 40,\n    \"orient\": \"top\"\n  },\n  \"path\": {},\n  \"shape\": {},\n  \"symbol\": {\n    \"strokeWidth\": 1.5,\n    \"size\": 50\n  }\n}\n</code></pre>"},{"location":"posts/Deneb-Gantt/#conclusion","title":"Conclusion","text":"<p>Deneb is a great tool, bring the frameworks of vega and vega-lite to Power BI. There is a steep learning curve, but that is mostly related to volume of knowledge required, knowing the schema of framework and what is possible. Will have to try and find more time to look at Deneb more in the future.</p>"},{"location":"posts/vega-rendering/","title":"Vega Embed","text":"<p>If I want to blog about Vega I want to be able to render the visuals on the post for full interactivity rather than use static images. In that vein I enabled Vega-Embed for the blog. Vega-Embed automatically renders img from the given spec, and adds the ability to export the graph as a image, view the source/compiled spec, or open the spec in Vega Editor. Lets have a look at how to enable and use it.</p>"},{"location":"posts/vega-rendering/#load-the-vega-libraries","title":"Load The Vega Libraries","text":"<p>Add the following to the html head.</p> <pre><code>...\n&lt;script src=\"https://cdn.jsdelivr.net/npm/vega@5\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/vega-lite@5\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"&gt;&lt;/script&gt;\n...\n</code></pre>"},{"location":"posts/vega-rendering/#render-vega-on-post","title":"Render Vega on Post","text":"<p>Add a script that calls the <code>vegaEmbed()</code> function in the post, passing a <code>spec</code>.</p>"},{"location":"posts/vega-rendering/#embedded-data-and-spec","title":"Embedded data and spec","text":"VisualIn-line SpecRemote Spec <p>{   \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",   \"description\": \"A simple bar chart with embedded data.\",   \"width\": 400,   \"height\": 200,   \"data\": {     \"values\": [       {\"a\": \"A\", \"b\": 28},       {\"a\": \"B\", \"b\": 55},       {\"a\": \"C\", \"b\": 43},       {\"a\": \"D\", \"b\": 91},       {\"a\": \"E\", \"b\": 81},       {\"a\": \"F\", \"b\": 53},       {\"a\": \"G\", \"b\": 19},       {\"a\": \"H\", \"b\": 87},       {\"a\": \"I\", \"b\": 52}     ]   },   \"mark\": \"bar\",   \"encoding\": {     \"x\": {\"field\": \"a\", \"type\": \"ordinal\"},     \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}   } }</p> <pre><code>&lt;div id=\"vis\"&gt;&lt;/div&gt;\n&lt;script type=\"text/javascript\"&gt;\n  var spec = {\n    $schema: 'https://vega.github.io/schema/vega-lite/v5.json',\n    description: 'A simple bar chart with embedded data.',\n    width: 400,\n    height: 200,\n    data: {\n      values: [\n        {a: 'A', b: 28},\n        {a: 'B', b: 55},\n        {a: 'C', b: 43},\n        {a: 'D', b: 91},\n        {a: 'E', b: 81},\n        {a: 'F', b: 53},\n        {a: 'G', b: 19},\n        {a: 'H', b: 87},\n        {a: 'I', b: 52}\n      ]\n    },\n    mark: 'bar',\n    encoding: {\n      x: {field: 'a', type: 'ordinal'},\n      y: {field: 'b', type: 'quantitative'}\n    }\n  };\n  vegaEmbed('#vis', spec);\n&lt;/script&gt;\n</code></pre> <pre><code>&lt;div id=\"vis\"&gt;&lt;/div&gt;\n&lt;script type=\"text/javascript\"&gt;\n  var spec = \"bar.v1.json\";\n  vegaEmbed('#vis', spec).then(function(result) {}).catch(console.error);\n&lt;/script&gt;\n</code></pre>"},{"location":"posts/vega-rendering/#interactive-visual","title":"Interactive Visual","text":"<p>Lets quickly test one of the examples provided by vega with some interactive elements.</p> <p>{   \"$schema\": \"https://vega.github.io/schema/vega/v5.json\",   \"width\": 700,   \"height\": 500,   \"padding\": 0,   \"autosize\": \"none\",   \"signals\": [     {\"name\": \"cx\", \"update\": \"width / 2\"},     {\"name\": \"cy\", \"update\": \"height / 2\"},     {       \"name\": \"nodeRadius\",       \"value\": 8,       \"bind\": {\"input\": \"range\", \"min\": 1, \"max\": 50, \"step\": 1}     },     {       \"name\": \"nodeCharge\",       \"value\": -30,       \"bind\": {\"input\": \"range\", \"min\": -100, \"max\": 10, \"step\": 1}     },     {       \"name\": \"linkDistance\",       \"value\": 30,       \"bind\": {\"input\": \"range\", \"min\": 5, \"max\": 100, \"step\": 1}     },     {\"name\": \"static\", \"value\": true, \"bind\": {\"input\": \"checkbox\"}},     {       \"description\": \"State variable for active node fix status.\",       \"name\": \"fix\",       \"value\": 0,       \"on\": [         {           \"events\": \"symbol:mouseout[!event.buttons], window:mouseup\",           \"update\": \"0\"         },         {\"events\": \"symbol:mouseover\", \"update\": \"fix || 1\"},         {           \"events\": \"[symbol:mousedown, window:mouseup] &gt; window:mousemove!\",           \"update\": \"2\",           \"force\": true         }       ]     },     {       \"description\": \"Graph node most recently interacted with.\",       \"name\": \"node\",       \"value\": null,       \"on\": [         {\"events\": \"symbol:mouseover\", \"update\": \"fix === 1 ? item() : node\"}       ]     },     {       \"description\": \"Flag to restart Force simulation upon data changes.\",       \"name\": \"restart\",       \"value\": false,       \"on\": [{\"events\": {\"signal\": \"fix\"}, \"update\": \"fix &gt; 1\"}]     }   ],   \"data\": [     {       \"name\": \"node-data\",       \"url\": \"https://raw.githubusercontent.com/vega/vega/master/docs/data/miserables.json\",       \"format\": {\"type\": \"json\", \"property\": \"nodes\"}     },     {       \"name\": \"link-data\",       \"url\": \"https://raw.githubusercontent.com/vega/vega/master/docs/data/miserables.json\",       \"format\": {\"type\": \"json\", \"property\": \"links\"}     }   ],   \"scales\": [     {\"name\": \"color\", \"type\": \"ordinal\", \"range\": {\"scheme\": \"category20c\"}}   ],   \"marks\": [     {       \"name\": \"nodes\",       \"type\": \"symbol\",       \"zindex\": 1,       \"from\": {\"data\": \"node-data\"},       \"on\": [         {           \"trigger\": \"fix\",           \"modify\": \"node\",           \"values\": \"fix === 1 ? {fx:node.x, fy:node.y} : {fx:x(), fy:y()}\"         },         {\"trigger\": \"!fix\", \"modify\": \"node\", \"values\": \"{fx: null, fy: null}\"}       ],       \"encode\": {         \"enter\": {           \"fill\": {\"scale\": \"color\", \"field\": \"group\"},           \"stroke\": {\"value\": \"white\"},           \"tooltip\": {\"signal\": \"datum.name\"}         },         \"update\": {           \"size\": {\"signal\": \"2 * nodeRadius * nodeRadius\"},           \"cursor\": {\"value\": \"pointer\"}         }       },       \"transform\": [         {           \"type\": \"force\",           \"iterations\": 300,           \"restart\": {\"signal\": \"restart\"},           \"static\": {\"signal\": \"static\"},           \"forces\": [             {\"force\": \"center\", \"x\": {\"signal\": \"cx\"}, \"y\": {\"signal\": \"cy\"}},             {\"force\": \"collide\", \"radius\": {\"signal\": \"nodeRadius\"}},             {\"force\": \"nbody\", \"strength\": {\"signal\": \"nodeCharge\"}},             {               \"force\": \"link\",               \"links\": \"link-data\",               \"distance\": {\"signal\": \"linkDistance\"}             }           ]         }       ]     },     {       \"type\": \"path\",       \"from\": {\"data\": \"link-data\"},       \"interactive\": false,       \"encode\": {         \"update\": {\"stroke\": {\"value\": \"#ccc\"}, \"strokeWidth\": {\"value\": 0.5}}       },       \"transform\": [         {           \"type\": \"linkpath\",           \"shape\": \"line\",           \"sourceX\": \"datum.source.x\",           \"sourceY\": \"datum.source.y\",           \"targetX\": \"datum.target.x\",           \"targetY\": \"datum.target.y\"         }       ]     }   ] }</p>"},{"location":"posts/vega-rendering/#embed-options","title":"Embed Options","text":"<p>You can also specify a number of Options in the <code>vegaEmbed()</code> function, such as Themes, Vega tooltips, renderer ['svg', 'canvas'], width,  height, etc.</p>"},{"location":"posts/vega-rendering/#themes","title":"Themes","text":"<p>I've seen a <code>powerbi</code> theme, so we have to give that a go.</p> VisualSpec <p>{   \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",   \"description\": \"A simple bar chart with PowerBI theme.\",   \"width\": 400,   \"height\": 200,   \"data\": {     \"values\": [       {\"a\": \"A\", \"b\": 28},       {\"a\": \"B\", \"b\": 55},       {\"a\": \"C\", \"b\": 43},       {\"a\": \"D\", \"b\": 91},       {\"a\": \"E\", \"b\": 81},       {\"a\": \"F\", \"b\": 53},       {\"a\": \"G\", \"b\": 19},       {\"a\": \"H\", \"b\": 87},       {\"a\": \"I\", \"b\": 52}     ]   },   \"mark\": \"bar\",   \"encoding\": {     \"x\": {\"field\": \"a\", \"type\": \"ordinal\"},     \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}   },   \"config\": {     \"view\": {\"stroke\": \"transparent\"},     \"font\": \"Segoe UI\",     \"arc\": {\"fill\": \"#118DFF\"},     \"area\": {\"fill\": \"#118DFF\"},     \"line\": {\"stroke\": \"#118DFF\", \"strokeWidth\": 2},     \"path\": {\"stroke\": \"#118DFF\"},     \"rect\": {\"fill\": \"#118DFF\"},     \"shape\": {\"stroke\": \"#118DFF\"},     \"bar\": {\"fill\": \"#118DFF\"},     \"point\": {\"stroke\": \"#118DFF\"}   } }</p> <pre><code>&lt;div id=\"vis\"&gt;&lt;/div&gt;\n&lt;script type=\"text/javascript\"&gt;\n  var spec = {\n    $schema: 'https://vega.github.io/schema/vega-lite/v5.json',\n    description: 'A simple bar chart with embedded data.',\n    width: 400,\n    height: 200,\n    data: {\n      values: [\n        {a: 'A', b: 28},\n        {a: 'B', b: 55},\n        {a: 'C', b: 43},\n        {a: 'D', b: 91},\n        {a: 'E', b: 81},\n        {a: 'F', b: 53},\n        {a: 'G', b: 19},\n        {a: 'H', b: 87},\n        {a: 'I', b: 52}\n      ]\n    },\n    mark: 'bar',\n    encoding: {\n      x: {field: 'a', type: 'ordinal'},\n      y: {field: 'b', type: 'quantitative'}\n    }\n  };\n  vegaEmbed('#vis', spec, {theme: 'powerbi'});\n&lt;/script&gt;\n</code></pre>"},{"location":"posts/graphframes/","title":"Who Actually Has Access To What In Power BI?","text":"<p>Giving permissions to users to Power BI content should be easy right? What about when you have a bunch of nested AAD groups? If I add a user to a group, what permissions will they actually be granted? In this solution I am using the Power BI Scanner APIs, Graph APIs and GraphFrames to generate a graph to disseminate Access Roles from Workspaces, Reports and Semantic Models directly granted to AAD groups, to all downstream members.</p>"},{"location":"posts/graphframes/#service-principal","title":"Service Principal","text":"<p>To call Scan API and Graph API you will ideally need a Service Principal, which must not have any admin-consent required permissions. The following scopes are required.</p> Service Scope Power BI <code>Tenant.Read.All</code> or <code>Tenant.ReadWrite.All</code> Graph API <code>Directory.Read.All</code>"},{"location":"posts/graphframes/#scanner-apis","title":"Scanner APIs","text":"<p>Microsoft Docs: Run metadata scanning</p> <p>With the scanner APIs, you can extract information such as item name, owner, sensitivity label, and endorsement status. For Power BI semantic models, you can also extract the metadata of some of the objects they contain, such as table and column names, measures, DAX expressions, mashup queries, and so forth. The metadata of these semantic model internal objects is referred to as subartifact metadata.</p> <p>-- Microsoft Docs: Run metadata scanning</p> <p>The following APIs are used return all the metadata for the Power BI service</p> API Function GetModifiedWorkspaces Return workspaceIds PostWorkspaceInfo Starts a scan. Accepts batches of 1-100 workspaces GetScanStatus Checks status of scan GetScanResult Returns scan results"},{"location":"posts/graphframes/#considerations-and-limitations","title":"Considerations and limitations","text":"<p>Microsoft Docs: Run metadata scanning</p> <ul> <li>Semantic models that haven't been refreshed or republished will be returned in API responses but without their subartifact information and expressions. For example, semantic model name and lineage are included in the response, but not the semantic model's table and column names.</li> <li>Semantic models containing only DirectQuery tables will return subartifact metadata only if some sort of action has been taken on the semantic model, such as someone building a report on top of it, someone viewing a report based on it, etc.</li> <li>Real-time datasets, semantic models with object-level security, semantic models with a live connection to AS-Azure and AS on-premises, and Excel full fidelity datasets aren't supported for subartifact metadata. For unsupported datasets, the response returns the reason for not getting the subartifact metadata from the dataset. It's found in a field named schemaRetrievalError, for example, schemaRetrievalError: Unsupported request. RealTime dataset are not supported.</li> <li>The API doesn't return subartifact metadata for semantic models that are larger than 1 GB in shared workspaces. In Premium workspaces, there's no size limitation on semantic models.</li> </ul> <p>-- Microsoft Docs: Run metadata scanning</p>"},{"location":"posts/graphframes/#schema","title":"Schema","text":"<p>Limited schema</p> <p>I have only setup the Power BI schema for entities I care about, <code>datasets</code> and <code>reports</code>, there are also structures for <code>dataflows</code>, <code>notebooks</code>, <code>dashboards</code>, <code>datamarts</code>, <code>DataPipelines</code>, <code>Reflex</code> etc. Since these are not/rarely used, these have not been built into the schema. Additional some fields like <code>schemaRetrievalError</code> have also not been considered. See Sandeep Pawar blog for some other items. Additionally if you use different connectors you might need to extent the connectionDetails.</p> <p>App</p> <p>There is no specific Power BI object for a workspace App. When you create a App you a copy of a report is generated, named <code>[App] ...</code>. You therefore need to look at reportUserAccessRight to determine App permissions.</p>"},{"location":"posts/graphframes/#graph-apis","title":"Graph APIs","text":"API Function ListGroups Return AAD groups ListUsers Return AAD Users ListApps Return AAD Apps"},{"location":"posts/graphframes/#graphframes","title":"GraphFrames","text":"<p>Graphframes</p> <p>GraphFrames is a package for Apache Spark which provides DataFrame-based Graphs. It provides high-level APIs in Scala, Java, and Python. It aims to provide both the functionality of GraphX and extended functionality taking advantage of Spark DataFrames. This extended functionality includes motif finding, DataFrame-based serialization, and highly expressive graph queries.</p> <p>-- GraphFrames Overview</p> <p>Graphs represent data as a set of vertices (nodes/entities) and edges (connections between nodes/entities). GraphFrames works on top of Spark Dataframes, and therefore easily fit into a Databricks/Fabric workflow. Vertices are defined by a dataframe with a <code>id</code> field and Edges as another dataframe with <code>src</code> and <code>dst</code> fields. <code>src</code> (source) and <code>dst</code> (destination) are the directional relationship between two vertices. Graphframes supports Scala, Java, and Python. Out of the box we get motif pattern matching, and a range of graph algorithms are provided, plus you can write your own with Pregel.</p> <p> Example Graph</p>"},{"location":"posts/graphframes/#pregel","title":"Pregel","text":"<p>In order to traverse the graph and disseminate Access Roles to ADD groups and users we are going to use Pregel. Pregel was originally developed by google as a method to rank Web Pages with the PageRank algorithm. In essence the graph is processed in a number of supersets. Within each superset, vertices emit a message along it's edges to neighboring vertices. Destination vertices can have many incoming edges, therefore the messages are aggregated. Then other superset occurs. This occurs until a max number of defined supersets are complete or a stop condition is met.</p> <p>Pregel stop conditions</p> <p>Graphframes only supports a stopping based on a defined number of iterations <code>setMaxIter(n)</code>. Stop conditions are not supported.</p>"},{"location":"posts/graphframes/#running-graphframes","title":"Running GraphFrames","text":"<p>GraphFrames is package for Apache Spark. To use it you need to install the .jar file.  I am running on databricks ML runtime which includes GraphFrames as default. I found a guide on how to do install the .jar file on Fabric.</p>"},{"location":"posts/graphframes/#notebook-script","title":"Notebook Script","text":""},{"location":"posts/graphframes/#setup","title":"Setup","text":""},{"location":"posts/graphframes/#dependencies","title":"Dependencies","text":"<pre><code>import json\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nimport requests\nimport pyspark.sql.functions as f\nimport logging\n\nlogger = logging.getLogger(\"logger\")\nlogger.setLevel(logging.INFO) # DEBUG, INFO, WARNING, ERROR, CRITICAL\nlogging.basicConfig(\n  format=\"{asctime} - {levelname} - {message}\",\n  style=\"{\",\n  datefmt=\"%Y-%m-%d %H:%M\",\n)\n\nclient_id = dbutils.secrets.get(scope=\"scopeabc\", key=\"abc-pbi-readonly-clientid\")\nclient_secret = dbutils.secrets.get(scope=\"scopeabc\", key=\"abc-pbi-readonly-secret\")\ntenant_id = \"00000000-0000-0000-0000-000000000000\"\nsavePath = 'hive_metastore.powerbicatalogue'\n</code></pre>"},{"location":"posts/graphframes/#functions","title":"Functions","text":"<pre><code>def GetAccessToken(client_id:str, client_secret:str, tenant_id:str, resource:str) -&gt; str:\n    \"\"\"\n    Get an access token from Azure AD.\n    parameters:\n        client_id:      str     the client ID for the application registered in Azure AD\n        client_secret:  str     the client secret for the application registered in Azure AD\n        tenant_id:      str     the tenant ID for the application registered in Azure AD\n        resource:       str     the resource for the application registered in Azure AD\n    returns:            str     the access token\n    \"\"\"\n    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n    scope = f\"{resource}/.default\"\n    data = {\n        \"grant_type\": \"client_credentials\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n        'scope': scope\n    }\n\n    r = Request(method=\"post\", url=url, data=data)\n    token_data = r.json()\n\n    logger.info(f\"{'GetAccessToken':25} Token Generated, {scope} expires in {token_data.get('expires_in')} seconds\")\n\n    return token_data.get(\"access_token\")\n\ndef Request(method:str, url:str, headers:dict=None, data:dict=None, proxies:dict=None):\n    \"\"\"\n    Make a request to the specified URL. Deals with error handling.\n    parameters:\n        method:     str     the HTTP method to use {get, post, put, delete}\n        url:        str     the URL to make the request to\n        headers:    dict    the headers to send with the request\n        data:       dict    the data to send with the request\n        proxies:    dict    the proxies to use with the request\n    returns:        str     the response from the request\n    \"\"\"\n\n    if method not in [\"get\", \"post\", \"put\", \"delete\"]:\n        return f\"Invalid method {method}, must be one of get, post, put, delete\"\n\n    try:\n        r = requests.request(method=method, url=url, headers=headers, data=data, proxies=proxies)\n        invalid_request_reason = r.text\n        if r.status_code == 400:\n            invalid_request_reason = r.text\n            raise Exception(f\"{'Request' :25} Your request has failed because {invalid_request_reason}\")\n        elif r.status_code &gt; 400:\n            raise Exception(f\"{'Request' :25} Your request has failed with status code {r.status_code}\")\n    except requests.exceptions.ConnectionError as err:\n        raise SystemExit(err)\n\n    return r\n\ndef WriteViewToTable(viewName:str, savePath:str, tableName:str=None, mode:str = \"Overwrite\") -&gt; None:\n    \"\"\"\n    Writes a View to a table in the specified database.\n    parameters:\n        viewName:       str\n        mode:           str     Overwrite, Append, Merge\n        tableName:      str     name of the table to write to\n        savePath:       str     path to save the table to i.e \"hive_metastore.xxx\"\n    \"\"\"\n\n    if mode not in (\"Overwrite\", \"Append\", \"Merge\"):\n        raise Exception(f\"{'WriteToTable' :25} Invalid mode {mode}, must be one of Overwrite, Append, Merge\")\n\n    if tableName is None:\n        tableName = viewName\n\n    spark.sql(f\"select * from {viewName}\").write.mode(mode).option(\"overwriteSchema\", \"true\").saveAsTable(f\"{savePath}.{tableName}\")\n\n    logger.info(f\"{'WriteToTable' :25} {viewName} to {savePath}.{tableName} ({mode})\")\n\ndef WriteDfToTable(df, savePath:str, tableName:str, mode:str = \"Overwrite\") -&gt; None:\n    \"\"\"\n    Writes a View to a table in the specified database.\n    parameters:\n        df:           pyspark dataframe\n        mode:         str       Overwrite, Append, Merge\n        tableName:    str       name of the table to write to\n        savePath:     str       path to save the table to i.e \"hive_metastore.xxx\"\n    \"\"\"\n\n    if mode not in (\"Overwrite\", \"Append\", \"Merge\"):\n        raise Exception(f\"{'WriteToTable' :25} Invalid mode {mode}, must be one of Overwrite, Append, Merge\")\n\n    df.write.format(\"delta\").mode(mode).option(\"overwriteSchema\", \"true\").saveAsTable(f\"{savePath}.{tableName}\")\n\n    logger.info(f\"{'WriteDfToTable' :25} {savePath}.{tableName} ({mode})\")\n</code></pre>"},{"location":"posts/graphframes/#power-bi-scan-api","title":"Power BI Scan API","text":""},{"location":"posts/graphframes/#functions_1","title":"Functions","text":"<pre><code>def GetModifiedWorkspaces(access_token: str) -&gt; list:\n    \"\"\"\n    Calls GetModifiedWorkspaces API [https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-modified-workspaces]\n    Excludes InActive Workspaces and Personal Workspaces\n    parameters:\n        access_token:       str     access token\n    Returns:                str     list of workspaceId\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/modified?excludeInActiveWorkspaces=true&amp;excludePersonalWorkspaces=true'\n\n    r = Request(method=\"get\", url=url, headers=headers)\n    workspaces = [workspace['id'] for workspace in r.json()]\n\n    logger.info(f\"{'GetModifiedWorkspaces':25} {len(workspaces)} workspaces returned\")\n\n    return workspaces\n\ndef PostWorkspaceInfo(access_token: str, workspaceIds: list) -&gt; dict:\n    \"\"\"\n    Calls PostWorkspaceInfo API [https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-post-workspace-info]\n    Calls for all avilaible data {datasetExpressions=true, datasourceDetails=true, datasetSchema=true, getArtifactUsers=true, lineage=true}\n    parameters:\n        access_token:       str     access token\n        workspaceIds:       list    list of 1-100 workspacesIds\n    returns:                dict    {'scanid', 'createdDateTime', 'status'}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    url = 'https://api.powerbi.com/v1.0/myorg/admin/workspaces/getInfo?lineage=True&amp;datasourceDetails=True&amp;datasetSchema=True&amp;datasetExpressions=True&amp;getArtifactUsers=True'\n\n    if len(workspaceIds) &gt; 100:\n        raise Exception(f\"{'PostWorkspaceInfo':25} PostWorkspaceInfo API only accepts 100 workspaces at a time\")\n        return\n\n    data = { 'workspaces': workspaceIds }\n    scan = Request(method=\"post\", url=url, headers=headers, data=data).json()\n\n    logger.info(f\"{'PostWorkspaceInfo':25} scanId {scan['id']} [{scan['status']}]\")\n\n    return scan\n\ndef GetScanStatus(access_token: str, scan:dict, delay:int = 2, max_retries:int = 5) -&gt; dict:\n    \"\"\"\n    Calls GetScanStatus API [https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-scan-status]\n    Calls until scan status is 'Succeeded' or max_retries (default: 5) exceeded\n    parmeters:\n        access_token:       str     access token\n        scan:               dict    {'scanid', 'createdDateTime', 'status'}\n        delay:              int     seconds to wait between retries (default: 2)\n        max_retries:        int     max number of retries (default: 5)\n    returns                 dict    {'scanid', 'createdDateTime', 'status'}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    url = f\"https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanStatus/{scan['id']}\"\n\n    for retry in range(max_retries):\n        r = Request(method=\"get\", url=url, headers=headers)\n        scan = r.json()\n        if scan['status'] != 'Succeeded':\n            retry += 1\n            if retry &gt;= max_retries:\n                Exception(f\"{f'GetScanStatus({retry})':25} scanId {scan['id']} Exceeded max_retries limit ({max_retries})\")\n                return\n            if retry &gt; 0:\n                logger.info(f\"{f'GetScanStatus({retry})':25} scanId {scan['id']} [{scan['status']}] Retrying in {delay} seconds...\")\n                time.sleep(delay)\n                delay *= 2  # incremental backoff\n\n    logger.info(f\"{'GetScanStatus':25} scanId {scan['id']} [{scan['status']}]\")\n\n    return scan\n\ndef GetScanResult(access_token: str, scan:dict) -&gt; dict:\n    \"\"\"\n    Calls GetScanResult API [https://learn.microsoft.com/en-us/rest/api/power-bi/admin/workspace-info-get-scan-result]\n    parameters:\n        access_token:       str     access token\n        scan:               dict    {'scanid', 'createdDateTime', 'status'}\n    returns:                dict    {'scanid', 'createdDateTime', 'status'}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    url = f\"https://api.powerbi.com/v1.0/myorg/admin/workspaces/scanResult/{scan['id']}\"\n\n    r = Request(method=\"get\", url=url, headers=headers)\n\n    logger.info(f\"{'GetScanResult':25} scanId {scan['id']} complete\")\n\n    return r.json()\n\ndef GetApps(access_token: str):\n    \"\"\"\n    Calls GetAppsAsAdmin API [https://learn.microsoft.com/en-us/rest/api/power-bi/admin/apps-get-apps-as-admin]\n    parameters:\n        access_token:       str     access token\n    Returns:                dict    {'id', 'description', 'name', 'publishedBy, 'lastUpdate', 'workspaceId', 'users'}\n    \"\"\"\n\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    url = 'https://api.powerbi.com/v1.0/myorg/admin/apps?$top=5000'\n\n    r = Request(method=\"get\", url=url, headers=headers)\n\n    logger.info(f\"{'GetAppsAsAdmin':25}\")\n\n    return r.json()\n</code></pre>"},{"location":"posts/graphframes/#run","title":"Run","text":"<pre><code>access_token = GetAccessToken(client_id, client_secret, tenant_id, resource='https://analysis.windows.net/powerbi/api')\n\nworkspaces = GetModifiedWorkspaces(access_token)\n\nscan_results = []\nchunk_size = 100 ## PostWorkspaceInfo accepts 100 workspaces at a time\n\nfor chunk in [workspaces[i:i+chunk_size] for i in range(0, len(workspaces), chunk_size)]:\n    scan = PostWorkspaceInfo(access_token, chunk)\n\n    if scan['status'] != 'Succeeded':\n        GetScanStatus(access_token, scan)\n\n    scan_results.append(GetScanResult(access_token, scan))\n\napps = GetApps(access_token)\n</code></pre>"},{"location":"posts/graphframes/#apply-schema-create-dataframes","title":"Apply Schema &amp; Create Dataframes","text":"<pre><code>workspaceSchema = StructType([\n    StructField('description', StringType(), True),\n    StructField('id', StringType(), True),\n    StructField('isOnDedicatedCapacity', BooleanType(), True),\n    StructField('name', StringType(), True),\n    StructField('state', StringType(), True),\n    StructField('type', StringType(), True),\n    StructField('capacityId', StringType(), True),\n    StructField('defaultDatasetStorageFormat', StringType(), True),\n    StructField('users', ArrayType( StructType([\n        StructField('groupUserAccessRight', StringType(), True),\n        StructField('emailAddress', StringType(), True),\n        StructField('displayName', StringType(), True),\n        StructField('identifier', StringType(), True),\n        StructField('graphId', StringType(), True),\n        StructField('principalType', StringType(), True),\n        StructField('userType', StringType(), True)\n        ])), True),\n    StructField('reports', ArrayType( StructType([\n        StructField('createdBy', StringType(), True),\n        StructField('createdById', StringType(), True),\n        StructField('createdDateTime', StringType(), True),\n        StructField('datasetId', StringType(), True),\n        StructField('id', StringType(), True),\n        StructField('description', StringType(), True),\n        StructField('modifiedBy', StringType(), True),\n        StructField('modifiedById', StringType(), True),\n        StructField('modifiedDateTime', StringType(), True),\n        StructField('name', StringType(), True),\n        StructField('reportType', StringType(), True),\n        StructField('users', ArrayType( StructType([\n            StructField('reportUserAccessRight', StringType(), True),\n            StructField('emailAddress', StringType(), True),\n            StructField('displayName', StringType(), True),\n            StructField('identifier', StringType(), True),\n            StructField('graphId', StringType(), True),\n            StructField('principalType', StringType(), True),\n            StructField('userType', StringType(), True),\n            ])), True),\n        ])), True),\n    StructField('datasets', ArrayType( StructType([\n        StructField('configuredBy', StringType(), True),\n        StructField('configuredById', StringType(), True),\n        StructField('contentProviderType', StringType(), True),\n        StructField('createdDate', StringType(), True),\n        StructField('id', StringType(), True),\n        StructField('isEffectiveIdentityRequired',BooleanType(), True),\n        StructField('isEffectiveIdentityRolesRequired', BooleanType(), True),\n        StructField('name', StringType(), True),\n        StructField('targetStorageMode', StringType(), True),\n        StructField('description', StringType(), True),\n        StructField('sensitivityLevel', StringType(), True),\n        StructField('endorsmentDetails', StringType(), True),\n        StructField('expressions', ArrayType( StructType([\n            StructField('expression', StringType(), True),\n            StructField('name', StringType(), True),\n            StructField('description', StringType(), True),\n            ])), True),\n        StructField('tables', ArrayType( StructType([\n            StructField('isHidden', BooleanType(), True),\n            StructField('name', StringType(), True),\n            StructField('source', StringType(), True),\n            StructField('storageMode', StringType(), True),\n            StructField('columns', ArrayType( StructType([\n                StructField('columnType', StringType(), True),\n                StructField('dataType', StringType(), True),\n                StructField('isHidden', BooleanType(), True),\n                StructField('name', StringType(), True),\n                StructField('expression', StringType(), True)\n                ])), True),\n            StructField('measures', ArrayType( StructType([\n                StructField('isHidden', BooleanType(), True),\n                StructField('name', StringType(), True),\n                StructField('description', StringType(), True),\n                StructField('expression', StringType(), True)\n                ])), True),\n            ])), True),\n        StructField('refreshSchedule', StructType([\n            StructField('days', ArrayType(StringType(), True), True),\n            StructField('times', ArrayType(StringType(), True),True),\n            StructField('enabled', BooleanType(), True),\n            StructField('localTimeZoneId', StringType(), True),\n            StructField('notifyOption', StringType(), True),\n            ]), True),\n        StructField('directQueryRefreshSchedule', StructType([\n            StructField('days', ArrayType(StringType(), True), True),\n            StructField('times', ArrayType(StringType(), True),True),\n            StructField('localTimeZoneId', StringType(), True),\n            StructField('frequency', IntegerType(), True),\n            ]), True),\n        StructField('datasourceUsages', ArrayType( StructType([\n            StructField('datasourceInstanceId', StringType(), True)\n            ])), True),\n        StructField('upstreamDatasets', ArrayType( StructType([\n            StructField('targetDatasetId', StringType(), True),\n            StructField('groupId', StringType(), True)\n            ])), True),\n        StructField('users', ArrayType( StructType([\n            StructField('datasetUserAccessRight', StringType(), True),\n            StructField('emailAddress', StringType(), True),\n            StructField('displayName', StringType(), True),\n            StructField('identifier', StringType(), True),\n            StructField('graphId', StringType(), True),\n            StructField('principalType', StringType(), True),\n            StructField('userType', StringType(), True)\n            ])), True),\n        StructField('roles', ArrayType( StructType([\n            StructField('name', StringType(), True),\n            StructField('modelPermissions', StringType(), True),\n            StructField('members', ArrayType( StructType([\n                StructField('memberName', StringType(), True),\n                StructField('memberId', StringType(), True),\n                StructField('memberType', StringType(), True),\n                StructField('identityProvider', StringType(), True)\n                ])), True),\n            StructField('tablePermissions', ArrayType( StructType([\n                StructField('name', StringType(), True),\n                StructField('filterExpression', StringType(), True)\n                ])), True)\n            ])), True)\n        ])), True)\n    ])\n\ndatasourceInstancesSchema = StructType([\n    StructField('connectionDetails', StructType([\n        StructField('extensionDataSourceKind', StringType()),\n        StructField('extensionDataSourcePath', StringType()),  \n        StructField('path', StringType()),\n        StructField('url', StringType()),\n        StructField('sharePointSiteUrl', StringType()),\n        StructField('server', StringType())  \n        ]), True),\n    StructField('datasourceId', StringType(), True),\n    StructField('datasourceType', StringType(), True),\n    StructField('gatewayId', StringType(), True)\n    ])\n\nappsSchema = StructType([\n    StructField('id', StringType(), True),\n    StructField('description', StringType(), True),\n    StructField('name', StringType(), True),\n    StructField('workspaceId', StringType(), True),\n    StructField('publishedBy', StringType(), True),\n    StructField('lastUpdate', StringType(), True)\n    ])\n</code></pre> <pre><code>workspacesdf = spark.createDataFrame([], schema = workspaceSchema)\ndatasourceInstancesdf = spark.createDataFrame([], schema = datasourceInstancesSchema)\n\nfor chunk in scan_results:\n    df1 = spark.createDataFrame(chunk['workspaces'], schema = workspaceSchema)\n    workspacesdf = workspacesdf.union(df1)\n    df2 = spark.createDataFrame(chunk['datasourceInstances'], schema = datasourceInstancesSchema)\n    datasourceInstancesdf = datasourceInstancesdf.union(df2)\n\nappsdf = spark.createDataFrame(apps['value'], schema = appsSchema)\n\nworkspacesdf.createOrReplaceTempView('workspacesAll')\ndatasourceInstancesdf.createOrReplaceTempView('datasourceInstance')\nappsdf.createOrReplaceTempView('apps')\n</code></pre>"},{"location":"posts/graphframes/#create-views","title":"Create views","text":"<pre><code>%sql\n-- datasourceInstances\n  CREATE OR REPLACE TEMPORARY VIEW connectionDetails AS\n  with x as (select *, connectionDetails.* from datasourceInstance)\n  select * except(connectionDetails) from x;\n\n-- workspaces\n  CREATE OR REPLACE TEMPORARY VIEW workspaces AS\n    SELECT * except (users, reports, datasets)\n    FROM workspacesAll\n  ;\n\n-- workspaces | Users\n  CREATE OR REPLACE TEMPORARY VIEW workspaceUsers AS\n    WITH explode AS (SELECT id AS workspaceId, explode(users) AS users FROM workspacesAll),\n    expand AS (SELECT *, users.* from explode)\n    SELECT * except(users) FROM expand;\n\n-- workspaces | reports*\n  CREATE OR REPLACE TEMPORARY VIEW reportsAll AS\n    WITH explode AS (SELECT id as workspaceId, explode(reports) AS reports FROM workspacesAll),\n    expand AS (SELECT *, reports.* FROM explode)\n    SELECT * FROM expand;\n\n-- workspaces | reports\n  CREATE OR REPLACE TEMPORARY VIEW reports AS\n    SELECT * except(reports, users) FROM reportsAll;\n\n-- workspaces | reports | Users\n  CREATE OR REPLACE TEMPORARY VIEW ReportUsers AS\n    WITH explode AS (SELECT id AS reportId, explode(users) AS users FROM reportsAll),\n    expand AS (SELECT *, users.* FROM explode)\n    SELECT * except(users) FROM expand;\n\n-- workspaces | datasets*\n  CREATE OR REPLACE TEMPORARY VIEW DatasetsAll AS\n    WITH explode AS (select id AS workspaceId, explode(datasets) AS datasets FROM workspacesAll),\n    expand AS (SELECT *, datasets.* FROM explode)\n    SELECT * FROM expand;\n\n-- workspaces | datasets\n  CREATE OR REPLACE TEMPORARY VIEW datasets AS\n    SELECT * except(datasets, expressions, tables, refreshSchedule, directQueryRefreshSchedule, upstreamDatasets, datasourceUsages, users, roles) FROM DatasetsAll;\n\n-- workspaces | datasets | expressions\n  CREATE OR REPLACE TEMPORARY VIEW datasetExpressions AS\n    WITH explode AS (SELECT id AS datasetId, explode(expressions) AS expressions FROM DatasetsAll),\n    expand AS (SELECT *, expressions.* FROM explode)\n    SELECT * except(expressions) FROM expand;\n\n-- workspaces | datasets | refreshSchedules\n  CREATE OR REPLACE TEMPORARY VIEW datasetRefreshSchedules AS\n    WITH expandrefreshSchedule AS (SELECT id AS datasetId, refreshSchedule.* FROM DatasetsAll),\n    explodeRefreshSchedule1 AS (\n      SELECT\n      datasetId,\n      localTimeZoneId,\n      enabled,\n      notifyOption,\n      explode_outer(days) AS days,\n      times\n      FROM expandrefreshSchedule\n    ),\n    explodeRefreshSchedule2 AS (\n      SELECT\n      datasetId,\n      localTimeZoneId,\n      enabled,\n      notifyOption,\n      days,\n      explode_outer(times) as times\n      FROM explodeRefreshSchedule1\n    ),\n    expandDirectQueryRefreshSchedule AS (SELECT id AS datasetId, DirectQueryRefreshSchedule.* FROM DatasetsAll),\n    explodeDirectQueryRefreshSchedule1 AS (\n      SELECT\n      datasetId,\n      localTimeZoneId,\n      frequency,\n      explode_outer(days) AS days,\n      times\n      FROM expandDirectQueryRefreshSchedule\n    ),\n    explodeDirectQueryRefreshSchedule2 AS (\n      SELECT\n      datasetId,\n      localTimeZoneId,\n      frequency,\n      days,\n      explode_outer(times) as times\n      FROM explodeDirectQueryRefreshSchedule1\n    )\n    SELECT\n    datasetId,\n    \"RefreshSchedule\" AS refreshScheduleType,\n    localTimeZoneId,\n    enabled,\n    notifyOption,\n    null AS frequency,\n    days,\n    times\n    FROM explodeRefreshSchedule2\n    WHERE enabled\n    UNION ALL\n    SELECT\n    datasetId,\n    \"directQueryRefreshSchedule\" AS refreshScheduleType,\n    localTimeZoneId,\n    null as enabled,\n    null as notifyOption,\n    frequency,\n    days,\n    times\n    FROM explodeDirectQueryRefreshSchedule2\n    WHERE localTimeZoneId is not null;\n\n-- workspaces | datasets | upstreamDatasets\n  CREATE OR REPLACE TEMPORARY VIEW datasetUpstreamDatasets AS\n    WITH explode AS (SELECT id AS datasetId, explode(upstreamDatasets) AS upstreamDatasets FROM DatasetsAll),\n    expand AS (SELECT *, upstreamDatasets.* FROM explode)\n    SELECT * except(upstreamDatasets) FROM expand;\n\n-- workspaces | datasets | datasourceUsages\n  CREATE OR REPLACE TEMPORARY VIEW datasetsDatasorucesUsages AS\n    WITH explode AS (SELECT id AS datasetId, explode(datasourceUsages) AS datasourceUsages FROM DatasetsAll),\n    expand AS (SELECT *, datasourceUsages.* FROM explode)\n    SELECT * except(datasourceUsages) FROM expand;\n\n-- workspaces | datasets | users\n  CREATE OR REPLACE TEMPORARY VIEW datasetsUsers AS\n    WITH explode AS (SELECT id AS datasetId, explode(users) AS users FROM DatasetsAll),\n    expand AS (SELECT *, users.* FROM explode)\n    SELECT * except(users) FROM expand;\n\n-- workspaces | datasets | tables *\n  CREATE OR REPLACE TEMPORARY VIEW datasetsTablesAll AS\n    WITH explode AS (SELECT id AS datasetId, explode(tables) AS tables FROM DatasetsAll),\n    expand AS (SELECT *, tables.* FROM explode)\n    SELECT concat(datasetId, name) AS datasetTableId, * FROM expand;\n\n-- workspaces | datasets | tables\n  CREATE OR REPLACE TEMPORARY VIEW datasetsTables AS\n    SELECT * except(tables, columns, measures) FROM datasetsTablesAll;\n\n-- workspaces | objects\n  CREATE OR REPLACE TEMPORARY VIEW objects AS\n    WITH workspace AS (\n      SELECT\n      id AS workspaceId,\n      id AS object_id,\n      null AS datasetId,\n      'Workspace' AS objectType,\n      name,\n      null AS createdDateTime\n      FROM workspaces\n    ),\n    dataset AS (\n      SELECT\n      workspaceId,\n      id AS object_id,\n      id AS datasetId,\n      'Semantic Model' AS objectType,\n      name,\n      createdDate AS createdDateTime\n      FROM datasets\n    ),\n    report AS (\n      SELECT\n      workspaceId,\n      id AS object_id,\n      datasetId,\n      'Report' AS objectType,\n      name,\n      createdDateTime\n      FROM reports\n      WHERE name NOT LIKE '[App] %'\n    ),\n    reportApp AS (\n      SELECT\n      workspaceId,\n      id AS object_id,\n      datasetId,\n      'Report App' AS objectType,\n      name,\n      createdDateTime\n      FROM reports\n      WHERE name LIKE '[App] %'\n    )\n    SELECT * FROM workspace\n    UNION ALL\n    SELECT * FROM dataset\n    UNION ALL\n    SELECT * FROM report\n    UNION ALL\n    SELECT * FROM reportApp;\n</code></pre>"},{"location":"posts/graphframes/#save-tables","title":"Save Tables","text":"<pre><code>for view in ['connectionDetails', 'workspaces', 'reports', 'datasets', 'datasetsTables', 'datasetExpressions', 'datasetRefreshSchedules'\n              ,'datasetUpstreamDatasets', 'datasetsDatasorucesUsages', 'objects', 'workspaceUsers', 'reportUsers', 'datasetsUsers', 'apps', 'tenantSettings']:\n\n  WriteViewToTable(view, savePath)\n</code></pre>"},{"location":"posts/graphframes/#graph-api","title":"Graph API","text":""},{"location":"posts/graphframes/#functions_2","title":"Functions","text":"<pre><code>def getGraphAPI(entity:str='groups') -&gt; list:\n  \"\"\"\n  Calls List groups API [https://learn.microsoft.com/en-us/graph/api/group-list?view=graph-rest-1.0&amp;tabs=http], List users API [https://learn.microsoft.com/en-us/graph/api/user-list?view=graph-rest-1.0&amp;tabs=http] or App list API [https://learn.microsoft.com/en-us/graph/api/application-list?view=graph-rest-1.0&amp;tabs=http]\n  parameters:\n    type:       str     groups, users or apps (default: groups)\n  returns:      list    array of users, groups or apps\n  \"\"\"\n\n  if entity not in ('groups', 'users', 'apps'):\n    raise Exception(f\"Invalid type: {entity}\")\n\n  access_token = GetAccessToken(client_id, client_secret, tenant_id, resource='https://graph.microsoft.com')\n\n  headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n  if entity == \"groups\":\n    url = https://graph.microsoft.com/v1.0/groups?$expand=members($select=id,displayName,mail,userType)\n  if entity == \"users\":\n      url = https://graph.microsoft.com/v1.0/users\n  if entity == \"apps\":\n      url = https://graph.microsoft.com/v1.0/applications\n\n  items = []\n\n  while True:\n    r = Request(method=\"get\", url=url, headers=headers).json()\n    newItems = [item for item in r['value'] if item not in items]\n    items += newItems\n    logger.info(f\"{'getGroupsGraphAPI':25} {len(items)} {entity} processed\")\n    if '@odata.nextLink' not in r:\n      break\n    url = r['@odata.nextLink']\n\n  logger.info(f\"{'getGroupsGraphAPI':25} {len(items)} {entity} returned\")\n\n  return items\n</code></pre>"},{"location":"posts/graphframes/#run_1","title":"Run","text":"<pre><code>groups = getGraphAPI(\"groups\")\nusers = getGraphAPI(\"users\")\napps = getGraphAPI(\"apps\")\n</code></pre>"},{"location":"posts/graphframes/#apply-schema-create-dataframes_1","title":"Apply Schema &amp; Create Dataframes","text":"<pre><code>groupsSchema = StructType([\n  StructField(\"id\", StringType(), True),\n  StructField(\"displayName\", StringType(), True),\n  StructField(\"description\", StringType(), True),\n  StructField(\"members\", ArrayType(MapType(StringType(), StringType())), True)\n  ])\n\nusersSchema = StructType([\n  StructField(\"id\", StringType(), True),\n  StructField(\"displayName\", StringType(), True),\n  StructField(\"userPrincipalName\", StringType(), True)\n  ])\n\nappsSchema = StructType([\n  StructField(\"appId\", StringType(), True),\n  StructField(\"displayName\", StringType(), True)\n  ])\n\naadGroups = spark.createDataFrame(groups, schema=groupsSchema)\naadUsers = spark.createDataFrame(users, schema=usersSchema)\naadApps = spark.createDataFrame(apps, schema=appsSchema)\n</code></pre>"},{"location":"posts/graphframes/#save-tables_1","title":"Save Tables","text":"<pre><code>for tableName, df in {'aadGroups': aadGroups, 'aadUsers': aadUsers, 'aadApps': aadApps}.items():\n    WriteDfToTable(df, savePath, tableName)\n</code></pre>"},{"location":"posts/graphframes/#graphframes_1","title":"GraphFrames","text":""},{"location":"posts/graphframes/#dependencies_1","title":"Dependencies","text":"<pre><code>from graphframes import *\nfrom graphframes.lib import Pregel\n</code></pre>"},{"location":"posts/graphframes/#generate-vertices-and-edges","title":"Generate vertices and edges","text":"<pre><code>v = spark.sql(f\"\"\"\n  select\n  concat(wu.workspaceId, wu.groupUserAccessRight) as id,\n  wu.workspaceId as nodeId,\n  w.name,\n  'Workspace' as type,\n  wu.groupUserAccessRight as accessRight\n  from {savePath}.workspaceusers as wu\n  left join {savePath}.workspaces as w\n    on wu.workspaceId = w.id\n  \"\"\")\\\n  .union(spark.sql(f\"\"\"\n    select\n    concat(ru.reportId, ru.reportUserAccessRight) as id,\n    ru.reportId as nodeId,\n    r.name,\n    case\n      when left(r.name, 5) = '[App]' then 'Report App'\n      else 'Report'\n    end as type,\n    ru.reportUserAccessRight as accessRight\n    from {savePath}.reportUsers as ru\n    left join {savePath}.reports as r\n      on ru.reportId = r.id\n  \"\"\"))\\\n  .union(spark.sql(f\"\"\"\n    select\n    concat(du.datasetId, du.datasetUserAccessRight) as id,\n    du.datasetId as nodeId,\n    d.name,\n    'Dataset' type,\n    du.datasetUserAccessRight as accessRight\n    from {savePath}.datasetsusers as du\n    left join {savePath}.datasets as d\n      on du.datasetId = d.id\n  \"\"\"))\\\n  .union(spark.sql(f\"\"\"\n    select\n    id,\n    id as nodeId,\n    displayName as name,\n    'Group' as type,\n    null as accessRight\n    from {savePath}.aadgroups\n  \"\"\"))\\\n  .union(spark.sql(f\"\"\"\n    select\n    id,\n    id as nodeId,\n    displayName as name,\n    'User' as type,\n    null as accessRight\n    from {savePath}.aadusers\n  \"\"\"))\\\n  .union(spark.sql(f\"\"\"\n    select\n    appId AS id,\n    appId as nodeId,\n    displayName as name,\n    'App' as type,\n    null as accessRight\n    from {savePath}.aadapps\n  \"\"\"))\\\n  .distinct()\n\ne = spark.sql(f\"\"\"\n  select\n  concat(workspaceId, groupUserAccessRight) as src,\n  graphId as dst,\n  groupUserAccessRight as edge_type\n  from {savePath}.workspaceUsers\n  \"\"\")\\\n  .union(spark.sql(f\"\"\"\n    select\n    concat(reportId, reportUserAccessRight) as src,\n    graphId as dst,\n    reportUserAccessRight as edge_type\n    from {savePath}.reportUsers\n  \"\"\"))\\\n  .union(spark.sql(f\"\"\"\n    select\n    concat(datasetId, datasetUserAccessRight) as src,\n    graphId as dst,\n    datasetUserAccessRight as edge_type\n    from {savePath}.datasetsusers\n  \"\"\"))\\\n  .union(\n    spark.sql(f\"select id, explode(members) as member from {savePath}.aadgroups\")\\\n          .selectExpr(\"id as groupId\", \"member['id'] as memberId\", \"member['userType'] as userType\", \"member['dispayName'] as displayName\" )\\\n          .selectExpr(\"groupId as src\", \"memberId as dst\", \"null as edge_type\")\\\n   )\n\n#   vertices                                                                          \n# +-------------------------------------------------------------------------------------------------------------------+\n# | id                                               | nodeId                               | type      | accessRight |\n# +-------------------------------------------------------------------------------------------------------------------+\n# | a2cc72b4-50e8-4c78-b875-6b1d6af6f04fAdmin        | a2cc72b4-50e8-4c78-b875-6b1d6af6f04f | workspace | Admin       |\n# | c6434512-6cec-45d6-91a0-e24d6ec8ae3fContributor  | c6434512-6cec-45d6-91a0-e24d6ec8ae3f | workspace | Contributor |\n# | 18f3f38d-d4e5-4861-9633-43c87cd6f444             | 18f3f38d-d4e5-4861-9633-43c87cd6f444 | group     |             |\n# +-------------------------------------------------------------------------------------------------------------------+\n\n#   edges\n# +-------------------------------------------+--------------------------------------+------------+\n# |  src                                      |  dst                                 | edge_type  |\n# +-------------------------------------------+--------------------------------------+------------+\n# | a2cc72b4-50e8-4c78-b875-6b1d6af6f04fAdmin | 27600b6f-6556-43ad-98de-9a6e068a8500 | admin      |\n# | 32282557-43f0-4182-9541-d9f3c44029c6      | bba07815-4eaf-4798-bc76-f7e55747cb3b |            |\n# | a9497daf-95e4-42aa-9edf-658edeb2205d      | ccb5abe8-8890-49a4-a8fc-d3bc2c3821b\" |            |\n# +-------------------------------------------+--------------------------------------+------------+\n</code></pre>"},{"location":"posts/graphframes/#save-vertices-and-edges","title":"Save Vertices and Edges","text":"<pre><code>for tableName, df in {'v': v, 'e': e}.items():\n  WriteDfToTable(df, savePath, tableName)\n</code></pre>"},{"location":"posts/graphframes/#generate-graph","title":"Generate Graph","text":"<pre><code>g = GraphFrame(v, e)\n</code></pre>"},{"location":"posts/graphframes/#pregel_1","title":"Pregel","text":"<pre><code># https://stackoverflow.com/questions/75410401/graphframes-pregel-doesnt-`converge`\n\n# TODO Want to be able to use edge attr (UserAccessRight) in pregel so that can have single node for workspace/artifact, instead having to generate vertices for each workspace/artifact crossjoined with AccessRole. Not been able to find the right syntax to allow for it. If can do that, then can drop nodeID and accessRight from vertices\n\nmappedRoles = (\n    g.pregel\n    .setMaxIter(10)                 # MaxIter should be set to a value at least as large as the longest path\n    .setCheckpointInterval(2)       # checkpointInterval should be set to a value smaller than maxIter, can be added to save state to avoid stackOverflowError due to long lineage chains\n    .withVertexColumn( \n        \"resolved_roots\",           # New column for the resolved roots\n        # The value is initialized by the original root value:\n        f.when( \n            f.col('type').isin(['workspace', 'report', 'dataset']),\n            f.array(f.to_json(f.struct('type', 'nodeId', 'accessRight')))\n        ).otherwise(f.array()),\n        # When new value arrives to the node, it gets merged with the existing list:\n        f.when(\n            Pregel.msg().isNotNull(), \n            f.array_union(Pregel.msg(), f.col('resolved_roots')) \n        ).otherwise(f.col(\"resolved_roots\"))\n    )\n    .sendMsgToDst(Pregel.src(\"resolved_roots\"))\n    # Once the message is delivered it is updated with the existing list of roots at the node:\n    .aggMsgs(f.flatten(f.collect_list(Pregel.msg())))\n    .run()\n)\n\n# +-------------------------------------------+--------------------------------------------------------------------------------------------------------------+---------------------------------------+-----------+--------------+\n# | id                                        | resolved_roots                                                                                               |  nodeId                               | type      | accessRight  |\n# +-------------------------------------------+--------------------------------------------------------------------------------------------------------------+---------------------------------------+-----------+--------------+\n# | 94c3fe39-0ed5-4eeb-a230-2e444638930fAdmin | [\"{\\\"type\\\":\\\"workspace\\\", \\\"id\\\":\\\"94c3fe39-0ed5-4eeb-a230-2e444638930f\\\",\\\"accessRight\\\":\\\"Member\\\"}\"]     |  94c3fe39-0ed5-4eeb-a230-2e444638930f | workspace | Contributor  |\n# | 5de589b4-f539-468d-96c8-7fd1034faf9e      | [\"{\\\"type\\\":\\\"workspace\\\", \\\"id\\\":\\\"31633c8d-6cac-4738-b6f3-f63ebbf29ea0\\\",\\\"accessRight\\\":\\\"Viewer\\\"}\",..\"] |  5de589b4-f539-468d-96c8-7fd1034faf9e | user      | null         |\n# +-------------------------------------------+--------------------------------------------------------------------------------------------------------------+---------------------------------------+-----------+--------------+\n</code></pre> <pre><code>mappedRoles.createOrReplaceTempView(\"accessToObject\")\n\naccessToObject = spark.sql(f\"\"\"\n  with explode as (select id, name, explode(resolved_roots) as roots, nodeID, type, accessRight from accessToObject),\n  defineStruct as (select id, name, type, from_json(roots, 'type string, nodeId string, accessRight string') as roots from explode)\n  select\n  defineStruct.id,\n  aadusers.userPrincipalName,\n  defineStruct.name,\n  defineStruct.type,\n  defineStruct.roots.nodeId as accessToObjectId,\n  defineStruct.roots.type as accessToObjectType,\n  defineStruct.roots.accessRight as accessToObjectPermission,\n  concat(defineStruct.roots.nodeId, defineStruct.roots.accessRight) as accessToObjectGroupId,\n  case\n      when workspaceusers.workspaceId is not null or datasetsusers.datasetId is not null or reportusers.reportId is not null then 'Direct'\n      else 'Indirect'\n  end as accessToObjectDirectlyGranted\n  from defineStruct\n  left join {savePath}.workspaceusers\n      on defineStruct.roots.nodeId = workspaceusers.workspaceId and defineStruct.roots.accessRight = workspaceusers.groupUserAccessRight -- src\n      and defineStruct.id = workspaceusers.graphId -- dst\n  left join {savePath}.datasetsusers\n      on defineStruct.roots.nodeId = datasetsusers.datasetId and defineStruct.roots.accessRight = datasetsusers.datasetUserAccessRight -- src\n      and defineStruct.id = datasetsusers.graphId -- dst\n  left join {savePath}.reportusers\n      on defineStruct.roots.nodeId = reportusers.reportId and defineStruct.roots.accessRight= reportusers.reportUserAccessRight -- src\n      and defineStruct.id = reportusers.graphId -- dst\n  left join {savePath}.aadusers\n      on aadusers.id = defineStruct.id\n  where defineStruct.type not in ('Workspace', 'Report', 'Dataset', 'Report App')\n\"\"\")\n\nfor tableName, df in {'accessToObject': accessToObject}.items():\n  WriteDfToTable(df, savePath, tableName)\n</code></pre>"},{"location":"posts/graphframes/#queries-the-results","title":"Queries the results","text":"<p>Now we are able to filter out any AAD Group or user and we will get back a list of all the Workspace, Report and Semantic Model roles they have inherited.</p> <pre><code>usersAccessRights = spark.sql(f\"select * from {savePath}.accessToObjectEdges\")\ndisplay(usersAccessRights.filter(\"name = 'someAADGroup'\"))\n</code></pre>"},{"location":"posts/graphframes/#conclusion","title":"Conclusion","text":"<p>Now we have a dataset that we can query and figure out what groups and users have which permissions. This all would be possible with a recursive CTE in SQL, but that is not supported by pyspark. Additionally now that we have a graph we are able to run graph algorithm such as Label Propagation Algorithm to find clusters in AAD groups and potentially consolidate and simplify. </p> <p>PS. If anyone knows the syntax use a edge attribute in a pregel message, I would love to know. </p>"},{"location":"posts/deneb-force-directed/","title":"Visualizing Power BI Permission Inheritance","text":"<p>In my previous post I used the Power BI Scanner APIs, Graph APIs and GraphFrames to generate a graph to disseminate Access Roles from Workspaces, Reports and Semantic Models directly granted to AAD groups, to all downstream members. Now we are going to create some visualizations to make this data more accessible.</p> <p>Info</p> <p>I updated some code in my previous post to align with the data model in this post. Plus updated this post to use triplet form instead of union of vertices and edges as it makes working with the graph simpler in Power BI</p>"},{"location":"posts/deneb-force-directed/#graphframe","title":"GraphFrame","text":"<p>We need to add a extra table <code>accessToObjectEdges</code> to the previous code from the previous post to power the visual, which is a triplet form of the graph. <code>accessToObjectEdges.accessToObjectGroupId</code> will be used to label all nodes that inherits permission from a objects (Workspace, Report, Semantic Model) role so we can trim unnecessary nodes from subgraphs.</p> <pre><code>g.triplets.createOrReplaceTempView(\"triplets\")\n\naccessToObjectEdges = spark.sql(f\"\"\"\n  select\n  t.src.nodeId as srcId,\n  t.src.name as srcName,\n  t.src.type as srcType,\n  t.dst.nodeId as dstId,\n  t.dst.name as dstName,\n  t.dst.type as dstType,\n  coalesce(r.accessToObjectGroupId, concat(t.src.nodeID, t.src.accessRight)) as accessToObjectGroupId\n  from triplets as t\n  left join {savePath}.accessToObject as r\n      on t.src.nodeID = r.id\n  \"\"\"\n)\n\nfor tableName, df in {'accessToObjectEdges': accessToObjectEdges}.items():\n  WriteDfToTable(df, savePath, tableName)\n</code></pre>"},{"location":"posts/deneb-force-directed/#power-bi-data-model","title":"Power BI Data Model","text":"<p>The point of running the Scanner API was to create a report that catalogues everything in Power BI. After playing around for a bit I ended with up a similar model to Rui Romano's PBI Scanner solution. To reduce the complexity of measures, all of the main artifacts (Workspaces, Report, Semantic Models) are considered as objects in a object table. As a side note this model integrates perfectly with the Fabric Log Analytics for Analysis Services Engine report template, giving all tenant metadata and alongside refresh and query performance (Artifacts == Objects).</p> <p>I imported the table above and called it <code>accessToObject Edges</code>. It is disconnected from the model so that we can use DAX measures to filter the graphs to give specific sub-graphs, from the perspective of specific objects or users. Additional I added a <code>Vertex Type</code> dimension to filter nodes to clear the graph up when required.</p> <p>Rather than just listing permission in a table, lets create a visualization to help make the data more understandable.</p>"},{"location":"posts/deneb-force-directed/#deneb","title":"Deneb","text":"<p>I tried using the Power BI Force-Directed Graph visual but the results were not what I was looking for, so I turned to Deneb. I found Davide Bacci's Force Directed Graph example to be a good starting point.</p>"},{"location":"posts/deneb-force-directed/#object-permissions","title":"Object Permissions","text":"<p>We create a page with the Deneb visual, and create the measure below. We add the <code>[Edge Selection]</code> measure to the filter well of the Deneb visual and filter to where the measure = 1.</p> <p></p> <pre><code>Edge Selection =\nvar singleUser = HASONEFILTER( accessToObject[name] )\nvar FilteredUsers =\n    FILTER(\n        'accessToObject Edges'\n        ,var user = SELECTEDVALUE( accessToObject[name] )\n        RETURN\n        ('accessToObject Edges'[srcType] &lt;&gt; \"User\" &amp;&amp;  'accessToObject Edges'[dstType] &lt;&gt; \"User\")\n        || ('accessToObject Edges'[srcType] = \"User\" &amp;&amp; 'accessToObject Edges'[srcName] = user)\n        || ('accessToObject Edges'[dstType] = \"User\" &amp;&amp; 'accessToObject Edges'[dstName] = user)\n    )\n\nRETURN\nSWITCH(\n    true\n    ,not ISCROSSFILTERED( accessToObject )\n        ,0\n    ,COUNTROWS( DISTINCT( INTERSECT( VALUES( accessToObject[accessToObjectGroupId] ), VALUES( 'accessToObject Edges'[accessToObjectGroupId] ) ) ) ) &gt; 0\n    &amp;&amp; COUNTROWS( DISTINCT( INTERSECT( VALUES( 'Vertex Type'[Type] ), VALUES( 'accessToObject Edges'[srcType] ) ) ) ) &gt; 0\n    &amp;&amp; COUNTROWS( DISTINCT( INTERSECT( VALUES( 'Vertex Type'[Type] ), VALUES( 'accessToObject Edges'[dstType] ) ) ) ) &gt; 0\n    &amp;&amp; IF( singleUser, COUNTROWS( FilteredUsers ) &gt; 0, true )\n        ,1\n)\n</code></pre>"},{"location":"posts/deneb-force-directed/#vega-spec","title":"Vega Spec","text":"<p>I wanted to use Bee Swarm to pin objects to the left and user/apps to the right and groups in the middle. But I am still quite new to Vega and I couldn't quite get the syntax right to achieve this. If anyone can get to this work for this spec I would be very interested in seeing it.</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega/v5.json\",\n  \"description\": \"Based off Force Directed example by David Bacci:https://github.com/PBI-David/Deneb-Showcase/blob/main/Force%20Directed%20Graph/Spec.json\",\n  \"padding\": {\n    \"left\": 0,\n    \"right\": 0,\n    \"top\": 0,\n    \"bottom\": 0\n  },\n\n  \"signals\": [\n    {\"name\": \"xrange\", \"update\": \"[0, width]\"},\n    {\"name\": \"yrange\", \"update\": \"[height, 0]\"},\n    {\"name\": \"xext\",\"update\": \"[0, width]\"},\n    {\"name\": \"yext\",\"update\": \"[height, 0]\"},\n    {\n      \"name\": \"down\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"mouseup,touchend\",\n          \"update\": \"null\"\n        },\n        {\n          \"events\": \"mousedown, touchstart\",\n          \"update\": \"xy()\"\n        },\n        {\n          \"events\": \"symbol:mousedown, symbol:touchstart\",\n          \"update\": \"null\"\n        }\n      ]\n    },\n    {\n      \"name\": \"xcur\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"mousedown, touchstart, touchend\",\n          \"update\": \"xdom\"\n        }\n      ]\n    },\n    {\n      \"name\": \"ycur\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"mousedown, touchstart, touchend\",\n          \"update\": \"ydom\"\n        }\n      ]\n    },\n    {\n      \"name\": \"delta\",\n      \"value\": [0, 0],\n      \"on\": [\n        {\n          \"events\": [\n            {\n              \"source\": \"window\",\n              \"type\": \"mousemove\",\n              \"consume\": true,\n              \"between\": [\n                {\"type\": \"mousedown\"},\n                {\n                  \"source\": \"window\",\n                  \"type\": \"mouseup\"\n                }\n              ]\n            },\n            {\n              \"type\": \"touchmove\",\n              \"consume\": true,\n              \"filter\": \"event.touches.length === 1\"\n            }\n          ],\n          \"update\": \"down ? [down[0]-x(), y()-down[1]] : [0,0]\"\n        }\n      ]\n    },\n    {\n      \"name\": \"anchor\",\n      \"value\": [0, 0],\n      \"on\": [\n        {\n          \"events\": \"wheel\",\n          \"update\": \"[invert('xscale', x()), invert('yscale', y())]\"\n        },\n        {\n          \"events\": {\n            \"type\": \"touchstart\",\n            \"filter\": \"event.touches.length===2\"\n          },\n          \"update\": \"[(xdom[0] + xdom[1]) / 2, (ydom[0] + ydom[1]) / 2]\"\n        }\n      ]\n    },\n    {\n      \"name\": \"zoom\",\n      \"value\": 1,\n      \"on\": [\n        {\n          \"events\": \"wheel!\",\n          \"force\": true,\n          \"update\": \"pow(1.001, event.deltaY * pow(16, event.deltaMode))\"\n        },\n        {\n          \"events\": {\"signal\": \"dist2\"},\n          \"force\": true,\n          \"update\": \"dist1 / dist2\"\n        },\n        {\n          \"events\": [\n            {\n              \"source\": \"view\",\n              \"type\": \"dblclick\"\n            }\n          ],\n          \"update\": \"1\"\n        }\n      ]\n    },\n    {\n      \"name\": \"dist1\",\n      \"value\": 0,\n      \"on\": [\n        {\n          \"events\": {\n            \"type\": \"touchstart\",\n            \"filter\": \"event.touches.length===2\"\n          },\n          \"update\": \"pinchDistance(event)\"\n        },\n        {\n          \"events\": {\"signal\": \"dist2\"},\n          \"update\": \"dist2\"\n        }\n      ]\n    },\n    {\n      \"name\": \"dist2\",\n      \"value\": 0,\n      \"on\": [\n        {\n          \"events\": {\n            \"type\": \"touchmove\",\n            \"consume\": true,\n            \"filter\": \"event.touches.length===2\"\n          },\n          \"update\": \"pinchDistance(event)\"\n        }\n      ]\n    },\n    {\n      \"name\": \"xdom\",\n      \"update\": \"xext\",\n      \"on\": [\n        {\n          \"events\": {\"signal\": \"delta\"},\n          \"update\": \"[xcur[0] + span(xcur) * delta[0] / width, xcur[1] + span(xcur) * delta[0] / width]\"\n        },\n        {\n          \"events\": {\"signal\": \"zoom\"},\n          \"update\": \"[anchor[0] + (xdom[0] - anchor[0]) * zoom, anchor[0] + (xdom[1] - anchor[0]) * zoom]\"\n        },\n        {\n          \"events\": [\n            {\n              \"source\": \"view\",\n              \"type\": \"dblclick\"\n            }\n          ],\n          \"update\": \"xrange\"\n        }\n      ]\n    },\n    {\n      \"name\": \"ydom\",\n      \"update\": \"yext\",\n      \"on\": [\n        {\n          \"events\": {\"signal\": \"delta\"},\n          \"update\": \"[ycur[0] + span(ycur) * delta[1] / height, ycur[1] + span(ycur) * delta[1] / height]\"\n        },\n        {\n          \"events\": {\"signal\": \"zoom\"},\n          \"update\": \"[anchor[1] + (ydom[0] - anchor[1]) * zoom, anchor[1] + (ydom[1] - anchor[1]) * zoom]\"\n        },\n        {\n          \"events\": [\n            {\n              \"source\": \"view\",\n              \"type\": \"dblclick\"\n            }\n          ],\n          \"update\": \"yrange\"\n        }\n      ]\n    },\n    {\n      \"name\": \"size\",\n      \"update\": \"clamp(20 / span(xdom), 1, 1000)\"\n    },\n    {\n      \"name\": \"cx\",\n      \"update\": \"width / 2\",\n      \"on\": [\n        {\n          \"events\": \"[symbol:mousedown, window:mouseup] &gt; window:mousemove\",\n          \"update\": \" cx==width/2?cx+0.001:width/2\"\n        }\n      ]\n    },\n    {\n      \"name\": \"cy\",\n      \"update\": \"height / 2\"\n    },\n    {\n      \"name\": \"nodeRadiusKey\",\n      \"description\": \"q=increase size, a=decrease size\",\n      \"value\": 8,\n      \"on\": [\n        {\n          \"events\": \"window:keypress\",\n          \"update\": \"event.key=='a'&amp;&amp;nodeRadiusKey&gt;1?nodeRadiusKey-1:event.key=='q'?nodeRadiusKey+1:nodeRadiusKey\"\n        }\n      ]\n    },\n    {\n      \"name\": \"nodeRadius\",\n      \"value\": 17,\n      \"on\": [\n        {\n          \"events\": {\n            \"signal\": \"nodeRadiusKey\"\n          },\n          \"update\": \"nodeRadiusKey\"\n        }\n      ]\n    },\n    {\"name\": \"nodeCharge\",\"value\": -0},\n    {\"name\": \"linkDistance\",\"value\": 5\n    },\n    {\n      \"description\": \"State variable for active node fix status.\",\n      \"name\": \"fix\",\n      \"value\": false,\n      \"on\": [\n        {\n          \"events\": \"symbol:mouseout[!event.buttons], window:mouseup\",\n          \"update\": \"false\"\n        },\n        {\n          \"events\": \"symbol:mouseover\",\n          \"update\": \"fix || true\",\n          \"force\": true\n        },\n        {\n          \"events\": \"[symbol:mousedown, window:mouseup] &gt; window:mousemove!\",\n          \"update\": \"xy()\",\n          \"force\": true\n        }\n      ]\n    },\n    {\n      \"description\": \"Graph node most recently interacted with.\",\n      \"name\": \"node\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"symbol:mouseover\",\n          \"update\": \"fix === true ? datum.index : node\"\n        }\n      ]\n    },\n    {\n      \"name\": \"nodeHover\",\n      \"value\": {\n        \"id\": null,\n        \"connections\": []\n      },\n      \"on\": [\n        {\n          \"events\": \"symbol:mouseover\",\n          \"update\": \"{'id':datum.index, 'connections':split(datum.sources+','+datum.targets,',')}\"\n        },\n        {\n          \"events\": \"symbol:mouseout\",\n          \"update\": \"{'id':null, 'connections':[]}\"\n        }\n      ]\n    },\n    {\n      \"description\": \"Flag to restart Force simulation upon data changes.\",\n      \"name\": \"restart\",\n      \"value\": false,\n      \"on\": [\n        {\n          \"events\": {\"signal\": \"fix\"},\n          \"update\": \"fix &amp;&amp; fix.length\"\n        }\n      ]\n    }\n  ],\n  \"data\": [\n    {\"name\": \"dataset\" },\n    {\n      \"name\": \"link-data\",\n      \"source\": \"dataset\",\n      \"transform\": [\n        {\n          \"type\": \"project\",\n          \"fields\": [\"srcId\", \"srcName\", \"srcType\", \"dstId\", \"dstName\", \"dstType\"],\n          \"as\": [\"source\", \"srcName\", \"srcType\", \"target\", \"dstName\", \"dstType\"]\n        }\n      ]\n    },\n    {\n      \"name\": \"source-connections\",\n      \"source\": \"link-data\",\n      \"transform\": [\n        {\n          \"type\": \"aggregate\",\n          \"groupby\": [\"source\"],\n          \"ops\": [\"values\"],\n          \"fields\": [\"source\"],\n          \"as\": [\"connections\"]\n        },\n        {\n          \"type\": \"formula\",\n          \"as\": \"targets\",\n          \"expr\": \"pluck(datum.connections, 'target')\"\n        }\n      ]\n    },\n    {\n      \"name\": \"target-connections\",\n      \"source\": \"link-data\",\n      \"transform\": [\n        {\n          \"type\": \"aggregate\",\n          \"groupby\": [\"target\"],\n          \"ops\": [\"values\"],\n          \"fields\": [\"source\"],///\n          \"as\": [\"connections\"]\n        },\n        {\n          \"type\": \"formula\",\n          \"as\": \"sources\",\n          \"expr\": \"pluck(datum.connections, 'source')\"\n        }\n      ]\n    },\n    {\n      \"name\": \"src\",\n      \"source\": \"dataset\",\n      \"transform\": [\n        {\n          \"type\": \"project\",\n          \"fields\": [\"srcId\", \"srcName\", \"srcType\"],\n          \"as\": [\"id\", \"Name\", \"Type\"]\n        }\n      ]\n    },\n    {\n      \"name\": \"dst\",\n      \"source\": \"dataset\",\n      \"transform\": [\n        {\n          \"type\": \"project\",\n          \"fields\": [\"dstId\", \"dstName\", \"dstType\"],\n          \"as\": [\"id\", \"Name\", \"Type\"]\n        }\n      ]\n    },\n    {\n      \"name\": \"vertices\",\n      \"source\": [\"src\", \"dst\"],\n      \"transform\": [\n        {\n          \"type\": \"aggregate\",\n          \"groupby\": [\"id\", \"Name\", \"Type\"]\n        },\n        {\n          \"type\": \"project\",\n          \"fields\": [\"id\", \"Name\", \"Type\"]\n        }\n      ]\n    },\n    {\n      \"name\": \"node-data\",\n      \"source\": \"vertices\",\n      \"transform\": [\n        {\n          \"type\": \"lookup\",\n          \"from\": \"source-connections\",\n          \"key\": \"source\",\n          \"fields\": [\"id\"],\n          \"values\": [\"targets\"],\n          \"as\": [\"targets\"],\n          \"default\": [\"\"]\n        },\n        {\n          \"type\": \"lookup\",\n          \"from\": \"target-connections\",\n          \"key\": \"target\",\n          \"fields\": [\"id\"],\n          \"values\": [\"sources\"],\n          \"as\": [\"sources\"],\n          \"default\": [\"\"]\n        },\n        {\n          \"type\": \"force\",\n          \"iterations\": 300,\n          \"restart\": {\n            \"signal\": \"restart\"\n          },\n          \"signal\": \"force\",\n          \"forces\": [\n            {\n              \"force\": \"center\",\n              \"x\": {\"signal\": \"cx\"},\n              \"y\": {\"signal\": \"cy\"}\n            },\n            {\n              \"force\": \"collide\",\n              \"radius\": {\n                \"signal\": \"sqrt(4 * nodeRadius * nodeRadius)\"\n              },\n              \"iterations\": 1,\n              \"strength\": 0.7\n            },\n            {\n              \"force\": \"nbody\",\n              \"strength\": {\n                \"signal\": \"nodeCharge\"\n              }\n            },\n            {\n              \"force\": \"link\",\n              \"links\": \"link-data\",\n              \"distance\": {\n                \"signal\": \"linkDistance\"\n              },\n              \"id\": \"id\"\n            }\n          ]\n        },\n        {\n          \"type\": \"formula\",\n          \"as\": \"fx\",\n          \"expr\": \"fix[0]!=null &amp;&amp; node==datum.index ?invert('xscale',fix[0]):null\"\n        },\n        {\n          \"type\": \"formula\",\n          \"as\": \"fy\",\n          \"expr\": \"fix[1]!=null &amp;&amp; node==datum.index ?invert('yscale',fix[1]):null\"\n        }\n      ]\n    }\n  ],\n  \"scales\": [\n    {\n      \"name\": \"color\",\n      \"type\": \"ordinal\",\n      \"domain\": [\"Workspace\", \"Dataset\", \"Report\", \"Report App\", \"Group\", \"User\", \"App\"],\n      \"range\": [\"#8661c5\", \"#01b8aa\", \"#FFB900\", \"#634e15\", \"#0078d4\", \"#999999\", \"#5A7378\"]\n    },\n    {\n      \"name\": \"xscale\",\n      \"zero\": false,\n      \"domain\": {\"signal\": \"xdom\"},\n      \"range\": {\"signal\": \"xrange\"}\n    },\n    {\n      \"name\": \"yscale\",\n      \"zero\": false,\n      \"domain\": {\"signal\": \"ydom\"},\n      \"range\": {\"signal\": \"yrange\"}\n    }\n  ],\n  \"legends\": [\n    {\n      \"fill\": \"color\",\n      \"encode\": {\n        \"title\": {\n          \"update\": {\n            \"fontSize\": {\"value\": 10}\n          }\n        },\n        \"labels\": {\n          \"interactive\": true,\n          \"update\": {\n            \"fontSize\": {\"value\": 10},\n            \"fill\": {\"value\": \"black\"}\n          }\n        },\n        \"symbols\": {\n          \"update\": {\n            \"stroke\": {\"value\": \"transparent\"}\n          }\n        },\n        \"legend\": {\n          \"update\": {\n            \"stroke\": {\"value\": \"#ccc\"},\n            \"strokeWidth\": {\"value\": 0}\n          }\n        }\n      }\n    }\n  ],\n  \"marks\": [\n\n    {\n      \"type\": \"path\",\n      \"name\": \"links\",\n      \"from\": {\"data\": \"link-data\"},\n      \"interactive\": false,\n      \"encode\": {\n        \"update\": {\n          \"stroke\": {\n            \"signal\": \"datum.source.index!=nodeHover.id &amp;&amp; datum.target.index!=nodeHover.id ? '#929399':merge(hsl(scale('color', datum.source.Type)), {l:0.64})\"\n          },\n          \"strokeWidth\": {\n            \"signal\": \"datum.source.index!=nodeHover.id &amp;&amp; datum.target.index!=nodeHover.id ? 0.2:1\"\n          }\n        }\n      },\n      \"transform\": [\n        {\n          \"type\": \"linkpath\",\n          \"require\": {\n            \"signal\": \"force\"\n          },\n          \"shape\": \"line\",\n          \"sourceX\": {\n            \"expr\": \"scale('xscale', datum.datum.source.x)\"\n          },\n          \"sourceY\": {\n            \"expr\": \"scale('yscale', datum.datum.source.y)\"\n          },\n          \"targetX\": {\n            \"expr\": \"scale('xscale', datum.datum.target.x)\"\n          },\n          \"targetY\": {\n            \"expr\": \"scale('yscale', datum.datum.target.y)\"\n          }\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"atan2(datum.datum.target.y - datum.datum.source.y,datum.datum.source.x - datum.datum.target.x)\",\n          \"as\": \"angle1\"\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"(datum.angle1&gt;=0?datum.angle1:(2*PI + datum.angle1)) * (360 / (2*PI))\",\n          \"as\": \"angle2\"\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"(360-datum.angle2)*(PI/180)\",\n          \"as\": \"angle3\"\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"(cos(datum.angle3)*(nodeRadius+5))+(scale('xscale',datum.datum.target.x))\",\n          \"as\": \"arrowX\"\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"(sin(datum.angle3)*(nodeRadius+5))+(scale('yscale',datum.datum.target.y))\",\n          \"as\": \"arrowY\"\n        }\n      ]\n    },\n    {\n      \"type\": \"symbol\",\n      \"name\": \"arrows\",\n      \"zindex\": 1,\n      \"from\": {\"data\": \"links\"},\n      \"encode\": {\n        \"update\": {\n          \"shape\": {\n            \"value\": \"triangle\"\n          },\n          \"angle\": {\n            \"signal\": \"-datum.angle2-90\"\n          },\n          \"x\": {\n            \"signal\": \"datum.arrowX\"\n          },\n          \"y\": {\n            \"signal\": \"datum.arrowY\"\n          },\n          \"text\": {\"signal\": \"'\u25b2'\"},\n          \"fill\": {\n            \"signal\": \"datum.datum.source.index!=nodeHover.id &amp;&amp; datum.datum.target.index!=nodeHover.id ? '#929399':merge(hsl(scale('color', datum.datum.source.Type)), {l:0.64})\"\n          },\n          \"size\": {\n            \"signal\": \"nodeRadius==1?0:30\"\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"nodes\",\n      \"type\": \"symbol\",\n      \"zindex\": 1,\n      \"from\": {\"data\": \"node-data\"},\n      \"encode\": {\n        \"update\": {\n          \"opacity\": {\"value\": 1},\n          \"fill\": {\n            \"signal\": \"nodeHover.id===datum.index || indexof(nodeHover.connections, datum.id)&gt;-1 ?scale('color', datum.Type):merge(hsl(scale('color', datum.Type)), {l:0.64})\"\n          },\n          \"stroke\": {\n            \"signal\": \"nodeHover.id===datum.index || indexof(nodeHover.connections, datum.id)&gt;-1 ?scale('color', datum.Type):merge(hsl(scale('color', datum.Type)), {l:0.84})\"\n          },\n          \"strokeWidth\": {\"value\": 0.5},\n          // \"strokeOpacity\": {\"value\": 1},\n          \"size\": {\"signal\": \"4 * nodeRadius * nodeRadius\"},\n          \"cursor\": {\"value\": \"pointer\"},\n          \"x\": {\"signal\": \"fix[0]!=null &amp;&amp; node===datum.index ?fix[0]:scale('xscale', datum.x)\"},\n          \"y\": {\"signal\": \"fix[1]!=null &amp;&amp; node===datum.index ?fix[1]:scale('yscale', datum.y)\"}\n        },\n        \"hover\": {\n          \"tooltip\": {\n            \"signal\": \"datum.Name\"\n          }\n        }\n      }\n    },\n    {\n      \"type\": \"text\",\n      \"name\": \"labels\",\n      \"from\": {\"data\": \"nodes\"},\n      \"zindex\": 2,\n      \"interactive\": false,\n      \"enter\": {},\n      \"encode\": {\n        \"update\": {\n          \"fill\": {\"signal\": \"'black'\"},\n          \"y\": {\"field\": \"y\"},\n          \"x\": {\"field\": \"x\"},\n          \"text\": {\n            \"field\": \"datum.Name\"\n          },\n          \"align\": {\"value\": \"center\"},\n          \"fontSize\": {\"value\": 10},\n          \"baseline\": {\n            \"value\": \"middle\"\n          },\n          \"limit\": {\n            \"signal\": \"clamp(sqrt(4 * nodeRadius * nodeRadius)-5,1,1000)\"\n          },\n          \"ellipsis\": {\"value\": \" \"}\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"posts/query-plan-graph/","title":"Graphing DAX Query Plans","text":"<p>Ever since completing SQLBI's Optimizing DAX course I started looking at DAX query plans in more depth. I've found the fully textual plans hard to parse, with only indents to denote nested operations. This is fine until you get multiple paths nested below the same node, it hard to keep the full execution plan in your head. I've always liked SQL servers graphical query plans so I want to investigate how to build the same.</p>"},{"location":"posts/query-plan-graph/#query","title":"Query","text":"<p>We are going to be looking at the execution of the following query, from a previous post.</p> <pre><code>EVALUATE\nSUMMARIZECOLUMNS(\n    'Calendar'[Date],\n    'States'[State],\n    \"Using_TOPN\", [Using TopN]\n)\n</code></pre>"},{"location":"posts/query-plan-graph/#sql-server-profiler","title":"SQL Server Profiler","text":"<p>The Query Plan can be obtained from running SQL Server Profiler and grabbing the <code>DAX Query Plan</code> event.</p> <p></p> <p>Clear Cache</p> <p>Clear cache prior to query execution to avoid using the cache </p><pre><code>&lt;Batch xmlns=\"http://schemas.microsoft.com/analysisservices/2003/engine\"&gt;\n  &lt;ClearCache&gt;\n    &lt;Object&gt;\n      &lt;DatabaseID&gt;SemanticModel_ABC&lt;/DatabaseID&gt;\n    &lt;/Object&gt;\n  &lt;/ClearCache&gt;\n&lt;/Batch&gt;\n</code></pre><p></p> <p>When we run our query we see two events, the Logical query plan and the Physical query plan. The SQLBI guys go into detail on DAX query plans in their Optimizing DAX course, plus they also have a white paper on the subject. The Logical Query Plan is created first, and resembles the DAX query. This is then converted the Physical Plan for execution. We can see a number of rows with increasing indent. Each increase in indent represents a nested operation. So we need to read the plan from the leaf to the root to understand how data stored in SSAS Tabular is being processed to return the output of the query.</p> <p></p> <p>My main bugbear with this is that if you have several groups of operations that all use the same step, it can be quite hard to keep track of all the interactions. What I'd really like is a graphical view like you get from SQL Server.</p> <p> SQL Server Query Plan </p> <p>Lets save this trace: File &gt; Save As &gt; Trace XML File... &gt; <code>trace.xml</code></p>"},{"location":"posts/query-plan-graph/#processing-xml-trace","title":"Processing XML Trace","text":"<p>We now need to process the XML to generate the appropriate data structure to generate a graph.</p> CodeOutput <pre><code>import xmltodict\nimport json\nimport re\n\n# XML to dict\nEventSubClass ={\n    1: 'logical',\n    2: 'physical'\n}\nqueries = {}\n\nwith open('trace.xml', encoding='utf-16') as fd:\n    queryPlan = xmltodict.parse(fd.read(), encoding='utf-16')\n\nfor event in queryPlan['TraceData']['Events']['Event']:\n    if event['@name'] == 'DAX Query Plan':\n\n        for column in event['Column']:\n\n            if column['@name'] == 'TextData':\n                query = column['#text']\n\n            if column['@name'] == 'EventSubclass':\n                queryType = EventSubClass[int(column['#text'])]\n\n        queries[queryType] = query\n\n# Generate Graph\ndef extract_records(line) -&gt; int:\n    match = re.search(r'#Records=(\\d+)', line)\n    if match:\n        return int(match.group(1))\n    return None\n\ndef extract_operation_type(line) -&gt; list:\n    match = re.search(r'(.*): (\\w*)', line)\n    if match:\n        return [match.group(1).strip(), match.group(2).strip()]\n    return None\n\ndef generate_graph(lines: list)-&gt;list:\n    stack = []\n    level_parents = {}\n    graph = []\n\n    for index, line in enumerate(lines):\n        current_level = len(line) - len(line.lstrip())\n\n        while len(stack) &gt; current_level:\n            stack.pop()\n\n        parent_index = level_parents.get(current_level - 1, None)\n\n        stack.append((index, line))\n\n        level_parents[current_level] = index\n\n        operationType = extract_operation_type(line)\n\n        graph.append({\n            'srcid': parent_index,\n            'dstid': index,\n            'operation': line.strip(),\n            'operationShort': operationType[0],\n            'operationType': operationType[1],\n            'isCache': operationType[0] == 'Cache',\n            'level': current_level,\n            'records': extract_records(line)\n        })\n\n    return graph\n\ngraphs = {}\n\nfor queryType, query in queries.items():\n    graphs[queryType] = generate_graph(lines = query.split('\\n'))\n\nwith open('queryPlan.json', 'w') as fp:\n    json.dump(graphs, fp)\n</code></pre> <pre><code>{\n  \"logical\": [\n    {\n      \"srcid\": null,\n      \"dstid\": 0,\n      \"operation\": \"GroupSemiJoin: RelLogOp DependOnCols()() 0-2 RequiredCols(0, 1, 2)('Calendar'[Date], 'States'[State], ''[Using_TOPN])\",\n      \"operationShort\": \"GroupSemiJoin\",\n      \"operationType\": \"RelLogOp\",\n      \"isCache\": false,\n      \"level\": 0,\n      \"records\": null\n    },\n    {\n      \"srcid\": 0,\n      \"dstid\": 1,\n      \"operation\": \"Scan_Vertipaq: RelLogOp DependOnCols()() 0-0 RequiredCols(0)('Calendar'[Date])\",\n      \"operationShort\": \"Scan_Vertipaq\",\n      \"operationType\": \"RelLogOp\",\n      \"isCache\": false,\n      \"level\": 1,\n      \"records\": null\n    },\n    ...\n  ],\n  \"physical\": [\n    {\n      \"srcid\": null,\n      \"dstid\": 0,\n      \"operation\": \"GroupSemijoin: IterPhyOp LogOp=GroupSemiJoin IterCols(0, 1, 2)('Calendar'[Date], 'States'[State], ''[Using_TOPN])\",\n      \"operationShort\": \"GroupSemijoin\",\n      \"operationType\": \"IterPhyOp\",\n      \"isCache\": false,\n      \"level\": 0,\n      \"records\": null\n    },\n    {\n      \"srcid\": 0,\n      \"dstid\": 1,\n      \"operation\": \"Spool_Iterator&lt;SpoolIterator&gt;: IterPhyOp LogOp=VarScope IterCols(0, 1)('Calendar'[Date], 'States'[State]) #Records=13413 #KeyCols=2 #ValueCols=1\",\n      \"operationShort\": \"Spool_Iterator&lt;SpoolIterator&gt;\",\n      \"operationType\": \"IterPhyOp\",\n      \"isCache\": false,\n      \"level\": 1,\n      \"records\": 13413\n    },\n    ...\n  ]\n}\n</code></pre>"},{"location":"posts/query-plan-graph/#graphs","title":"Graphs","text":"<p>We can now use Vega to draw the graph.</p>"},{"location":"posts/query-plan-graph/#logical-query-plan","title":"Logical Query Plan","text":"VisualCode <p>{   \"$schema\": \"https://vega.github.io/schema/vega/v5.json\",   \"description\": \"A force-directed graph for a DAX Query Plan\",   \"width\": 700,   \"height\": 500,   \"padding\": 5,   \"autosize\": \"none\",   \"background\": \"#263238\",   \"signals\": [     {\"name\": \"cx\", \"update\": \"width / 2\"},     {\"name\": \"cy\", \"update\": \"height / 2\"},     {\"name\": \"nodeRadius\", \"value\": 8},     {\"name\": \"nodeCharge\", \"value\": -30},     {\"name\": \"linkDistance\", \"value\": 30},     {\"name\": \"static\", \"value\": false},     {\"name\": \"gravityX\", \"value\": 0.2},     {\"name\": \"gravityY\", \"value\": 0.9},     {       \"description\": \"State variable for active node fix status.\",       \"name\": \"fix\",       \"value\": false,       \"on\": [         {           \"events\": \"symbol:pointerout[!event.buttons], window:pointerup\",           \"update\": \"false\"         },         {\"events\": \"symbol:pointerover\", \"update\": \"fix || true\"},         {           \"events\": \"[symbol:pointerdown, window:pointerup] &gt; window:pointermove!\",           \"update\": \"xy()\",           \"force\": true         }       ]     },     {       \"description\": \"Graph node most recently interacted with.\",       \"name\": \"node\",       \"value\": null,       \"on\": [         {           \"events\": \"symbol:pointerover\",           \"update\": \"fix === true ? item() : node\"         }       ]     },     {       \"description\": \"Flag to restart Force simulation upon data changes.\",       \"name\": \"restart\",       \"value\": false,       \"on\": [{\"events\": {\"signal\": \"fix\"}, \"update\": \"fix &amp;&amp; fix.length\"}]     }   ],   \"data\": [     {       \"name\": \"node-data\",       \"url\": \"https://raw.githubusercontent.com/EvaluationContext/evaluationcontext.github.io/refs/heads/master/Resources/blog/2025-01-28-QueryPlanGraph/queryPlan.json\",       \"format\": {\"type\": \"json\", \"property\": \"logical\"},       \"transform\": [         {           \"type\": \"project\",           \"fields\": [\"dstid\", \"operation\", \"level\", \"records\", \"operationType\", \"operationShort\"],           \"as\": [\"index\", \"Operation\", \"Level\", \"Records\"]         },         {               \"type\": \"formula\",               \"expr\": \"length(datum.operationShort) * (width / 160)\",               \"as\": \"operationShortLen\"         }       ]     },     {       \"name\": \"link-data\",       \"url\": \"https://raw.githubusercontent.com/EvaluationContext/evaluationcontext.github.io/refs/heads/master/Resources/blog/2025-01-28-QueryPlanGraph/queryPlan.json\",       \"format\": {\"type\": \"json\", \"property\": \"logical\"},       \"transform\": [         {\"type\": \"filter\", \"expr\": \"datum.srcid != null\"},         {           \"type\": \"project\",           \"fields\": [\"srcid\", \"dstid\"],           \"as\": [\"source\", \"target\"]         }       ]     }   ],    \"legends\": [     {       \"direction\": \"vertical\",       \"legendX\": 1050,       \"legendY\": 0,       \"orient\": \"none\",       \"stroke\": \"color\",       \"fill\": \"color\",       \"titleColor\": \"#b6bcdd\",       \"padding\": 10,       \"encode\": {         \"title\": { \"update\": { \"fontSize\": { \"value\": 14 } } },         \"symbols\": { \"update\": { \"fillOpacity\": { \"value\": 0.4 } } },         \"labels\": {           \"update\": {             \"fontSize\": { \"value\": 12 },             \"fill\": { \"value\": \"#b6bcdd\" }           }         }       }     }   ],    \"scales\": [     {       \"name\": \"xscale\",       \"type\": \"band\",       \"domain\": {\"data\": \"node-data\", \"field\": \"Level\", \"sort\": true},       \"paddingOuter\": 1,       \"range\": \"width\"     },     {       \"name\": \"yscale\",       \"type\": \"band\",       \"domain\": {\"data\": \"node-data\", \"field\": \"index\", \"sort\": true},       \"range\": \"height\"     },     {       \"name\": \"color\",       \"type\": \"ordinal\",       \"domain\": {\"data\": \"node-data\", \"field\": \"operationType\"},       \"range\": {\"scheme\": \"tableau10\"}     },     {       \"name\": \"size\",       \"domain\": {\"data\": \"node-data\", \"field\": \"Records\"},       \"range\": [300, 2000]     }   ],    \"marks\": [     {       \"name\": \"nodes\",       \"type\": \"symbol\",       \"zindex\": 1,       \"from\": {\"data\": \"node-data\"},       \"on\": [         {           \"trigger\": \"fix\",           \"modify\": \"node\",           \"values\": \"fix === true ? {fx: node.x, fy: node.y} : {fx: fix[0], fy: fix[1]}\"         },         {\"trigger\": \"!fix\", \"modify\": \"node\", \"values\": \"{fx: null, fy: null}\"}       ],       \"encode\": {         \"enter\": {           \"size\": {\"scale\": \"size\", \"field\": \"Records\"},           \"stroke\": {\"scale\": \"color\", \"field\": \"operationType\"},           \"fill\": {\"scale\": \"color\", \"field\": \"operationType\"},           \"fillOpacity\": {\"value\": 0.1},            \"xfocus\": {\"scale\": \"xscale\", \"field\": \"Level\", \"band\": 0.5},           \"yfocus\": {\"scale\": \"yscale\", \"field\": \"index\", \"band\": 0.5},           \"tooltip\": {             \"signal\": \"{OperationType: datum.operationType, Operation: datum.Operation, Records: datum.Records}\"           }         },         \"update\": {           \"cursor\": {\"value\": \"pointer\"}         }       },       \"transform\": [         {           \"type\": \"force\",           \"iterations\": 300,           \"restart\": {\"signal\": \"restart\"},           \"static\": {\"signal\": \"static\"},           \"signal\": \"force\",           \"forces\": [             {\"force\": \"center\", \"x\": {\"signal\": \"cx\"}, \"y\": {\"signal\": \"cy\"}},             {\"force\": \"collide\", \"radius\": {\"signal\": \"nodeRadius\"}},             {\"force\": \"nbody\", \"strength\": {\"signal\": \"nodeCharge\"}},             {               \"force\": \"link\",               \"links\": \"link-data\",               \"distance\": {\"signal\": \"linkDistance\"},               \"id\": \"index\"             },             {\"force\": \"x\", \"x\": \"xfocus\", \"strength\": {\"signal\": \"gravityX\"}},             {\"force\": \"y\", \"y\": \"yfocus\", \"strength\": {\"signal\": \"gravityY\"}}           ]         }       ]     },     {       \"type\": \"path\",       \"from\": {\"data\": \"link-data\"},       \"interactive\": false,       \"encode\": {         \"update\": {\"stroke\": {\"value\": \"#ccc\"}, \"strokeWidth\": {\"value\": 0.5}}       },       \"transform\": [         {           \"type\": \"linkpath\",           \"require\": {\"signal\": \"force\"},           \"shape\": \"line\",           \"sourceX\": \"datum.source.x\",           \"sourceY\": \"datum.source.y\",           \"targetX\": \"datum.target.x\",           \"targetY\": \"datum.target.y\"         }       ]     },     {       \"type\": \"text\",       \"name\": \"labels\",       \"from\": {\"data\": \"nodes\"},        \"zindex\": 2,       \"interactive\": false,       \"enter\": {},       \"encode\": {         \"update\": {           \"fill\": {\"signal\": \"'white'\"},           \"y\": {\"field\": \"y\"},           \"x\": {\"field\": \"x\"},           \"text\": {\"field\": \"datum.operationShort\"},           \"align\": {\"value\": \"left\"},           \"fontSize\": {\"value\": 10},           \"baseline\": {\"value\": \"top\"},           \"limit\": {             \"signal\": \"clamp(60, 1, 1000)\"           },           \"ellipsis\": {\"value\": \" \"}         }       }     }   ] }</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega/v5.json\",\n  \"description\": \"A force-directed graph for a DAX Query Plan\",\n  \"width\": 700,\n  \"height\": 500,\n  \"padding\": 5,\n  \"autosize\": \"none\",\n  \"background\": \"#263238\",\n  \"signals\": [\n    {\"name\": \"cx\", \"update\": \"width / 2\"},\n    {\"name\": \"cy\", \"update\": \"height / 2\"},\n    {\"name\": \"nodeRadius\", \"value\": 8},\n    {\"name\": \"nodeCharge\", \"value\": -30},\n    {\"name\": \"linkDistance\", \"value\": 30},\n    {\"name\": \"static\", \"value\": false},\n    {\"name\": \"gravityX\", \"value\": 0.2},\n    {\"name\": \"gravityY\", \"value\": 0.9},\n    {\n      \"description\": \"State variable for active node fix status.\",\n      \"name\": \"fix\",\n      \"value\": false,\n      \"on\": [\n        {\n          \"events\": \"symbol:pointerout[!event.buttons], window:pointerup\",\n          \"update\": \"false\"\n        },\n        {\"events\": \"symbol:pointerover\", \"update\": \"fix || true\"},\n        {\n          \"events\": \"[symbol:pointerdown, window:pointerup] &gt; window:pointermove!\",\n          \"update\": \"xy()\",\n          \"force\": true\n        }\n      ]\n    },\n    {\n      \"description\": \"Graph node most recently interacted with.\",\n      \"name\": \"node\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"symbol:pointerover\",\n          \"update\": \"fix === true ? item() : node\"\n        }\n      ]\n    },\n    {\n      \"description\": \"Flag to restart Force simulation upon data changes.\",\n      \"name\": \"restart\",\n      \"value\": false,\n      \"on\": [{\"events\": {\"signal\": \"fix\"}, \"update\": \"fix &amp;&amp; fix.length\"}]\n    }\n  ],\n  \"data\": [\n    {\n      \"name\": \"node-data\",\n      \"url\": \"https://raw.githubusercontent.com/EvaluationContext/evaluationcontext.github.io/refs/heads/master/assets/vega/0020-queryPlanGraph/queryPlan.json\",\n      \"format\": {\"type\": \"json\", \"property\": \"logical\"},\n      \"transform\": [\n        {\n          \"type\": \"project\",\n          \"fields\": [\"dstid\", \"operation\", \"level\", \"records\", \"operationType\", \"operationShort\"],\n          \"as\": [\"index\", \"Operation\", \"Level\", \"Records\"]\n        },\n        {\n              \"type\": \"formula\",\n              \"expr\": \"length(datum.operationShort) * (width / 160)\",\n              \"as\": \"operationShortLen\"\n        }\n      ]\n    },\n    {\n      \"name\": \"link-data\",\n      \"url\": \"https://raw.githubusercontent.com/EvaluationContext/evaluationcontext.github.io/refs/heads/master/assets/vega/0020-queryPlanGraph/queryPlan.json\",\n      \"format\": {\"type\": \"json\", \"property\": \"logical\"},\n      \"transform\": [\n        {\"type\": \"filter\", \"expr\": \"datum.srcid != null\"},\n        {\n          \"type\": \"project\",\n          \"fields\": [\"srcid\", \"dstid\"],\n          \"as\": [\"source\", \"target\"]\n        }\n      ]\n    }\n  ],\n\n  \"legends\": [\n    {\n      \"direction\": \"vertical\",\n      \"legendX\": 1050,\n      \"legendY\": 0,\n      \"orient\": \"none\",\n      \"stroke\": \"color\",\n      \"fill\": \"color\",\n      \"titleColor\": \"#b6bcdd\",\n      \"padding\": 10,\n      \"encode\": {\n        \"title\": { \"update\": { \"fontSize\": { \"value\": 14 } } },\n        \"symbols\": { \"update\": { \"fillOpacity\": { \"value\": 0.4 } } },\n        \"labels\": {\n          \"update\": {\n            \"fontSize\": { \"value\": 12 },\n            \"fill\": { \"value\": \"#b6bcdd\" }\n          }\n        }\n      }\n    }\n  ],\n\n  \"scales\": [\n    {\n      \"name\": \"xscale\",\n      \"type\": \"band\",\n      \"domain\": {\"data\": \"node-data\", \"field\": \"Level\", \"sort\": true},\n      \"paddingOuter\": 1,\n      \"range\": \"width\"\n    },\n    {\n      \"name\": \"yscale\",\n      \"type\": \"band\",\n      \"domain\": {\"data\": \"node-data\", \"field\": \"index\", \"sort\": true},\n      \"range\": \"height\"\n    },\n    {\n      \"name\": \"color\",\n      \"type\": \"ordinal\",\n      \"domain\": {\"data\": \"node-data\", \"field\": \"operationType\"},\n      \"range\": {\"scheme\": \"tableau10\"}\n    },\n    {\n      \"name\": \"size\",\n      \"domain\": {\"data\": \"node-data\", \"field\": \"Records\"},\n      \"range\": [300, 2000]\n    }\n  ],\n\n  \"marks\": [\n    {\n      \"name\": \"nodes\",\n      \"type\": \"symbol\",\n      \"zindex\": 1,\n      \"from\": {\"data\": \"node-data\"},\n      \"on\": [\n        {\n          \"trigger\": \"fix\",\n          \"modify\": \"node\",\n          \"values\": \"fix === true ? {fx: node.x, fy: node.y} : {fx: fix[0], fy: fix[1]}\"\n        },\n        {\"trigger\": \"!fix\", \"modify\": \"node\", \"values\": \"{fx: null, fy: null}\"}\n      ],\n      \"encode\": {\n        \"enter\": {\n          \"size\": {\"scale\": \"size\", \"field\": \"Records\"},\n          \"stroke\": {\"scale\": \"color\", \"field\": \"operationType\"},\n          \"fill\": {\"scale\": \"color\", \"field\": \"operationType\"},\n          \"fillOpacity\": {\"value\": 0.1}, \n          \"xfocus\": {\"scale\": \"xscale\", \"field\": \"Level\", \"band\": 0.5},\n          \"yfocus\": {\"scale\": \"yscale\", \"field\": \"index\", \"band\": 0.5},\n          \"tooltip\": {\n            \"signal\": \"{OperationType: datum.operationType, Operation: datum.Operation, Records: datum.Records}\"\n          }\n        },\n        \"update\": {\n          \"cursor\": {\"value\": \"pointer\"}\n        }\n      },\n      \"transform\": [\n        {\n          \"type\": \"force\",\n          \"iterations\": 300,\n          \"restart\": {\"signal\": \"restart\"},\n          \"static\": {\"signal\": \"static\"},\n          \"signal\": \"force\",\n          \"forces\": [\n            {\"force\": \"center\", \"x\": {\"signal\": \"cx\"}, \"y\": {\"signal\": \"cy\"}},\n            {\"force\": \"collide\", \"radius\": {\"signal\": \"nodeRadius\"}},\n            {\"force\": \"nbody\", \"strength\": {\"signal\": \"nodeCharge\"}},\n            {\n              \"force\": \"link\",\n              \"links\": \"link-data\",\n              \"distance\": {\"signal\": \"linkDistance\"},\n              \"id\": \"index\"\n            },\n            {\"force\": \"x\", \"x\": \"xfocus\", \"strength\": {\"signal\": \"gravityX\"}},\n            {\"force\": \"y\", \"y\": \"yfocus\", \"strength\": {\"signal\": \"gravityY\"}}\n          ]\n        }\n      ]\n    },\n    {\n      \"type\": \"path\",\n      \"from\": {\"data\": \"link-data\"},\n      \"interactive\": false,\n      \"encode\": {\n        \"update\": {\"stroke\": {\"value\": \"#ccc\"}, \"strokeWidth\": {\"value\": 0.5}}\n      },\n      \"transform\": [\n        {\n          \"type\": \"linkpath\",\n          \"require\": {\"signal\": \"force\"},\n          \"shape\": \"line\",\n          \"sourceX\": \"datum.source.x\",\n          \"sourceY\": \"datum.source.y\",\n          \"targetX\": \"datum.target.x\",\n          \"targetY\": \"datum.target.y\"\n        }\n      ]\n    },\n    {\n      \"type\": \"text\",\n      \"name\": \"labels\",\n      \"from\": {\"data\": \"nodes\"},\n\n      \"zindex\": 2,\n      \"interactive\": false,\n      \"enter\": {},\n      \"encode\": {\n        \"update\": {\n          \"fill\": {\"signal\": \"'white'\"},\n          \"y\": {\"field\": \"y\"},\n          \"x\": {\"field\": \"x\"},\n          \"text\": {\"field\": \"datum.operationShort\"},\n          \"align\": {\"value\": \"left\"},\n          \"fontSize\": {\"value\": 10},\n          \"baseline\": {\"value\": \"top\"},\n          \"limit\": {\n            \"signal\": \"clamp(60, 1, 1000)\"\n          },\n          \"ellipsis\": {\"value\": \" \"}\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"posts/query-plan-graph/#physical-query-plan","title":"Physical Query Plan","text":"VisualCode <p>{   \"$schema\": \"https://vega.github.io/schema/vega/v5.json\",   \"description\": \"A force-directed graph for a DAX Query Plan\",   \"width\": 700,   \"height\": 500,   \"padding\": 5,   \"autosize\": \"none\",   \"background\": \"#263238\",   \"signals\": [     {\"name\": \"cx\", \"update\": \"width / 2\"},     {\"name\": \"cy\", \"update\": \"height / 2\"},     {\"name\": \"nodeRadius\", \"value\": 8},     {\"name\": \"nodeCharge\", \"value\": -30},     {\"name\": \"linkDistance\", \"value\": 30},     {\"name\": \"static\", \"value\": false},     {\"name\": \"gravityX\", \"value\": 0.2},     {\"name\": \"gravityY\", \"value\": 0.9},     {       \"description\": \"State variable for active node fix status.\",       \"name\": \"fix\",       \"value\": false,       \"on\": [         {           \"events\": \"symbol:pointerout[!event.buttons], window:pointerup\",           \"update\": \"false\"         },         {\"events\": \"symbol:pointerover\", \"update\": \"fix || true\"},         {           \"events\": \"[symbol:pointerdown, window:pointerup] &gt; window:pointermove!\",           \"update\": \"xy()\",           \"force\": true         }       ]     },     {       \"description\": \"Graph node most recently interacted with.\",       \"name\": \"node\",       \"value\": null,       \"on\": [         {           \"events\": \"symbol:pointerover\",           \"update\": \"fix === true ? item() : node\"         }       ]     },     {       \"description\": \"Flag to restart Force simulation upon data changes.\",       \"name\": \"restart\",       \"value\": false,       \"on\": [{\"events\": {\"signal\": \"fix\"}, \"update\": \"fix &amp;&amp; fix.length\"}]     }   ],   \"data\": [     {       \"name\": \"node-data\",       \"url\": \"https://raw.githubusercontent.com/EvaluationContext/evaluationcontext.github.io/refs/heads/master/Resources/blog/2025-01-28-QueryPlanGraph/queryPlan.json\",       \"format\": {\"type\": \"json\", \"property\": \"physical\"},       \"transform\": [         {           \"type\": \"project\",           \"fields\": [\"dstid\", \"operation\", \"level\", \"records\", \"operationType\", \"operationShort\"],           \"as\": [\"index\", \"Operation\", \"Level\", \"Records\"]         },         {               \"type\": \"formula\",               \"expr\": \"length(datum.operationShort) * (width / 160)\",               \"as\": \"operationShortLen\"         }       ]     },     {       \"name\": \"link-data\",       \"url\": \"https://raw.githubusercontent.com/EvaluationContext/evaluationcontext.github.io/refs/heads/master/Resources/blog/2025-01-28-QueryPlanGraph/queryPlan.json\",       \"format\": {\"type\": \"json\", \"property\": \"physical\"},       \"transform\": [         {\"type\": \"filter\", \"expr\": \"datum.srcid != null\"},         {           \"type\": \"project\",           \"fields\": [\"srcid\", \"dstid\"],           \"as\": [\"source\", \"target\"]         }       ]     }   ],    \"legends\": [     {       \"direction\": \"vertical\",       \"legendX\": 1050,       \"legendY\": 0,       \"orient\": \"none\",       \"stroke\": \"color\",       \"fill\": \"color\",       \"titleColor\": \"#b6bcdd\",       \"padding\": 10,       \"encode\": {         \"title\": { \"update\": { \"fontSize\": { \"value\": 14 } } },         \"symbols\": { \"update\": { \"fillOpacity\": { \"value\": 0.4 } } },         \"labels\": {           \"update\": {             \"fontSize\": { \"value\": 12 },             \"fill\": { \"value\": \"#b6bcdd\" }           }         }       }     }   ],    \"scales\": [     {       \"name\": \"xscale\",       \"type\": \"band\",       \"domain\": {\"data\": \"node-data\", \"field\": \"Level\", \"sort\": true},       \"paddingOuter\": 1,       \"range\": \"width\"     },     {       \"name\": \"yscale\",       \"type\": \"band\",       \"domain\": {\"data\": \"node-data\", \"field\": \"index\", \"sort\": true},       \"range\": \"height\"     },     {       \"name\": \"color\",       \"type\": \"ordinal\",       \"domain\": {\"data\": \"node-data\", \"field\": \"operationType\"},       \"range\": {\"scheme\": \"tableau10\"}     },     {       \"name\": \"size\",       \"domain\": {\"data\": \"node-data\", \"field\": \"Records\"},       \"range\": [300, 2000]     }   ],    \"marks\": [     {       \"name\": \"nodes\",       \"type\": \"symbol\",       \"zindex\": 1,       \"from\": {\"data\": \"node-data\"},       \"on\": [         {           \"trigger\": \"fix\",           \"modify\": \"node\",           \"values\": \"fix === true ? {fx: node.x, fy: node.y} : {fx: fix[0], fy: fix[1]}\"         },         {\"trigger\": \"!fix\", \"modify\": \"node\", \"values\": \"{fx: null, fy: null}\"}       ],       \"encode\": {         \"enter\": {           \"size\": {\"scale\": \"size\", \"field\": \"Records\"},           \"stroke\": {\"scale\": \"color\", \"field\": \"operationType\"},           \"fill\": {\"scale\": \"color\", \"field\": \"operationType\"},           \"fillOpacity\": {\"value\": 0.1},            \"xfocus\": {\"scale\": \"xscale\", \"field\": \"Level\", \"band\": 0.5},           \"yfocus\": {\"scale\": \"yscale\", \"field\": \"index\", \"band\": 0.5},           \"tooltip\": {             \"signal\": \"{OperationType: datum.operationType, Operation: datum.Operation, Records: datum.Records}\"           }         },         \"update\": {           \"cursor\": {\"value\": \"pointer\"}         }       },       \"transform\": [         {           \"type\": \"force\",           \"iterations\": 300,           \"restart\": {\"signal\": \"restart\"},           \"static\": {\"signal\": \"static\"},           \"signal\": \"force\",           \"forces\": [             {\"force\": \"center\", \"x\": {\"signal\": \"cx\"}, \"y\": {\"signal\": \"cy\"}},             {\"force\": \"collide\", \"radius\": {\"signal\": \"nodeRadius\"}},             {\"force\": \"nbody\", \"strength\": {\"signal\": \"nodeCharge\"}},             {               \"force\": \"link\",               \"links\": \"link-data\",               \"distance\": {\"signal\": \"linkDistance\"},               \"id\": \"index\"             },             {\"force\": \"x\", \"x\": \"xfocus\", \"strength\": {\"signal\": \"gravityX\"}},             {\"force\": \"y\", \"y\": \"yfocus\", \"strength\": {\"signal\": \"gravityY\"}}           ]         }       ]     },     {       \"type\": \"path\",       \"from\": {\"data\": \"link-data\"},       \"interactive\": false,       \"encode\": {         \"update\": {\"stroke\": {\"value\": \"#ccc\"}, \"strokeWidth\": {\"value\": 0.5}}       },       \"transform\": [         {           \"type\": \"linkpath\",           \"require\": {\"signal\": \"force\"},           \"shape\": \"line\",           \"sourceX\": \"datum.source.x\",           \"sourceY\": \"datum.source.y\",           \"targetX\": \"datum.target.x\",           \"targetY\": \"datum.target.y\"         }       ]     },     {       \"type\": \"text\",       \"name\": \"labels\",       \"from\": {\"data\": \"nodes\"},        \"zindex\": 2,       \"interactive\": false,       \"enter\": {},       \"encode\": {         \"update\": {           \"fill\": {\"signal\": \"'white'\"},           \"y\": {\"field\": \"y\"},           \"x\": {\"field\": \"x\"},           \"text\": {\"field\": \"datum.operationShort\"},           \"align\": {\"value\": \"left\"},           \"fontSize\": {\"value\": 10},           \"baseline\": {\"value\": \"top\"},           \"limit\": {             \"signal\": \"clamp(60, 1, 1000)\"           },           \"ellipsis\": {\"value\": \" \"}         }       }     }   ] }</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega/v5.json\",\n  \"description\": \"A force-directed graph for a DAX Query Plan\",\n  \"width\": 700,\n  \"height\": 500,\n  \"padding\": 5,\n  \"autosize\": \"none\",\n  \"background\": \"#263238\",\n  \"signals\": [\n    {\"name\": \"cx\", \"update\": \"width / 2\"},\n    {\"name\": \"cy\", \"update\": \"height / 2\"},\n    {\"name\": \"nodeRadius\", \"value\": 8},\n    {\"name\": \"nodeCharge\", \"value\": -30},\n    {\"name\": \"linkDistance\", \"value\": 30},\n    {\"name\": \"static\", \"value\": false},\n    {\"name\": \"gravityX\", \"value\": 0.2},\n    {\"name\": \"gravityY\", \"value\": 0.9},\n    {\n      \"description\": \"State variable for active node fix status.\",\n      \"name\": \"fix\",\n      \"value\": false,\n      \"on\": [\n        {\n          \"events\": \"symbol:pointerout[!event.buttons], window:pointerup\",\n          \"update\": \"false\"\n        },\n        {\"events\": \"symbol:pointerover\", \"update\": \"fix || true\"},\n        {\n          \"events\": \"[symbol:pointerdown, window:pointerup] &gt; window:pointermove!\",\n          \"update\": \"xy()\",\n          \"force\": true\n        }\n      ]\n    },\n    {\n      \"description\": \"Graph node most recently interacted with.\",\n      \"name\": \"node\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"symbol:pointerover\",\n          \"update\": \"fix === true ? item() : node\"\n        }\n      ]\n    },\n    {\n      \"description\": \"Flag to restart Force simulation upon data changes.\",\n      \"name\": \"restart\",\n      \"value\": false,\n      \"on\": [{\"events\": {\"signal\": \"fix\"}, \"update\": \"fix &amp;&amp; fix.length\"}]\n    }\n  ],\n  \"data\": [\n    {\n      \"name\": \"node-data\",\n      \"url\": \"https://raw.githubusercontent.com/EvaluationContext/evaluationcontext.github.io/refs/heads/master/assets/vega/0020-queryPlanGraph/queryPlan.json\",\n      \"format\": {\"type\": \"json\", \"property\": \"physical\"},\n      \"transform\": [\n        {\n          \"type\": \"project\",\n          \"fields\": [\"dstid\", \"operation\", \"level\", \"records\", \"operationType\", \"operationShort\"],\n          \"as\": [\"index\", \"Operation\", \"Level\", \"Records\"]\n        },\n        {\n              \"type\": \"formula\",\n              \"expr\": \"length(datum.operationShort) * (width / 160)\",\n              \"as\": \"operationShortLen\"\n        }\n      ]\n    },\n    {\n      \"name\": \"link-data\",\n      \"url\": \"https://raw.githubusercontent.com/EvaluationContext/evaluationcontext.github.io/refs/heads/master/assets/vega/0020-queryPlanGraph/queryPlan.json\",\n      \"format\": {\"type\": \"json\", \"property\": \"physical\"},\n      \"transform\": [\n        {\"type\": \"filter\", \"expr\": \"datum.srcid != null\"},\n        {\n          \"type\": \"project\",\n          \"fields\": [\"srcid\", \"dstid\"],\n          \"as\": [\"source\", \"target\"]\n        }\n      ]\n    }\n  ],\n\n  \"legends\": [\n    {\n      \"direction\": \"vertical\",\n      \"legendX\": 1050,\n      \"legendY\": 0,\n      \"orient\": \"none\",\n      \"stroke\": \"color\",\n      \"fill\": \"color\",\n      \"titleColor\": \"#b6bcdd\",\n      \"padding\": 10,\n      \"encode\": {\n        \"title\": { \"update\": { \"fontSize\": { \"value\": 14 } } },\n        \"symbols\": { \"update\": { \"fillOpacity\": { \"value\": 0.4 } } },\n        \"labels\": {\n          \"update\": {\n            \"fontSize\": { \"value\": 12 },\n            \"fill\": { \"value\": \"#b6bcdd\" }\n          }\n        }\n      }\n    }\n  ],\n\n  \"scales\": [\n    {\n      \"name\": \"xscale\",\n      \"type\": \"band\",\n      \"domain\": {\"data\": \"node-data\", \"field\": \"Level\", \"sort\": true},\n      \"paddingOuter\": 1,\n      \"range\": \"width\"\n    },\n    {\n      \"name\": \"yscale\",\n      \"type\": \"band\",\n      \"domain\": {\"data\": \"node-data\", \"field\": \"index\", \"sort\": true},\n      \"range\": \"height\"\n    },\n    {\n      \"name\": \"color\",\n      \"type\": \"ordinal\",\n      \"domain\": {\"data\": \"node-data\", \"field\": \"operationType\"},\n      \"range\": {\"scheme\": \"tableau10\"}\n    },\n    {\n      \"name\": \"size\",\n      \"domain\": {\"data\": \"node-data\", \"field\": \"Records\"},\n      \"range\": [300, 2000]\n    }\n  ],\n\n  \"marks\": [\n    {\n      \"name\": \"nodes\",\n      \"type\": \"symbol\",\n      \"zindex\": 1,\n      \"from\": {\"data\": \"node-data\"},\n      \"on\": [\n        {\n          \"trigger\": \"fix\",\n          \"modify\": \"node\",\n          \"values\": \"fix === true ? {fx: node.x, fy: node.y} : {fx: fix[0], fy: fix[1]}\"\n        },\n        {\"trigger\": \"!fix\", \"modify\": \"node\", \"values\": \"{fx: null, fy: null}\"}\n      ],\n      \"encode\": {\n        \"enter\": {\n          \"size\": {\"scale\": \"size\", \"field\": \"Records\"},\n          \"stroke\": {\"scale\": \"color\", \"field\": \"operationType\"},\n          \"fill\": {\"scale\": \"color\", \"field\": \"operationType\"},\n          \"fillOpacity\": {\"value\": 0.1}, \n          \"xfocus\": {\"scale\": \"xscale\", \"field\": \"Level\", \"band\": 0.5},\n          \"yfocus\": {\"scale\": \"yscale\", \"field\": \"index\", \"band\": 0.5},\n          \"tooltip\": {\n            \"signal\": \"{OperationType: datum.operationType, Operation: datum.Operation, Records: datum.Records}\"\n          }\n        },\n        \"update\": {\n          \"cursor\": {\"value\": \"pointer\"}\n        }\n      },\n      \"transform\": [\n        {\n          \"type\": \"force\",\n          \"iterations\": 300,\n          \"restart\": {\"signal\": \"restart\"},\n          \"static\": {\"signal\": \"static\"},\n          \"signal\": \"force\",\n          \"forces\": [\n            {\"force\": \"center\", \"x\": {\"signal\": \"cx\"}, \"y\": {\"signal\": \"cy\"}},\n            {\"force\": \"collide\", \"radius\": {\"signal\": \"nodeRadius\"}},\n            {\"force\": \"nbody\", \"strength\": {\"signal\": \"nodeCharge\"}},\n            {\n              \"force\": \"link\",\n              \"links\": \"link-data\",\n              \"distance\": {\"signal\": \"linkDistance\"},\n              \"id\": \"index\"\n            },\n            {\"force\": \"x\", \"x\": \"xfocus\", \"strength\": {\"signal\": \"gravityX\"}},\n            {\"force\": \"y\", \"y\": \"yfocus\", \"strength\": {\"signal\": \"gravityY\"}}\n          ]\n        }\n      ]\n    },\n    {\n      \"type\": \"path\",\n      \"from\": {\"data\": \"link-data\"},\n      \"interactive\": false,\n      \"encode\": {\n        \"update\": {\"stroke\": {\"value\": \"#ccc\"}, \"strokeWidth\": {\"value\": 0.5}}\n      },\n      \"transform\": [\n        {\n          \"type\": \"linkpath\",\n          \"require\": {\"signal\": \"force\"},\n          \"shape\": \"line\",\n          \"sourceX\": \"datum.source.x\",\n          \"sourceY\": \"datum.source.y\",\n          \"targetX\": \"datum.target.x\",\n          \"targetY\": \"datum.target.y\"\n        }\n      ]\n    },\n    {\n      \"type\": \"text\",\n      \"name\": \"labels\",\n      \"from\": {\"data\": \"nodes\"},\n\n      \"zindex\": 2,\n      \"interactive\": false,\n      \"enter\": {},\n      \"encode\": {\n        \"update\": {\n          \"fill\": {\"signal\": \"'white'\"},\n          \"y\": {\"field\": \"y\"},\n          \"x\": {\"field\": \"x\"},\n          \"text\": {\"field\": \"datum.operationShort\"},\n          \"align\": {\"value\": \"left\"},\n          \"fontSize\": {\"value\": 10},\n          \"baseline\": {\"value\": \"top\"},\n          \"limit\": {\n            \"signal\": \"clamp(60, 1, 1000)\"\n          },\n          \"ellipsis\": {\"value\": \" \"}\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"posts/query-plan-graph/#conclusion","title":"Conclusion","text":"<p>I find this way of looking at the query plan much easier to parse and understand. While I'm not fully happy with the visuals they are a good proof of concept, and hopefully the Power BI or Dax Studio teams could consider creating something like this. Please vote on my Fabric idea if you want something like this too.</p>"},{"location":"posts/label-propogation/","title":"Label Propagation To Identify End User Persona?","text":"<p>Lets start with a quick recap, from the last few blog post. </p> <ul> <li>Visualizing Power BI Permission Inheritance </li> <li>I pulled data from Power BI Scanner APIs and Graph APIs and used GraphFrames and Pregel to disseminate Access Roles from Workspaces, Reports and Semantic Models directly granted to AAD groups, to all downstream members.</li> <li>Visualizing Power BI Permission Inheritance:</li> <li>I used this data and Deneb to draw a Force Directed Graph. </li> </ul> <p>At the end of the first post I mentioned Label Propagation as a potential method to group users into communities that represent personas of report consumers. So lets give that a try.</p> <p>Updates to previous posts</p> <p>I have updated the previous posts based on some pain points. Originally I was using a union of vertices and edges, but this made juggling the filtering of vertices and edges tricky. I am now using the triplet form, where a single row represents a edge, with srcId and dstId, but also feature vertices and edge attributes.</p>"},{"location":"posts/label-propogation/#label-propagation","title":"Label Propagation","text":"<p>Label Propagation works by assigning a distinct label to every vertex in the graph. We perform a number of supersteps. During each superstep each vertex sends it's label to all of it's neighbors (regardless of edge direction). When a destination vertex receives messages, it performs as aggregation, by accepting the highest frequency label, or if there a number of equal frequent labels, pick from those at random. At the end of all supersets all highly connected will likely share a common label and can be considered as a community.</p> <p>GraphFrames has a built-in Label Propagation Algorithm (LPA).</p> <pre><code>from graphframes.examples import Graphs\ng = Graphs.friends()  # Get example graph\n\nresult = g.labelPropagation(maxIter=5)\nresult.select(\"id\", \"label\").show()\n</code></pre> <p>In this test I want to only consider users that access Power BI Apps, that means we need to filter the vertices and edges prior to running Label Propagation. Then I'll save the triplet form so we can pull into Power BI.</p> <p>Info</p> <p>The code below uses functions and modules defined in previous post I started by working with 10 iterations but ramped up to 100 to allow time for convergence to a static state</p> <pre><code>## Filter the graph to remove workspace, Report and Dataset to focus on Report Apps\ngReportApp = g.filterVertices(\"type = 'Group' or type = 'User' or type = 'App' or type = 'Report App'\")\n\nvlabelProp = gReportApp.labelPropagation(maxIter=100)\n\n## create new graph with the added labels, to get the triplet representation\ng2 = GraphFrame(vlabelProp, gReportApp.edges)\ng2.triplets.createOrReplaceTempView(\"tripletsLabelProp\")\n\naccessToObjectEdgesLabelProp = spark.sql(f\"\"\"\n  select\n  t.src.nodeId as srcId,\n  t.src.name as srcName,\n  t.src.type as srcType,\n  t.src.label as srcLabel,\n  t.dst.nodeId as dstId,\n  t.dst.name as dstName,\n  t.dst.type as dstType,\n  t.dst.label as dstLabel,\n  coalesce(r.accessToObjectGroupId, concat(t.src.nodeID, t.src.accessRight)) as accessToObjectGroupId\n  ,r.accessToObjectType\n  from tripletsLabelProp as t\n  left join {savePath}.accessToObject as r\n      on t.src.nodeID = r.id\n      and r.accessToObjectType = 'Report App' -- Only consider access to Report Apps\n  \"\"\"\n)\n\nWriteDfToTable(accessToObjectEdgesLabelProp, savePath, 'accessToObjectEdgesLabelProp')\n</code></pre>"},{"location":"posts/label-propogation/#power-bi","title":"Power BI","text":""},{"location":"posts/label-propogation/#all-labels","title":"All Labels","text":"<p>For fun I wanted to look at the entire Graph, with vertices coloured by label. Besides being interesting to look at it brings no insights, so lets look at each label one at a time.</p> <p></p>"},{"location":"posts/label-propogation/#per-label","title":"Per Label","text":"<p>Now we have the data we can move over to Power BI and add a new table <code>accessToObjects Edges Label Prop</code> to the existing Semantic Model. Plus a disconnected <code>Label</code> dimension to support filtering, and a table <code>Vertices Label Prop</code> to help with the analysis.</p> <p></p> <pre><code>Vertices Label Prop =\nvar src =\n  SELECTCOLUMNS(\n    SUMMARIZE(\n      'accessToObject Edges Label Propogation'\n      ,'accessToObject Edges Label Propogation'[srcId]\n      ,'accessToObject Edges Label Propogation'[srcLabel]\n      ,'accessToObject Edges Label Propogation'[srcType]\n      ,'accessToObject Edges Label Propogation'[accessToObjectGroupId]\n    )\n    ,\"id\", 'accessToObject Edges Label Propogation'[srcId]\n    ,\"Label\", 'accessToObject Edges Label Propogation'[srcLabel]\n    ,\"Type\", 'accessToObject Edges Label Propogation'[srcType]\n    ,\"accessToObjectGroupId\", 'accessToObject Edges Label Propogation'[accessToObjectGroupId]\n  )\nvar dst =\n  SELECTCOLUMNS(\n    SUMMARIZE(\n      'accessToObject Edges Label Propogation'\n      ,'accessToObject Edges Label Propogation'[dstId]\n      ,'accessToObject Edges Label Propogation'[dstLabel]\n      ,'accessToObject Edges Label Propogation'[dstType]\n      ,'accessToObject Edges Label Propogation'[accessToObjectGroupId]\n    )\n    ,\"id\", 'accessToObject Edges Label Propogation'[dstId]\n    ,\"Label\", 'accessToObject Edges Label Propogation'[dstLabel]\n    ,\"Type\", 'accessToObject Edges Label Propogation'[dstType]\n    ,\"accessToObjectGroupId\", 'accessToObject Edges Label Propogation'[accessToObjectGroupId]\n  )\nvar vertices =\n  DISTINCT(\n    FILTER(\n      UNION( src, dst )\n      ,not ISBLANK( [accessToObjectGroupId] )\n      &amp;&amp; [Type] &lt;&gt; \"Report App\"\n    )\n  )\nRETURN\n\nvertices\n</code></pre> <p>In order to use the new new <code>srcLabel</code> and <code>dstLable</code> fields we need to update the Vega spec and the measure used to filter the edges table (below). This measure is added to the filter well of the visual and set to where value = 1</p> <pre><code>{\n  \"$schema\": \"https://vega.github.io/schema/vega/v5.json\",\n  \"description\": \"By Jake Duddy: https://evaluationcontext.github.io/post/label-propogation/ based off Force Directed example by David Bacci:https://github.com/PBI-David/Deneb-Showcase/blob/main/Force%20Directed%20Graph/Spec.json\",\n  \"padding\": {\n    \"left\": 0,\n    \"right\": 0,\n    \"top\": 0,\n    \"bottom\": 0\n  },\n\n  \"signals\": [\n    {\"name\": \"xrange\", \"update\": \"[0, width]\"},\n    {\"name\": \"yrange\", \"update\": \"[height, 0]\"},\n    {\"name\": \"xext\",\"update\": \"[0, width]\"},\n    {\"name\": \"yext\",\"update\": \"[height, 0]\"},\n    {\n      \"name\": \"down\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"mouseup,touchend\",\n          \"update\": \"null\"\n        },\n        {\n          \"events\": \"mousedown, touchstart\",\n          \"update\": \"xy()\"\n        },\n        {\n          \"events\": \"symbol:mousedown, symbol:touchstart\",\n          \"update\": \"null\"\n        }\n      ]\n    },\n    {\n      \"name\": \"xcur\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"mousedown, touchstart, touchend\",\n          \"update\": \"xdom\"\n        }\n      ]\n    },\n    {\n      \"name\": \"ycur\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"mousedown, touchstart, touchend\",\n          \"update\": \"ydom\"\n        }\n      ]\n    },\n    {\n      \"name\": \"delta\",\n      \"value\": [0, 0],\n      \"on\": [\n        {\n          \"events\": [\n            {\n              \"source\": \"window\",\n              \"type\": \"mousemove\",\n              \"consume\": true,\n              \"between\": [\n                {\"type\": \"mousedown\"},\n                {\n                  \"source\": \"window\",\n                  \"type\": \"mouseup\"\n                }\n              ]\n            },\n            {\n              \"type\": \"touchmove\",\n              \"consume\": true,\n              \"filter\": \"event.touches.length === 1\"\n            }\n          ],\n          \"update\": \"down ? [down[0]-x(), y()-down[1]] : [0,0]\"\n        }\n      ]\n    },\n    {\n      \"name\": \"anchor\",\n      \"value\": [0, 0],\n      \"on\": [\n        {\n          \"events\": \"wheel\",\n          \"update\": \"[invert('xscale', x()), invert('yscale', y())]\"\n        },\n        {\n          \"events\": {\n            \"type\": \"touchstart\",\n            \"filter\": \"event.touches.length===2\"\n          },\n          \"update\": \"[(xdom[0] + xdom[1]) / 2, (ydom[0] + ydom[1]) / 2]\"\n        }\n      ]\n    },\n    {\n      \"name\": \"zoom\",\n      \"value\": 1,\n      \"on\": [\n        {\n          \"events\": \"wheel!\",\n          \"force\": true,\n          \"update\": \"pow(1.001, event.deltaY * pow(16, event.deltaMode))\"\n        },\n        {\n          \"events\": {\"signal\": \"dist2\"},\n          \"force\": true,\n          \"update\": \"dist1 / dist2\"\n        },\n        {\n          \"events\": [\n            {\n              \"source\": \"view\",\n              \"type\": \"dblclick\"\n            }\n          ],\n          \"update\": \"1\"\n        }\n      ]\n    },\n    {\n      \"name\": \"dist1\",\n      \"value\": 0,\n      \"on\": [\n        {\n          \"events\": {\n            \"type\": \"touchstart\",\n            \"filter\": \"event.touches.length===2\"\n          },\n          \"update\": \"pinchDistance(event)\"\n        },\n        {\n          \"events\": {\"signal\": \"dist2\"},\n          \"update\": \"dist2\"\n        }\n      ]\n    },\n    {\n      \"name\": \"dist2\",\n      \"value\": 0,\n      \"on\": [\n        {\n          \"events\": {\n            \"type\": \"touchmove\",\n            \"consume\": true,\n            \"filter\": \"event.touches.length===2\"\n          },\n          \"update\": \"pinchDistance(event)\"\n        }\n      ]\n    },\n    {\n      \"name\": \"xdom\",\n      \"update\": \"xext\",\n      \"on\": [\n        {\n          \"events\": {\"signal\": \"delta\"},\n          \"update\": \"[xcur[0] + span(xcur) * delta[0] / width, xcur[1] + span(xcur) * delta[0] / width]\"\n        },\n        {\n          \"events\": {\"signal\": \"zoom\"},\n          \"update\": \"[anchor[0] + (xdom[0] - anchor[0]) * zoom, anchor[0] + (xdom[1] - anchor[0]) * zoom]\"\n        },\n        {\n          \"events\": [\n            {\n              \"source\": \"view\",\n              \"type\": \"dblclick\"\n            }\n          ],\n          \"update\": \"xrange\"\n        }\n      ]\n    },\n    {\n      \"name\": \"ydom\",\n      \"update\": \"yext\",\n      \"on\": [\n        {\n          \"events\": {\"signal\": \"delta\"},\n          \"update\": \"[ycur[0] + span(ycur) * delta[1] / height, ycur[1] + span(ycur) * delta[1] / height]\"\n        },\n        {\n          \"events\": {\"signal\": \"zoom\"},\n          \"update\": \"[anchor[1] + (ydom[0] - anchor[1]) * zoom, anchor[1] + (ydom[1] - anchor[1]) * zoom]\"\n        },\n        {\n          \"events\": [\n            {\n              \"source\": \"view\",\n              \"type\": \"dblclick\"\n            }\n          ],\n          \"update\": \"yrange\"\n        }\n      ]\n    },\n    {\n      \"name\": \"size\",\n      \"update\": \"clamp(20 / span(xdom), 1, 1000)\"\n    },\n    {\n      \"name\": \"cx\",\n      \"update\": \"width / 2\",\n      \"on\": [\n        {\n          \"events\": \"[symbol:mousedown, window:mouseup] &gt; window:mousemove\",\n          \"update\": \" cx==width/2?cx+0.001:width/2\"\n        }\n      ]\n    },\n    {\n      \"name\": \"cy\",\n      \"update\": \"height / 2\"\n    },\n    {\n      \"name\": \"nodeRadiusKey\",\n      \"description\": \"q=increase size, a=decrease size\",\n      \"value\": 8,\n      \"on\": [\n        {\n          \"events\": \"window:keypress\",\n          \"update\": \"event.key=='a'&amp;&amp;nodeRadiusKey&gt;1?nodeRadiusKey-1:event.key=='q'?nodeRadiusKey+1:nodeRadiusKey\"\n        }\n      ]\n    },\n    {\n      \"name\": \"nodeRadius\",\n      \"value\": 15,\n      \"on\": [\n        {\n          \"events\": {\n            \"signal\": \"nodeRadiusKey\"\n          },\n          \"update\": \"nodeRadiusKey\"\n        }\n      ]\n    },\n    {\"name\": \"nodeCharge\",\"value\": 0},\n    {\"name\": \"linkDistance\",\"value\": 5\n    },\n    {\n      \"description\": \"State variable for active node fix status.\",\n      \"name\": \"fix\",\n      \"value\": false,\n      \"on\": [\n        {\n          \"events\": \"symbol:mouseout[!event.buttons], window:mouseup\",\n          \"update\": \"false\"\n        },\n        {\n          \"events\": \"symbol:mouseover\",\n          \"update\": \"fix || true\",\n          \"force\": true\n        },\n        {\n          \"events\": \"[symbol:mousedown, window:mouseup] &gt; window:mousemove!\",\n          \"update\": \"xy()\",\n          \"force\": true\n        }\n      ]\n    },\n    {\n      \"description\": \"Graph node most recently interacted with.\",\n      \"name\": \"node\",\n      \"value\": null,\n      \"on\": [\n        {\n          \"events\": \"symbol:mouseover\",\n          \"update\": \"fix === true ? datum.index : node\"\n        }\n      ]\n    },\n    {\n      \"name\": \"nodeHover\",\n      \"value\": {\n        \"id\": null,\n        \"connections\": []\n      },\n      \"on\": [\n        {\n          \"events\": \"symbol:mouseover\",\n          \"update\": \"{'id':datum.index, 'connections':split(datum.sources+','+datum.targets,',')}\"\n        },\n        {\n          \"events\": \"symbol:mouseout\",\n          \"update\": \"{'id':null, 'connections':[]}\"\n        }\n      ]\n    },\n    {\n      \"description\": \"Flag to restart Force simulation upon data changes.\",\n      \"name\": \"restart\",\n      \"value\": false,\n      \"on\": [\n        {\n          \"events\": {\"signal\": \"fix\"},\n          \"update\": \"fix &amp;&amp; fix.length\"\n        }\n      ]\n    }\n  ],\n  \"data\": [\n    {\"name\": \"dataset\" },\n    {\n      \"name\": \"link-data\",\n      \"source\": \"dataset\",\n      \"transform\": [\n        {\n          \"type\": \"filter\",\n          \"expr\": \"datum.srcLabel == datum.SelectedLabel &amp;&amp; datum.dstLabel == datum.SelectedLabel\"\n        },\n        {\n          \"type\": \"project\",\n          \"fields\": [\"srcId\", \"srcName\", \"srcType\", \"dstId\", \"dstName\", \"dstType\"],\n          \"as\": [\"source\", \"srcName\", \"srcType\", \"target\", \"dstName\", \"dstType\"]\n        }\n      ]\n    },\n    {\n      \"name\": \"source-connections\",\n      \"source\": \"link-data\",\n      \"transform\": [\n        {\n          \"type\": \"aggregate\",\n          \"groupby\": [\"source\"],\n          \"ops\": [\"values\"],\n          \"fields\": [\"source\"],\n          \"as\": [\"connections\"]\n        },\n        {\n          \"type\": \"formula\",\n          \"as\": \"targets\",\n          \"expr\": \"pluck(datum.connections, 'target')\"\n        }\n      ]\n    },\n    {\n      \"name\": \"target-connections\",\n      \"source\": \"link-data\",\n      \"transform\": [\n        {\n          \"type\": \"aggregate\",\n          \"groupby\": [\"target\"],\n          \"ops\": [\"values\"],\n          \"fields\": [\"source\"],\n          \"as\": [\"connections\"]\n        },\n        {\n          \"type\": \"formula\",\n          \"as\": \"sources\",\n          \"expr\": \"pluck(datum.connections, 'source')\"\n        }\n      ]\n    },\n    {\n      \"name\": \"src\",\n      \"source\": \"dataset\",\n      \"transform\": [\n        {\n          \"type\": \"filter\",\n          \"expr\": \"datum.srcLabel == datum.SelectedLabel\"\n        },\n        {\n          \"type\": \"project\",\n          \"fields\": [\"srcId\", \"srcName\", \"srcType\"],\n          \"as\": [\"id\", \"Name\", \"Type\"]\n        }\n      ]\n    },\n    {\n      \"name\": \"dst\",\n      \"source\": \"dataset\",\n      \"transform\": [\n        {\n          \"type\": \"filter\",\n          \"expr\": \"datum.dstLabel == datum.SelectedLabel\"\n        },\n        {\n          \"type\": \"project\",\n          \"fields\": [\"dstId\", \"dstName\", \"dstType\"],\n          \"as\": [\"id\", \"Name\", \"Type\"]\n        }\n      ]\n    },\n    {\n      \"name\": \"vertices\",\n      \"source\": [\"src\", \"dst\"],\n      \"transform\": [\n        {\n          \"type\": \"aggregate\",\n          \"groupby\": [\"id\", \"Name\", \"Type\"]\n        },\n        {\n          \"type\": \"project\",\n          \"fields\": [\"id\", \"Name\", \"Type\"]\n        }\n      ]\n    },\n    {\n      \"name\": \"node-data\",\n      \"source\": \"vertices\",\n      \"transform\": [\n        {\n          \"type\": \"lookup\",\n          \"from\": \"source-connections\",\n          \"key\": \"source\",\n          \"fields\": [\"id\"],\n          \"values\": [\"targets\"],\n          \"as\": [\"targets\"],\n          \"default\": [\"\"]\n        },\n        {\n          \"type\": \"lookup\",\n          \"from\": \"target-connections\",\n          \"key\": \"target\",\n          \"fields\": [\"id\"],\n          \"values\": [\"sources\"],\n          \"as\": [\"sources\"],\n          \"default\": [\"\"]\n        },\n        {\n          \"type\": \"force\",\n          \"iterations\": 300,\n          \"restart\": {\n            \"signal\": \"restart\"\n          },\n          \"signal\": \"force\",\n          \"forces\": [\n            {\n              \"force\": \"center\",\n              \"x\": {\"signal\": \"cx\"},\n              \"y\": {\"signal\": \"cy\"}\n            },\n            {\n              \"force\": \"collide\",\n              \"radius\": {\n                \"signal\": \"sqrt(4 * nodeRadius * nodeRadius)\"\n              },\n              \"iterations\": 1,\n              \"strength\": 0.7\n            },\n            {\n              \"force\": \"nbody\",\n              \"strength\": {\n                \"signal\": \"nodeCharge\"\n              }\n            },\n            {\n              \"force\": \"link\",\n              \"links\": \"link-data\",\n              \"distance\": {\n                \"signal\": \"linkDistance\"\n              },\n              \"id\": \"id\"\n            }\n          ]\n        },\n        {\n          \"type\": \"formula\",\n          \"as\": \"fx\",\n          \"expr\": \"fix[0]!=null &amp;&amp; node==datum.index ?invert('xscale',fix[0]):null\"\n        },\n        {\n          \"type\": \"formula\",\n          \"as\": \"fy\",\n          \"expr\": \"fix[1]!=null &amp;&amp; node==datum.index ?invert('yscale',fix[1]):null\"\n        }\n      ]\n    }\n  ],\n  \"scales\": [\n    {\n      \"name\": \"color\",\n      \"type\": \"ordinal\",\n      \"domain\": [\"Workspace\", \"Dataset\", \"Report\", \"Report App\", \"Group\", \"User\", \"App\"],\n      \"range\": [\"#8661c5\", \"#01b8aa\", \"#FFB900\", \"#634e15\", \"#0078d4\", \"#999999\", \"#5A7378\"]\n    },\n    {\n      \"name\": \"xscale\",\n      \"zero\": false,\n      \"domain\": {\"signal\": \"xdom\"},\n      \"range\": {\"signal\": \"xrange\"}\n    },\n    {\n      \"name\": \"yscale\",\n      \"zero\": false,\n      \"domain\": {\"signal\": \"ydom\"},\n      \"range\": {\"signal\": \"yrange\"}\n    }\n  ],\n  \"legends\": [\n    {\n      \"fill\": \"color\",\n      \"encode\": {\n        \"title\": {\n          \"update\": {\n            \"fontSize\": {\"value\": 8}\n          }\n        },\n        \"labels\": {\n          \"interactive\": true,\n          \"update\": {\n            \"fontSize\": {\"value\": 8},\n            \"fill\": {\"value\": \"black\"}\n          }\n        },\n        \"symbols\": {\n          \"update\": {\n            \"stroke\": {\"value\": \"transparent\"}\n          }\n        },\n        \"legend\": {\n          \"update\": {\n            \"stroke\": {\"value\": \"#ccc\"},\n            \"strokeWidth\": {\"value\": 0}\n          }\n        }\n      }\n    }\n  ],\n\n  \"marks\": [\n    {\n      \"type\": \"path\",\n      \"name\": \"links\",\n      \"from\": {\"data\": \"link-data\"},\n      \"interactive\": false,\n      \"encode\": {\n        \"update\": {\n          \"stroke\": {\n            \"signal\": \"datum.source.index!=nodeHover.id &amp;&amp; datum.target.index!=nodeHover.id ? '#929399':merge(hsl(scale('color', datum.source.Type)), {l:0.64})\"\n          },\n          \"strokeWidth\": {\n            \"signal\": \"datum.source.index!=nodeHover.id &amp;&amp; datum.target.index!=nodeHover.id ? 0.2:1\"\n          }\n        }\n      },\n      \"transform\": [\n        {\n          \"type\": \"linkpath\",\n          \"require\": {\n            \"signal\": \"force\"\n          },\n          \"shape\": \"line\",\n          \"sourceX\": {\n            \"expr\": \"scale('xscale', datum.datum.source.x)\"\n          },\n          \"sourceY\": {\n            \"expr\": \"scale('yscale', datum.datum.source.y)\"\n          },\n          \"targetX\": {\n            \"expr\": \"scale('xscale', datum.datum.target.x)\"\n          },\n          \"targetY\": {\n            \"expr\": \"scale('yscale', datum.datum.target.y)\"\n          }\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"atan2(datum.datum.target.y - datum.datum.source.y,datum.datum.source.x - datum.datum.target.x)\",\n          \"as\": \"angle1\"\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"(datum.angle1&gt;=0?datum.angle1:(2*PI + datum.angle1)) * (360 / (2*PI))\",\n          \"as\": \"angle2\"\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"(360-datum.angle2)*(PI/180)\",\n          \"as\": \"angle3\"\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"(cos(datum.angle3)*(nodeRadius+5))+(scale('xscale',datum.datum.target.x))\",\n          \"as\": \"arrowX\"\n        },\n        {\n          \"type\": \"formula\",\n          \"expr\": \"(sin(datum.angle3)*(nodeRadius+5))+(scale('yscale',datum.datum.target.y))\",\n          \"as\": \"arrowY\"\n        }\n      ]\n    },\n    {\n      \"type\": \"symbol\",\n      \"name\": \"arrows\",\n      \"zindex\": 1,\n      \"from\": {\"data\": \"links\"},\n      \"encode\": {\n        \"update\": {\n          \"shape\": {\n            \"value\": \"triangle\"\n          },\n          \"angle\": {\n            \"signal\": \"-datum.angle2-90\"\n          },\n          \"x\": {\n            \"signal\": \"datum.arrowX\"\n          },\n          \"y\": {\n            \"signal\": \"datum.arrowY\"\n          },\n          \"text\": {\"signal\": \"'\u25b2'\"},\n          \"fill\": {\n            \"signal\": \"datum.datum.source.index!=nodeHover.id &amp;&amp; datum.datum.target.index!=nodeHover.id ? '#929399':merge(hsl(scale('color', datum.datum.source.Type)), {l:0.64})\"\n          },\n          \"size\": {\n            \"signal\": \"nodeRadius==1?0:30\"\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"nodes\",\n      \"type\": \"symbol\",\n      \"zindex\": 1,\n      \"from\": {\"data\": \"node-data\"},\n      \"encode\": {\n        \"update\": {\n          \"opacity\": {\"value\": 1},\n          \"fill\": {\n            \"signal\": \"nodeHover.id===datum.index || indexof(nodeHover.connections, datum.id)&gt;-1 ?scale('color', datum.Type):merge(hsl(scale('color', datum.Type)), {l:0.64})\"\n          },\n          \"stroke\": {\n            \"signal\": \"nodeHover.id===datum.index || indexof(nodeHover.connections, datum.id)&gt;-1 ?scale('color', datum.Type):merge(hsl(scale('color', datum.Type)), {l:0.84})\"\n          },\n          \"strokeWidth\": {\"value\": 0.5},\n          \"size\": {\"signal\": \"4 * nodeRadius * nodeRadius\"},\n          \"cursor\": {\"value\": \"pointer\"},\n          \"x\": {\"signal\": \"fix[0]!=null &amp;&amp; node===datum.index ?fix[0]:scale('xscale', datum.x)\"},\n          \"y\": {\"signal\": \"fix[1]!=null &amp;&amp; node===datum.index ?fix[1]:scale('yscale', datum.y)\"}\n        },\n        \"hover\": {\n          \"tooltip\": {\n            \"signal\": \"datum.Name\"\n          }\n        }\n      }\n    },\n    {\n      \"type\": \"text\",\n      \"name\": \"labels\",\n      \"from\": {\"data\": \"nodes\"},\n      \"zindex\": 2,\n      \"interactive\": false,\n      \"enter\": {},\n      \"encode\": {\n        \"update\": {\n          \"fill\": {\"signal\": \"'black'\"},\n          \"y\": {\"field\": \"y\"},\n          \"x\": {\"field\": \"x\"},\n          \"text\": {\n            \"field\": \"datum.Name\"\n          },\n          \"align\": {\"value\": \"center\"},\n          \"fontSize\": {\"value\": 8},\n          \"baseline\": {\n            \"value\": \"middle\"\n          },\n          \"limit\": {\n            \"signal\": \"clamp(sqrt(4 * nodeRadius * nodeRadius)-5,1,1000)\"\n          },\n          \"ellipsis\": {\"value\": \" \"}\n        }\n      }\n    }\n  ]\n}\n</code></pre> <pre><code>Edge Selection w/ Label =\nvar FilteredEdges =\n  FILTER(\n    'accessToObject Edges Label Propogation'\n    ,'accessToObject Edges Label Propogation'[srcLabel] in VALUES( labels[Label] )\n    || 'accessToObject Edges Label Propogation'[dstLabel] in VALUES( labels[Label] )\n  )\nRETURN\n\nIF( COUNTROWS( FilteredEdges ) &gt; 0, 1 )\n</code></pre> <p>The question I'm looking to answer is do the labels partition users/group into segments that have access to similar Report Apps?</p> <p>I'll use <code>Vertices Label Prop</code> table to create a Sankey chart as a quick and dirty way to check this.</p> <p>In this example, the user/groups have access to the same Report Apps.</p> <p></p> <p>But not in this cases.</p> <p></p> <p>When you get to a large number of Report Apps it becomes hard to interpret.</p> <p></p> <p>We really want to quantify how many users/groups in the partition have access to the same Report App. We can create a measure to do this. If we calculate the number of edges vs the number of users/group, per Report App, and then take an average, we can get a linkage strength.</p> <pre><code>User/Group -&gt; Report App Linkage Strength =\nvar PossibleInDegree = COUNTROWS( DISTINCT( 'Vertices Label Prop'[id] ) )\nvar tbl =\n  ADDCOLUMNS(\n    VALUES( 'Vertices Label Prop'[accessToObjectGroupId] )\n    ,\"@Strength\",\n        var InDegree = CALCULATE( COUNTROWS( 'Vertices Label Prop' ) )\n        return\n        DIVIDE( InDegree, PossibleInDegree)\n  )\nreturn\n\nAVERAGEX( tbl, [@Strength] )\n</code></pre> <pre><code># Distinct Vertices Label Prop = \nCALCULATE( \n  COUNTROWS( DISTINCT( 'Vertices Label Prop'[id] ) )\n  , 'Vertices Label Prop'[Type] in {\"User\", \"Group\"} \n)\n</code></pre> <p>The largest group has low correlation. This is not to say that users can't access all of the same subset of Report App, but there are large number of Report Apps that some User/Groups within the partition can access that the other can't.</p> <p></p> <p>But there are some with high correlation.</p> <p></p> <p>In partitions with strong correlation It would be sensible to check the users in these partitions to see if they are in the same department or share some other characteristic that could be used to further define and validate the persona. These could then be represented by a single AAD group to simplify the assignment of future permissions.</p>"},{"location":"posts/label-propogation/#conclusion","title":"Conclusion","text":"<p>Realistically I\u2019m not completely sold by this approach but with the graph already setup, Label Propagation was a quick one liner that was worth investigating.</p> <p>Perhaps some kind dimensional reduction approach might work better.</p>"},{"location":"posts/TenantSettings/","title":"Exporting Power BI Tenant Settings","text":"<p>With Power BI APIs spread across Power BI and Fabric APIs, it's easy to overlook the introduction of valuable ones like List Tenant Settings. Previously, solutions for exporting Tenant settings involved scraping the Admin Portal WebPage, as highlighted in a post by Kurt Buhler. The inclusion of this API is a significant improvement. It eliminates the need for a Privileged Access request solely to verify a configuration, making it more accessible for non-Fabric Admins to review tenant settings independently, thus reducing administrative workload. Moreover, by continuously updating this data, you can monitor the evolution of your settings over time.</p>"},{"location":"posts/TenantSettings/#list-tenant-settings","title":"List Tenant Settings","text":"<p>Microsoft Docs: list-tenant-settings</p> <p>Permissions The caller must be a Fabric administrator or authenticate using a service principal.</p> <p>Required Delegated Scopes Tenant.Read.All or Tenant.ReadWrite.All</p> <p><code>GET https://api.fabric.microsoft.com/v1/admin/tenantsettings</code></p> <p>-- Microsoft Docs: list-tenant-settings</p>"},{"location":"posts/TenantSettings/#script","title":"Script","text":"<p>We can get the tenant setting and write them to a delta table like this:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nimport requests\nimport logging\n\nlogger = logging.getLogger(\"logger\")\nlogger.setLevel(logging.INFO) # DEBUG, INFO, WARNING, ERROR, CRITICAL\nlogging.basicConfig(\n  format=\"{asctime} - {levelname} - {message}\",\n  style=\"{\",\n  datefmt=\"%Y-%m-%d %H:%M\",\n)\n\nclient_id = dbutils.secrets.get(scope=\"scopeabc\", key=\"abc-pbi-readonly-clientid\")\nclient_secret = dbutils.secrets.get(scope=\"scopeabc\", key=\"abc-pbi-readonly-secret\")\ntenant_id = \"00000000-0000-0000-0000-000000000000\"\nsavePath = 'hive_metastore.powerbicatalogue'\n\ndef GetAccessToken(client_id:str, client_secret:str, tenant_id:str, resource:str) -&gt; str:\n    \"\"\"\n    Get an access token from Azure AD.\n    parameters:\n        client_id:      str     the client ID for the application registered in Azure AD\n        client_secret:  str     the client secret for the application registered in Azure AD\n        tenant_id:      str     the tenant ID for the application registered in Azure AD\n        resource:       str     the resource for the application registered in Azure AD\n    returns:            str     the access token\n    \"\"\"\n    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n    scope = f\"{resource}/.default\"\n    data = {\n        \"grant_type\": \"client_credentials\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n        'scope': scope\n    }\n\n    r = Request(method=\"post\", url=url, data=data)\n    token_data = r.json()\n\n    logger.info(f\"{'GetAccessToken':25} Token Generated, {scope} expires in {token_data.get('expires_in')} seconds\")\n\n    return token_data.get(\"access_token\")\n\ndef Request(method:str, url:str, headers:dict=None, data:dict=None, proxies:dict=None):\n    \"\"\"\n    Make a request to the specified URL. Deals with error handling.\n    parameters:\n        method:     str     the HTTP method to use {get, post, put, delete}\n        url:        str     the URL to make the request to\n        headers:    dict    the headers to send with the request\n        data:       dict    the data to send with the request\n        proxies:    dict    the proxies to use with the request\n    returns:        str     the response from the request\n    \"\"\"\n\n    if method not in [\"get\", \"post\", \"put\", \"delete\"]:\n        return f\"Invalid method {method}, must be one of get, post, put, delete\"\n\n    try:\n        r = requests.request(method=method, url=url, headers=headers, data=data, proxies=proxies)\n        invalid_request_reason = r.text\n        if r.status_code == 400:\n            invalid_request_reason = r.text\n            raise Exception(f\"{'Request' :25} Your request has failed because {invalid_request_reason}\")\n        elif r.status_code &gt; 400:\n            raise Exception(f\"{'Request' :25} Your request has failed with status code {r.status_code}\")\n    except requests.exceptions.ConnectionError as err:\n        raise SystemExit(err)\n\n    return r\n\ndef WriteDfToTable(df, savePath:str, tableName:str, mode:str = \"Overwrite\") -&gt; None:\n    \"\"\"\n    Writes a View to a table in the specified database.\n    parameters:\n        df:           pyspark dataframe\n        mode:         str       Overwrite, Append, Merge\n        tableName:    str       name of the table to write to\n        savePath:     str       path to save the table to i.e \"hive_metastore.xxx\"\n    \"\"\"\n\n    if mode not in (\"Overwrite\", \"Append\", \"Merge\"):\n        raise Exception(f\"{'WriteToTable' :25} Invalid mode {mode}, must be one of Overwrite, Append, Merge\")\n\n    df.write.format(\"delta\").mode(mode).option(\"overwriteSchema\", \"true\").saveAsTable(f\"{savePath}.{tableName}\")\n\n    logger.info(f\"{'WriteDfToTable' :25} {savePath}.{tableName} ({mode})\")\n\ndef GetTenantSettings():\n    \"\"\"\n    Calls GetAppsAsAdmin API [https://learn.microsoft.com/en-us/rest/api/fabric/admin/tenants/list-tenant-settings?tabs=HTTP]\n    parameters:\n        access_token:       str     access token\n    Returns:                dict\n    \"\"\"\n\n    access_token = GetAccessToken(client_id, client_secret, tenant_id, resource='https://api.fabric.microsoft.com')\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n    url = 'https://api.fabric.microsoft.com/v1/admin/tenantsettings'\n    r = Request(method=\"get\", url=url, headers=headers)\n\n    logger.info(f\"{'GetTenantSettings':25}\")\n    return r.json()\n\ntenantSettings = GetTenantSettings()\n\ntenantSettingsSchema = StructType([\n    StructField('canSpecifySecurityGroups', BooleanType(), True),\n    StructField('enabled', BooleanType(), True),\n    StructField('enabledSecurityGroups', ArrayType( StructType([\n        StructField('name', StringType(), True),\n        StructField('graphId', StringType(), True)\n        ])), True),\n    StructField('settingName', StringType(), True),\n    StructField('tenantSettingGroup', StringType(), True),\n    StructField('title', StringType(), True),\n    StructField('delegateToCapacity', StringType(), True),\n    StructField('properties', ArrayType( StructType([\n        StructField('name', StringType(), True),\n        StructField('type', StringType(), True),\n        StructField('value', StringType(), True)\n        ])), True),\n    StructField('delegateToDomain', StringType(), True),\n    StructField('delegateToWorkspace', StringType(), True)\n])\n\ntenantSettingsdf = spark.createDataFrame(tenantSettings['tenantSettings'], schema = tenantSettingsSchema)\nWriteDfToTable(tenantSettingsdf, savePath, tenantSettings)\n</code></pre>"},{"location":"posts/slowly-changing-dimensions/","title":"Incremental Refresh on Slowly Changing Dimension in Power BI","text":"<p>Incremental Refresh is a powerful tool for reducing processing during Power BI Semantic Model Refreshes. However, for Dimensions it is often overlooked. Here are some thoughts and considerations to keep in mind when implementing Incremental Refresh.</p>"},{"location":"posts/slowly-changing-dimensions/#incremental-refresh","title":"Incremental Refresh","text":"<p>A Semantic Model table, by default, has a single partition. If table has a Incremental Refresh Policy configured, when the model is deployed to the service, on the first refresh, the policy is applied and multiple partitions are created, each accounting for a continuous date range. Within the policy you can define a Incremental Window and a Archive Window. Partitions within the Incremental Window are hot, and are refreshed when the model is refreshed. Those in the Archived Window are cold and are not refreshed. As time progresses new partitions are created to capture data from new dates, and partitions that fall out of the Incremental Window archived.</p> <p>Refreshing Archived Partitions</p> <p>You are able to refresh Archived partitions using the XMLA endpoint, by calling a full Refresh command for the given table or partition. A full refresh command on the model will not touch archived partitions. You can also use a Enhanced Refresh API call.</p> <p> Microsoft Docs: Incremental refresh and real-time data for semantic models </p> <p>Incremental Refresh can decrease Refresh processing and times:</p> <ul> <li>Partitions are refreshed in parallel</li> <li>Only import recent data, archiving older date</li> <li>Polling Expression can be set to avoid refreshing partition with no new data</li> </ul> <p>Partition Size</p> <p>When considering the Incremental Refresh policy you need to consider the size of data and how many rows you'd expect in each partition. Power BI by default has a segment size of 1 million rows, and 8 million when Large Data Format is enabled. If data within a partition size is expected to be below the size of a segment you could except worse query performance.</p> <p>Incremental Refresh also allows you set a Polling Expression that determines if a partition will refresh or not. Polling expression are a M expression that is called for each partition, returning a scalar value. This value is checked against a refreshBookmark store within the metadata of the partition, if the value returned by the Polling Expression matches the refreshBookmark, the partition does not refresh. If it differs, the refresh occurs and the refreshBookmark is updated. Chris Webb has a good blog post on this topic, and the Microsoft Docs also describes this detect data changes feature.</p>"},{"location":"posts/slowly-changing-dimensions/#incremental-refresh-on-dimensions","title":"Incremental Refresh on Dimensions?","text":"<p>In dimensional modelling there are various forms of dimension, each of which differ in the approach applied in handling the evolution of dimensions attributes over time. This is a concept described by Kimball as Slowly Changing Dimension (SCD).</p> <p>Type 1 SCD are full overwrites of data and therefore are not suitable for incremental refresh.</p> <p>With higher order SCDs, incremental refresh can be possible since new rows are added to capture data changes, with a date for the change. We can use the modified date for incremental refresh. Of note, within our Semantic Model we need to use surrogate keys in the fact and dimension table, as the natural key would result in a many-many relationship. </p> <p>When including higher order SCDs in your model you have to consider how you want filtering to occur. In some circumstances you may want to filter on a historical attribute, returning only the transactions related to the dimension by the surrogate key, representing transaction for a given time period. This might be because a customer is related to different sales reps over time and you want to only attribute sales to a rep when they were working with the customer. But you may also want to filter on the current status and return all transactions related to the natural key, not the current surrogate key. In this case you have two options, (1) add fields to the table for current attributes, but that would not work with incremental refresh because of archived partitions. (2) Create another dimension in the SCD 1 form representing current data related to the fact table via the natural key.</p>"},{"location":"posts/slowly-changing-dimensions/#polling-expression-on-type-1-scd","title":"Polling Expression on Type 1 SCD","text":"<p>I only just said Type 1 SCD are not suitable for incremental refresh. But polling expressions could be helpful to reduce the import of unnecessary data, and they are only available with Incremental Refresh.</p> <p>For the polling expression to work we need to have some scalar value that we would expect to change if the data updates. If we consider higher order case above, we have a modified date we can leverage. We can create the SCD type 1 table by filtering the higher order table to return the most recent record for each natural key, keeping the <code>modified date</code>.</p> <p>If we want to setup Incremental Refresh we would only want a Incremental Window, with no Archive Window, otherwise we risk having duplicates of the same natural key. The problem is we cannot set a Archive Window of 0. We can overcome this limitation by adding a additional <code>Date</code> field to the table, with today's date.</p> <p>We can now setup Incremental Refresh. First we define our <code>RangeStart</code> and <code>RangeEnd</code> and use this to filter on <code>Date</code>.</p> <pre><code>let\n    Source = Sql.Database(\"dwdev02\",\"AdventureWorksDW2017\"),\n    Data  = Source{[Schema=\"dbo\",Item=\"FactInternetSales\"]}[Data],\n    IncrementalRefresh = Table.SelectRows(Data, each [Date] &gt;= RangeStart and [Date] &lt; RangeEnd)\nin\n    IncrementalRefresh\n</code></pre> <p>I'll set Incremental Window period to 2 year and the Archive Window to 1 year. Additionally we'll add <code>modifiedDate</code> for the polling Expression.</p> <p></p> <p>We can open Tabular Editor and apply the Refresh Policy.</p> <p>Partition in Power BI Desktop</p> <p>Power BI Desktop does not support multiple partitions. You can recover by creating a new partition and copy in the source expression. You then need to delete the incremental refresh partitions and remove the policy.</p> <p></p> <p>We get 3 partitions, one for each year. Since we have set the <code>Date</code> to today's date, only this year's partition will ever get data. We added an additional year so that we will only ever have a empty partition in the Incremental Window as a buffer to roll into the archive window. This effectively gives us one active partition like we had before applying Incremental Refresh but we get the benefit of a polling expression checking to see if there is a more recent <code>modified date</code>. If you fear your dimension will not update within the 2 year timeframe, you can increase the Incremental Window to a longer period.</p>"},{"location":"posts/minimizing-svg-strings/","title":"Minimizing SVG Strings to Avoid Hitting Power BI Memory Limits","text":"<p>When working with visuals in Power BI, SVGs offer great flexibility for creating engaging graphics in cards and tables. However, it's essential to be mindful of the limitations related to string size. This post delves into effective strategies for minimizing SVG size, particularly when handling repetitive elements. String reduction techniques are valuable tool in lowering the memory overhead of SVG visuals.</p>"},{"location":"posts/minimizing-svg-strings/#string-memory-error","title":"String Memory Error","text":"<p>I opened up my report the other day and was greeting with this error.</p> <p></p> <p>The visual in question was the following. </p> <p></p> <p>The error was caused by the <code>Refresh Barcode SVG</code> measure below. It creates a SVG, plotting Semantic Model refreshes as a coloured line, signifying success or failure.</p> <pre><code>MEASURE '_measures'[Refresh Barcode SVG] = \nVAR __svgHeight = 20\nVAR __svgWidth = 150\nVAR __Categories = SUMMARIZE( 'Progress Report', 'DateTimes'[Date], 'DateTimes'[DateTime], 'Progress Report'[XmlaRequestId] )\nVAR __Data =\n    TOPN(\n        400\n        ,ADDCOLUMNS(\n            KEEPFILTERS(\n                FILTER(\n                    KEEPFILTERS( __Categories ),\n                    not ISBLANK( CALCULATE ( MAX( 'Execution Metrics'[Status] ) ) )\n                )\n            ),\n            \"Value\", CALCULATE ( MAX( 'Execution Metrics'[Status] ) )\n        )\n        ,'DateTimes'[DateTime]\n        ,DESC\n    )\nVAR __All_Categories =\n    CALCULATETABLE(\n        FILTER(\n            KEEPFILTERS( SUMMARIZE( 'Progress Report', 'DateTimes'[Date], 'DateTimes'[DateTime] ) ),\n            not ISBLANK( CALCULATE ( MAX( 'Execution Metrics'[Status] ) ) ) &amp;&amp; not ISBLANK( 'DateTimes'[Date] )\n        ),\n        ALLSELECTED()\n    )  \nVAR __MinX_Value = MINX( __All_Categories, 'DateTimes'[DateTime] )\nVAR __MaxX_Value = MAXX( __All_Categories, 'DateTimes'[DateTime] )\nVAR __RangeX = __MaxX_Value - __MinX_Value\nVAR __Lines =\n    CONCATENATEX(\n        __Data,\n        VAR __Value =\n            SWITCH(\n                [Value]\n                ,\"Started\", 1\n                ,\"Succeeded\", 0.7\n                ,\"Failed\", 0.85\n                ,0\n            )\n        VAR _Hex =\n            SWITCH(\n                [Value]\n                ,\"Started\",\"#FFB900\"        // Orange\n                ,\"Failed\", \"#DD6B7F\"        // Red \n                ,\"Succeeded\", \"#37A794\"     // Green\n                ,\"gray\"\n            )\n        VAR _x = FORMAT( DIVIDE( 'DateTimes'[DateTime] - __MinX_Value, __RangeX, 0 ) * 100, \"0.0\", \"en-US\" )\n        RETURN\n        \"&lt;line x1='\" &amp; _x &amp; \"' y1='\" &amp; __svgHeight * __Value &amp; \"' x2='\" &amp; _x &amp; \"' y2='\" &amp; __svgHeight - (__svgHeight * __Value) &amp; \"' stroke='\" &amp; _Hex &amp; \"' stroke-width='2' /&gt;\"\n        ,\",\", [value], ASC\n    )\nVAR __Svg =\n    \"data:image/svg+xml;utf8, &lt;svg width=\"\"\" &amp; __svgWidth &amp; \"\"\" height=\"\"\" &amp; __svgHeight &amp;\"\"\" xmlns=\"http://www.w3.org/2000/svg\"&gt;\" &amp;\n    __Lines &amp;\n    \"&lt;/svg&gt;\"\nRETURN\nIF( not ISEMPTY(__Data) , __Svg )\n</code></pre> <p>From the error, it looks like the length of the string for the SVG is exceeding Power BI's memory limits. To resolve this error we could reduce the data, by Filtering or Sampling. As you can see, I am already using <code>TOPN()</code> to filter to 400 most recent refreshes. Sampling is the approach used Power BI's built in sparklines. Another approach, to avoid reducing data would be to reduce the size of strings required to represent the data marks, this is the approach I'll be looking at in this post.</p> <p>The bulk of this SVG is the large number of <code>&lt;line&gt;</code> elements being calculated in <code>__Lines</code>. Which when resolved, end up looking like this:</p> <pre><code>&lt;line x1='1' y1='2.5' x2='1' y2='7.5' stroke='#37A794\"' stroke-width='2'/&gt;\n...\n&lt;line x1='150' y1='2.5' x2='150' y2='7.5' stroke='#37A794\"' stroke-width='2'/&gt;\n</code></pre> <p>The nth element costs us 78 characters,can we do better than that? </p>"},{"location":"posts/minimizing-svg-strings/#swap-line-to-path","title":"Swap <code>&lt;line&gt;</code> to <code>&lt;path&gt;</code>","text":"<p>The first thing we can look at is the type of element using to represent the lines. We could swap <code>&lt;line&gt;</code> to <code>&lt;path&gt;</code>.</p> <pre><code>&lt;path d='M1,2.5 L2,7.5' stroke='#37A794' stroke-width='2'/&gt;\n...\n&lt;path d='M150,2.5 L150,7.5' stroke='#37A794' stroke-width='2'/&gt;\n</code></pre> <p>This approach reduces the size of our nth mark to 63 characters, a 19.2% reduction. </p>"},{"location":"posts/minimizing-svg-strings/#use-defs-and-use","title":"Use <code>&lt;defs&gt;</code> and <code>&lt;use&gt;</code>","text":"<p>We can notice that is duplication of variables used to define formatting for the line's color and width. Can we we define a style that we can reuse for each mark? Yes we can use the <code>&lt;use&gt;</code> element. If we first use <code>&lt;defs&gt;</code> to define each mark types (failed, succeed ...) properties then we can reference it and only apply variables we want to overwrite, in this case just the x position of the mark.</p> <pre><code>&lt;defs&gt;\n    &lt;path id=\"L1\" d=\"M1,1 L1,9\" stroke='#37A794' stroke-width='2'/&gt;\n&lt;/defs&gt;\n&lt;use href=\"#L1\" x=\"1\"/&gt;\n...\n&lt;use href=\"#L1\" x=\"150\"/&gt;\n</code></pre> <p>We can see in the nth element we are now down to 25 characters, a 67.9% reduction.</p>"},{"location":"posts/minimizing-svg-strings/#implementing","title":"Implementing","text":"<p>I can now update the measure with this new pattern</p> <pre><code>MEASURE '_measures'[Refresh Barcode SVG] = \nVAR __svgHeight = 20\nVAR __svgWidth = 150\nVAR __Categories = SUMMARIZE( 'Progress Report', 'DateTimes'[Date], 'DateTimes'[DateTime], 'Progress Report'[XmlaRequestId] )\nVAR __Data =\n    TOPN(\n        400\n        ,ADDCOLUMNS(\n            KEEPFILTERS(\n                FILTER(\n                    KEEPFILTERS( __Categories ),\n                    not ISBLANK( CALCULATE ( MAX( 'Execution Metrics'[Status] ) ) )\n                )\n            ),\n            \"Value\", CALCULATE ( MAX( 'Execution Metrics'[Status] ) )\n        )\n        ,'DateTimes'[DateTime]\n        ,DESC\n    )\nVAR __All_Categories =\n    CALCULATETABLE(\n        FILTER(\n            KEEPFILTERS( SUMMARIZE( 'Progress Report', 'DateTimes'[Date], 'DateTimes'[DateTime] ) ),\n            not ISBLANK( CALCULATE ( MAX( 'Execution Metrics'[Status] ) ) ) &amp;&amp; not ISBLANK( 'DateTimes'[Date] )\n        ),\n        ALLSELECTED()\n    )\nVAR __MinX_Value = MINX( __All_Categories, 'DateTimes'[DateTime] )\nVAR __MaxX_Value = MAXX( __All_Categories, 'DateTimes'[DateTime] )\nVAR __RangeX = __MaxX_Value - __MinX_Value\nVAR __Lines =\n    CONCATENATEX(\n        __Data,\n        VAR __def =\n            SWITCH(\n                [Value]\n                ,\"Started\",     \"#L1\"\n                ,\"Failed\",      \"#L2\"\n                ,\"Succeeded\",   \"#L3\"\n                ,0\n            )\n        VAR _x = FORMAT( DIVIDE( 'DateTimes'[DateTime] - __MinX_Value, __RangeX, 0 ) * 100, \"0.0\", \"en-US\" )\n        RETURN\n        \"&lt;use href='\" &amp; __def &amp; \"' x='\" &amp; _x &amp; \"'/&gt;\"\n        ,\",\", [value], ASC\n    )\nVAR __Svg =\n    \"data:image/svg+xml;utf8, &lt;svg width='\" &amp; __svgWidth &amp; \"' height='\" &amp; __svgHeight &amp; \"' xmlns=\"http://www.w3.org/2000/svg\"&gt;\" &amp;\n    \"&lt;defs&gt;\" &amp;\n        \"&lt;path id='L1' d='M1,\" &amp; __svgHeight * 1    &amp; \" L1,\" &amp; __svgHeight - (__svgHeight * 1)    &amp; \"' stroke='#FFB900' stroke-width='2'/&gt;\" &amp; // Started Orange\n        \"&lt;path id='L2' d='M1,\" &amp; __svgHeight * 0.85 &amp; \" L1,\" &amp; __svgHeight - (__svgHeight * 0.85) &amp; \"' stroke='#DD6B7F' stroke-width='2'/&gt;\" &amp; // Failed Red\n        \"&lt;path id='L3' d='M1,\" &amp; __svgHeight * 0.7  &amp; \" L1,\" &amp; __svgHeight - (__svgHeight * 0.7)  &amp; \"' stroke='#37A794' stroke-width='2'/&gt;\" &amp; // Succeeded Green\n    \"&lt;/defs&gt;\" &amp;\n    __Lines &amp;\n    \"&lt;/svg&gt;\"\nRETURN\nIF( not ISEMPTY(__Data) , __Svg )\n</code></pre>"},{"location":"posts/minimizing-svg-strings/#conclusion","title":"Conclusion","text":"<p>By swapping from <code>&lt;line&gt;</code> to <code>&lt;path&gt;</code> and using <code>&lt;defs&gt;</code> and <code>&lt;use&gt;</code> I was able to reduce the size of the mark definitions 67.9%. This allows for more marks per SVG before hitting Power BI's memory limits. I would also argue it is slightly more readable. I believe this approach should be used regardless of if memory limits are hit to reduce the memory overhead of SVG visuals.</p>"},{"location":"posts/vertipaq-analyzer/","title":"Vertipaq Analyzer, Deep Dive","text":"<p>Vertipaq Analyzer, a powerful tool widely utilized in DAX Studio for enhancing Power BI Semantic Models. Recently, Semantic Link Labs has introduced their own version of Vertipaq Analyzer. But how does each version work, and how similar are they?</p>"},{"location":"posts/vertipaq-analyzer/#vertipaq-analyzer","title":"Vertipaq Analyzer","text":"<p>Vertipaq Analyzer is a C# package, which connects to a Power BI Semantic Model and extracts the models schema and metrics. It is included in DAX studio and has been the cornerstone of Tabular Model optimization for years. </p> <p> DAX Studio</p> <p>More recently Semantic Link Labs released a vertipaq_analyzer() function. </p> <p>Both versions use Dynamic Management Views (DMVs) provided by Analysis Services to collect model metrics.</p>"},{"location":"posts/vertipaq-analyzer/#dynamic-management-views-dmvs","title":"Dynamic Management Views (DMVs)","text":"<p>Microsoft Docs: Analysis Service</p> <p>Analysis Services Dynamic Management Views (DMVs) are queries that return information about model objects, server operations, and server health. The query, based on SQL, is an interface to schema rowsets. Schema rowsets are predescribed tables that contain information about Analysis Services objects and server state, including database schema, active sessions, connections, commands, and jobs that are executing on the server.</p> <p>-- Microsoft Docs: Analysis Service</p> <p>You are able to call these DMVs via the XMLA endpoint, most commonly via DAX Studio or SQL Server Management Studio (SSMS), in a SQL-like format.</p> <pre><code>select * from $SYSTEM.TMSCHEMA_TABLE_STORAGES\n</code></pre> <p>With the introduction of the INFO DAX functions they can also be accessed in DAX Query View in Power BI.</p> <pre><code>EVALUATE\nINFO.STORAGETABLES()\n</code></pre> <p>Interestingly, If you look at the documentation you'll see there are two protocols for the DMVs, <code>MS-SSAS</code> for multidimensional models and tabular models at the 1100 and 1103 compatibility levels, and <code>MS-SSAS-T</code> for Tabular Models at the 1200 and higher compatibility levels, for pre and post release of Tabular Object Model (TOM). </p> <p>Since the <code>MS-SSAS</code> protocol was built for Multidimensional models, you will notice the results sets will map Tabular attributes to fields like <code>CUBE_NAME</code> and <code>MEASURE_GROUP_NAME</code>. Additionally some processing is required to extract object names and IDs.</p> <pre><code>select * from $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS\n</code></pre> DATABASE_NAME CUBE_NAME MEASURE_GROUP_NAME PARTITION_NAME DIMENSION_NAME TABLE_ID COLUMN_ID SEGMENT_NUMBER TABLE_PARTITION_NUMBER RECORDS_COUNT ... cc5fdef3-032a-4b36-b9fd-e47565c4001d Model Calendar Calendar-3114231e-ebec-4885-9c88-30dce2c6eb92 Calendar H$Table (1110)$Date (1161) POS_TO_ID 0 0 0 ... cc5fdef3-032a-4b36-b9fd-e47565c4001d Model Calendar Calendar-3114231e-ebec-4885-9c88-30dce2c6eb92 Calendar H$Table (1110)$Date (1161) POS_TO_ID 1 0 0 ... cc5fdef3-032a-4b36-b9fd-e47565c4001d Model Calendar Calendar-3114231e-ebec-4885-9c88-30dce2c6eb92 Calendar H$Table (1110)$Date (1161) ID_TO_POS 0 0 0 ... <p>While the <code>MS-SSAS-T</code> harmonize more with TOM. You will note all the DMVs for this protocol are prefixed \"TM\".</p> <pre><code>select * from $SYSTEM.TMSCHEMA_COLUMN_STORAGES\n</code></pre> ID ColumnID Name StoragePosition DictionaryStorageID Settings ColumnFlags Collation OrderByColumn Locale ... 68 13 RowNumber 2662979B 1795 4F74 8F37 6A1BA8059B61 (13) 0 69 1 31 1033 ... 271 125 DateTime (125) 1 272 1 8 1033 ... 303 133 TestID (133) 2 304 1 8 1033 ..."},{"location":"posts/vertipaq-analyzer/#semantic-link-labs","title":"Semantic Link Labs","text":"<p>Since it is the new kid on the block, and written in python, I first looked at Semantic Link Labs.</p> <pre><code>vertipaq_analyzer(dataset: str | UUID, workspace: str | UUID | None = None, export: str | None = None, read_stats_from_data: bool = False, **kwargs)\n</code></pre> <p>The Docstring is as follows:</p> <pre><code>def vertipaq_analyzer(\n    dataset: str | UUID,\n    workspace: Optional[str | UUID] = None,\n    export: Optional[str] = None,\n    read_stats_from_data: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Displays an HTML visualization of the Vertipaq Analyzer statistics from a semantic model.\n\n    Parameters\n    ----------\n    dataset : str | uuid.UUID\n        Name or ID of the semantic model.\n    workspace : str| uuid.UUID, default=None\n        The Fabric workspace name or ID in which the semantic model exists.\n        Defaults to None which resolves to the workspace of the attached lakehouse\n        or if no lakehouse attached, resolves to the workspace of the notebook.\n    export : str, default=None\n        Specifying 'zip' will export the results to a zip file in your lakehouse (which can be imported using the import_vertipaq_analyzer function.\n        Specifying 'table' will export the results to delta tables (appended) in your lakehouse.\n        Default value: None.\n    read_stats_from_data : bool, default=False\n        Setting this parameter to true has the function get Column Cardinality and Missing Rows using DAX (Direct Lake semantic models achieve this using a Spark query to the lakehouse).\n    \"\"\"\n</code></pre> <p>If you call <code>vertipaq_analyzer()</code> in a notebook it will display the results in a HTML visualization. </p> <p> Elegant BI</p> <p>If you set <code>export</code> to 'zip' or 'table' you can instead save the result as a zip folder or delta table, to a connected lakehouse.</p> <p>If we set <code>read_stats_from_data</code> to <code>True</code> additional fields for column cardinality and missing rows are returned. This is achieved by a DAX query <code>COUNT(DISTINCT(table[column]))</code>. If the model is Direct Lake mode, spark queries are used instead.</p> <p>Direct Lake - Distinct Count</p> <p>Michael Kovalsky informed me that in the Direct Lake case, Semantic Links Labs implementation is preferable to SQLBIs. When a DAX query runs on Direct Lake model, columns have to be paged into memory. Since we are querying every column to perform a distinct count, pulling every column sequentially into memory, this can be a very intensive operation,  and can and has crashed customer's capacities. Using spark queries, the semantic model is bypassed, and the lakehouse is queried directly, avoiding this issue.</p> <p>If we go to GitHub we can see how this is implemented, for simplicity I'm going to skip the DirectLake parts.</p> <p>It first connects to TOM to get some model attributes.</p> <pre><code>with connect_semantic_model(\n        dataset=dataset_id, workspace=workspace_id, readonly=True\n    ) as tom:\n        compat_level = tom.model.Model.Database.CompatibilityLevel\n        is_direct_lake = tom.is_direct_lake()\n        def_mode = tom.model.DefaultMode\n        table_count = tom.model.Tables.Count\n        column_count = len(list(tom.all_columns()))\n</code></pre> <p>It then gathers the required metrics for various TOM entities. Columns, hierarchies and partitions are directly derived from sempy. </p> <pre><code>columns =           sempy.fabric.list_columns(dataset=dataset_id, extended=True, workspace=workspace_id)\nhierarchies =       sempy.fabric.list_hierarchies(dataset=dataset_id, extended=True, workspace=workspace_id)\npartitions =        sempy.fabric.list_partitions(dataset=dataset_id, extended=True, workspace=workspace_id)\n</code></pre> <p>Tables and relationships use sempy to get a list of items, which are enriched with metrics from the DMVs and DAX <code>INFO</code> functions. Lets look at these in more detail.</p>"},{"location":"posts/vertipaq-analyzer/#tables","title":"Tables","text":"<p>Firstly dictionary sizes per column are obtained, and summed per table.</p> <pre><code>EVALUATE \nSELECTCOLUMNS(\n    FILTER(\n        INFO.STORAGETABLECOLUMNS(), \n        [COLUMN_TYPE] = \"BASIC_DATA\"\n    ),\n    [DIMENSION_NAME],\n    [DICTIONARY_SIZE]\n)\n</code></pre> <p>It gets the sizes sizes of Tables, Hierarchies, Relationships and User Hierarchies. </p> <pre><code>EVALUATE \nSELECTCOLUMNS(\n    INFO.STORAGETABLECOLUMNSEGMENTS(),\n    [TABLE_ID],\n    [DIMENSION_NAME],\n    [USED_SIZE]\n)\n</code></pre> <p>Then aggregates with <code>SUM [USED_SIZE] GROUP BY [DIMENSION_NAME], [USED_SIZE]</code> for Data Size (<code>WHERE NOT RLIKE r'(R\\$|U\\$|H\\$).*'</code>), Hierarchy Size (<code>WHERE RLIKE r'H\\$.*'</code>), Relationship Size (<code>WHERE RLIKE r'R\\$.*'</code>) and User Hierarchy Size (<code>WHERE RLIKE r'U\\$.*'</code>).</p> <p>Finally it gets the Row Counts per Table.</p> <pre><code>SELECT \n[DIMENSION_NAME],\n[ROWS_COUNT] \nFROM $SYSTEM.DISCOVER_STORAGE_TABLES\nWHERE RIGHT ( LEFT ( TABLE_ID, 2 ), 1 ) &lt;&gt; '$'\n</code></pre> <p>Additionally it also returns descriptions, table types, refresh policies etc from TOM, but only a few of those are surfaced by <code>vertipaq_analyzer()</code>.</p>"},{"location":"posts/vertipaq-analyzer/#relationships","title":"Relationships","text":"<p>Firstly <code>fabric.list_relationships()</code> provides a list of all relationships. </p> <p>Then relationshipId is obtained to allow for a JOIN.</p> <pre><code>SELECT\n[ID] AS [RelationshipID]\n,[Name]\nFROM $SYSTEM.TMSCHEMA_RELATIONSHIPS\n</code></pre> <p>Then the storage size of relationship (where TABLE_ID starts with R$), </p> <pre><code>SELECT\n[TABLE_ID]\n,[USED_SIZE]\nFROM $SYSTEM.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS\n</code></pre> <p>The list of relationships are finally joined to their sizes ON relationshipId.</p>"},{"location":"posts/vertipaq-analyzer/#model","title":"Model","text":"<p>Finally some overall Model metrics are derived from the above: Table Count, Column Count, Total Size etc.</p>"},{"location":"posts/vertipaq-analyzer/#vertipaq-analyzer_1","title":"VertiPaq-Analyzer","text":"<p>We can now have a look at the c# version by SQLBI. </p> <p><code>Dax.Metadata</code> holds a representation of the Tabular model, including stats obtained from DMVs, which are populated by <code>Dax.Model.Extractor</code>. </p> <p>Within <code>Dax.Model.Extractor</code> there are three files of interest <code>TomExtractor.cs</code>, <code>DmvExtractor.cs</code> and <code>StatExtractor.cs</code>.</p> <ul> <li><code>TomExtractor.cs</code> uses <code>Microsoft.AnalysisServices.Tabular</code> to extract model metadata to populate <code>Dax.Metadata</code></li> <li><code>DmvExtractor.cs</code> calls DMVs to add additional metrics to <code>Dax.Metadata</code>. This is called from <code>TomExtractor.cs</code></li> <li><code>StatExtractor.cs</code> calls DAX queries to get additional metrics, like Cardinality, MissingKeys etc. This is called from <code>TomExtractor.cs</code></li> </ul> <p>We can see <code>StatExtractor.cs</code> in action if we take <code>LoadColumnStatistics()</code> as an example. We can see a DAX query is constructed to perform a <code>DISTINCTCOUNT()</code> on each column to determine cardinality.</p> <pre><code>private void LoadColumnStatistics(bool analyzeDirectQuery, DirectLakeExtractionMode analyzeDirectLake, int columnBatchSize)\n        {\n            var allColumns = \n                (from t in DaxModel.Tables\n                 // skip direct query tables if the analyzeDirectQuery is false\n                 where t.Columns.Count &gt; 1 &amp;&amp; (analyzeDirectQuery || !t.HasDirectQueryPartitions)   \n                     from c in t.Columns\n                     where c.State == \"Ready\" &amp;&amp; !c.IsRowNumber\n                        // only include the column if the table does not have Direct Lake partitions or if they are resident or if analyzeDirectLake is true\n                        &amp;&amp; (!t.HasDirectLakePartitions \n                            || (analyzeDirectLake &gt;= DirectLakeExtractionMode.ResidentOnly &amp;&amp; c.IsResident) \n                            || (analyzeDirectLake &gt;= DirectLakeExtractionMode.Referenced &amp;&amp; c.IsReferenced )\n                            || (analyzeDirectLake == DirectLakeExtractionMode.Full)\n                            )\n                     select c).ToList();\n            var loopColumns = allColumns.SplitList(columnBatchSize); // no more than 9999\n            foreach ( var columnSet in loopColumns ) {\n                var idString = 0;\n                var dax = \"EVALUATE \";\n                //only union if there is more than 1 column in the columnSet\n                if (columnSet.Count &gt; 1) { dax += \"UNION(\"; } \n                dax += string.Join(\",\", columnSet\n                    .Select(c =&gt; $\"\\n    ROW(\\\"Table\\\", \\\"{idString++:0000}{EmbedNameInString(c.Table.TableName.Name)}\\\", \\\"Column\\\", \\\"{idString++:0000}{EmbedNameInString(c.ColumnName.Name)}\\\", \\\"Cardinality\\\", {DistinctCountExpression(c)})\").ToList());\n                //only close the union call if there is more than 1 column in the columnSet\n                if (columnSet.Count &gt; 1) { dax += \")\"; }\n\n                var cmd = CreateCommand(dax);\n                cmd.CommandTimeout = CommandTimeout;\n\n                using (var reader = cmd.ExecuteReader()) {\n                    while (reader.Read()) {\n                        var tableName = reader.GetString(0).Substring(4);\n                        var columnName = reader.GetString(1).Substring(4);\n                        var cardinality = reader.IsDBNull(2) ? 0 : reader.GetInt64(2);\n\n                        var column = DaxModel.Tables.Single(t =&gt; t.TableName.Name == tableName)\n                                    .Columns.Single(c =&gt; c.ColumnName.Name == columnName);\n\n                        column.ColumnCardinality = cardinality;\n                    }\n                }\n            }\n        }\n</code></pre>"},{"location":"posts/vertipaq-analyzer/#conclusion","title":"Conclusion","text":"<p>Both flavours of Vertipaq Analyzer leverage Analysis Services DMVs to get the Semantic Models metrics. SQLBI's version is battle warn, and been the gold standard for many years. It is also convenient as it part of DAX Studio. Semantic Link Labs avoids the use of external tools, and is convenient for Fabric users. Plus it better handles the cardinality checks on Direct Lake models. With the recent addition of the VertiPaq Analyzer CLI, both options can now be used to script the analysis of a large number of models. Great thanks goes to the creators and maintainers, it looks like alot of time and effort has gone into initial development and continued support.</p>"},{"location":"posts/AdminMonitoring/","title":"Monitoring for Power BI Admins","text":"<p>Administration of a Power BI tenant can be tough. You have tread the line of giving developer space to develop reports to meet the companies reporting requirements, whilst also make sure the platform runs smoothly. A single report, that was not fully reviewed can consume all your CUs, leading to throttling and service degradation. What options are there for tenant observability?</p>"},{"location":"posts/AdminMonitoring/#fabric-capacity-metrics-app","title":"Fabric Capacity Metrics App","text":"<p>The most important report provided is one provided by Microsoft, the Fabric Capacity Metrics App. You need to install this from the App store.</p> <p>When you purchase a capacity, you get a set number of Capacity Units (CU) to perform any compute. For P1/F64, for example, this is 64 CU per second, which you can use to perform operations. There are two types of operations; Background operations, which are long duration, high CU tasks like Semantic Model refreshes, and Interactive operations for short term, low CU operations like queries from Power BI report visuals. In general usage of the capacity can be quite spiky, smoothing is applied to help even out the spikes, averaging out a operations CUs over a period of time. Background operations are smoothed over 24 hours and Interactive operations over 5 minutes. If you exceed your allocated limit you can experience various tier of throttling. </p> <p>The Fabric Capacity Metrics App shows your usage of the CUs over time. This allows you to keep track of your usage, identify expensive Semantic Models for optimization work, or load balance between capacities.</p> <p> Microsoft Docs</p> <p>You are able to build custom Reports from the Semantic Model. I found the following view to provide the most value to me, helping pinpoint problematic Semantic Models. This visual uses <code>Dates[Date]</code>, <code>Items[WorkspaceName]</code>, <code>Items[ItemName]</code>, <code>MetricByItemandOperationandDay[Operation Name]</code> and <code>MetricByItemandOperationandDay[sum_CU]</code>.</p> <p></p>"},{"location":"posts/AdminMonitoring/#feature-usage-and-adoption-report","title":"Feature Usage and Adoption Report","text":"<p>If you have the Fabric Administrator role you also have access to the Feature Usage and Adoption Report out of the box, in the Admin monitoring Workspace. This provides some basic reporting on usage and activities on the tenant. This provides data for the last 30 days.</p> <p></p>"},{"location":"posts/AdminMonitoring/#log-analytics-integration","title":"Log Analytics Integration","text":"<p>Power BI has a integration with Log Analytics. This sends Analysis Services engine trace events, from connected Workspaces to Log Analytics.</p> <p>Microsoft has provided the Fabric Log Analytics for Analysis Services Engine report template. This uses Execution Metrics Logs to provide reporting on CPU and duration metrics for operations performed by Semantic Models. Additionally Progress Report Logs are used to give addition details on Refreshes. </p> <p> Fabric Log Analytics for Analysis Services Engine report template</p> <p>This data is very useful for:</p> <ul> <li>Identifying expensive queries and refreshes</li> <li>Track and debug Refresh failures</li> <li>Track and trend errors</li> <li>Determine exact queries that resulted in errors for users</li> <li>Generate report usage metrics</li> </ul>"},{"location":"posts/AdminMonitoring/#scanner-apis","title":"Scanner APIs","text":"<p>Microsoft Docs: Run metadata scanning</p> <p>With the scanner APIs, you can extract information such as item name, owner, sensitivity label, and endorsement status. For Power BI semantic models, you can also extract the metadata of some of the objects they contain, such as table and column names, measures, DAX expressions, mashup queries, and so forth. The metadata of these semantic model internal objects is referred to as subartifact metadata.</p> <p>-- Microsoft Docs: Run metadata scanning</p> <p>I have a previous post on using the Scanner APIs to extracting tenant metadata. </p> <p>Rui Romano has a reporting solution, pbimonitor, that visuals that data as a catalogue for all Power BI assets.</p> <p> pbimonitor</p>"},{"location":"posts/AdminMonitoring/#log-analytics-integration-and-scanner-apis-bffs","title":"Log Analytics Integration and Scanner APIs BFFs","text":"<p>The real power comes from combining the Log Analytics and Scanner APIs. Lets first look at the Scanner. We can define a <code>Objects</code> table, which is a union of Workspaces, Semantic Models and Reports, whose IDs are all captured in a single field <code>objectId</code>. Additionally it worth having <code>datasetId</code> against both Semantic Models and Reports, meaning for Reports, you can determine the upstream Semantic Model, and for Semantic Model, all the downstream Reports.</p> <p></p> <p>Log Analytics records Semantic Model server traces, so we can easily appended the data model from the Fabric Log Analytics for Analysis Services Engine report template. We connect it to the <code>Objects</code> table rather than to the <code>Semantic Models</code> dimension so it respects the filters from the other dimensions.</p> <p></p> <p>Now the Server Traces have been enriched you get some benefits. Firstly you can see items that have no traces, which means you can identify unused artifact that can be decommissioned. Secondly, the logs have <code>reportId</code>, with a report dimension you can provide the report name making the data more understandable.</p> <p>You can see in the Scanner ER diagram I have the <code>accessToObjects</code> and <code>accessToObject Edges</code> tables. These are from my previous posts on GraphFrames, and are used in a Deneb Force Direct graph. These allow you to know the exact permissions a specific user has on Workspace, Semantic Models etc, even if they inherit the permission through a long chain of User Groups. Additionally you can filter to a specific object and visually see what permissions are granted and by what path.</p> <p></p> <p>For me, the last piece of the puzzle, is to add a tenant wide Vertipaq Analyzer scan. If you have Fabric this is easy as you can run the DMVs with Sempy, as investigated in my previous post on Vertipaq Analyzer. I don't, and annoyingly the REST APIs don't support DMVs or DAX INFO functions. This could be a fantastic addition, allowing the identification of potential areas for optimizations, by comparing refresh times to Semantic Model sizes for example.</p>"},{"location":"posts/AdminMonitoring/#fuam","title":"FUAM","text":"<p>If you have Fabric there is now another solution that is part of Microsoft's Fabric Toolbox: Fabric Unified Admin Monitoring (FUAM).</p> <p>FUAM extracts the following data from the tenant and stores it in a lakehouse:</p> <ul> <li>Tenant Settings</li> <li>Delegated Tenant Settings</li> <li>Activities</li> <li>Workspaces</li> <li>Capacities</li> <li>Capacity Metrics</li> <li>Tenant meta data (Scanner API)</li> <li>Capacity Refreshables</li> <li>Git Connections</li> </ul> <p>It calls a large number of Power BI and Fabrics APIs, including the the Activity, and Scanner APIs. It also gets CU usage data from the Fabric Capacity Metrics App. Interestingly it also runs Semantic Links Labs Vertipaq Analyzer to get sizes or Semantic Models tables, columns etc.</p> <p>If you have Fabric and need some quick information this could be a fantastic start. It does lack the Server Traces, but if you setup the Log Analytics integration you can still report on this separately with the template mentioned above.</p>"},{"location":"posts/fabric-cicd/","title":"Fabric-cicd, Kicking the tyres","text":"<p>Fabric-cicd, is a open source Python library designed to streamline Continuous Deployment workflows within Microsoft Fabric. It offers a clean abstraction on top of the on top of the Fabric APIs allowing you to easily deploy your Source Controlled Fabric items. Let's take a look.</p>"},{"location":"posts/fabric-cicd/#fabric-cicd","title":"Fabric-cicd","text":"<p>Fabric-cicd is a python library designed to deploy Fabric Items to Fabric workspaces. When Power BI Projects (PBIP) were released last year, the project team provided a Powershell script to help deploy them. I have been successfully using this script for the last year. Both the PowerShell script and Fabric-cicd are wrapper on the Create Item and Update Item Definition Fabric APIs. Personally, I am more partial to python, so this new library piqued my interest.</p> <p>Of note, Kevin Chant has produced a range of blog posts on this library that are worth looking at, including some example Github and Azure DevOps pipelines.</p> <p>Fabric-cicd currently supports the deployment of:</p> <ul> <li>DataPipeline</li> <li>Environment</li> <li>Notebook</li> <li>Report</li> <li>SemanticModel</li> <li>Lakehouse</li> <li>MirroredDatabase</li> <li>VariableLibrary</li> </ul>"},{"location":"posts/fabric-cicd/#setup","title":"Setup","text":"<p>You can install fabric-cicd via:</p> <pre><code>pip install fabric-cicd\n</code></pre> <p>The following folder structure below is expected. </p> <pre><code>\u251c\u2500\u2500 \ud83d\udcc1 fabricItems\n\u2502    \u251c\u2500\u2500 \ud83d\udcc1 test.SemanticModel\n\u2502    \u251c\u2500\u2500 \ud83d\udcc1 test.Report\n\u2502    \u2514\u2500\u2500 \ud83d\udcc4 parameter.yml           # *optional* find and replace text in files \n\u251c\u2500\u2500 \ud83d\udcc4 deployment.py                # script to call fabric-cicd\n\u2514\u2500\u2500 \ud83d\udcc4 .gitignore\n</code></pre> <p>Firstly we'll add a test Power BI Project file to the <code>fabricItems</code> folder.</p> <p><code>parameter.yml</code> can be use to find and replace any value in your files. If you provide a <code>environment</code> variable in your <code>FabricWorkspace()</code> definition you can replace specific values for test vs prod. This provides alot of flexibility, but it feels like for general variables you want to change by default, you might want a more well defined and concise structure that improves the readability of the repo. I like pbitools use of a json file to adjust properties, but yaml might also be a good option. I am going to add a <code>deploymentManifest.json</code> to define the Workspace name to deploy to, based on a Environment name.</p> StructuredeploymentManifest.json <pre><code>\u251c\u2500\u2500 \ud83d\udcc1 fabricItems\n\u2502    \u251c\u2500\u2500 \ud83d\udcc1 test.SemanticModel\n\u2502    \u251c\u2500\u2500 \ud83d\udcc1 test.Report\n\u2502    \u2514\u2500\u2500 \ud83d\udcc4 parameter.yml\n\u251c\u2500\u2500 \ud83d\udcc4 deployment.py\n+\u251c\u2500\u2500 \ud83d\udcc4 deploymentManifest.json\n\u2514\u2500\u2500 \ud83d\udcc4 .gitignore\n</code></pre> <p>For now I'll just set all environments to deploy to the same workspace.</p> <pre><code>```json\n{\n  \"repo\": {\n    \"environment\": {\n      \"dev\": {\"workspace\": \"fabric-cicd\", \"workspaceId\": \"c01092c3-7f18-4488-840b-34b5764ecfcb\"},\n      \"uat\": {\"workspace\": \"fabric-cicd\", \"workspaceId\": \"c01092c3-7f18-4488-840b-34b5764ecfcb\"},\n      \"prod\": {\"workspace\": \"fabric-cicd\", \"workspaceId\": \"c01092c3-7f18-4488-840b-34b5764ecfcb\"}\n    }\n  }\n}\n```\n</code></pre> <p>We can now define our deployment script in <code>deployment.py</code>.</p> <pre><code>from azure.identity import InteractiveBrowserCredential\nfrom fabric_cicd import FabricWorkspace, publish_all_items\nimport json\n\n# Authentication\ncredential = InteractiveBrowserCredential() # To use another auth method if not testing locally https://microsoft.github.io/fabric-cicd/latest/example/authentication/\n\n# Environment\nenvironment = \"dev\" # To be passed from pipeline In DevOps https://microsoft.github.io/fabric-cicd/latest/example/deployment_variable/\n\n# deploymentManifest\nwith open(\"assets\\scripts\\\\0027-fabric-cicd\\deploymentManifest.json\") as json_data:\n    manifest = json.load(json_data)\n\n# Initialize the FabricWorkspace object with the required parameters\ntarget_workspace = FabricWorkspace(\n    workspace_id = manifest['repo']['environment'][environment]['workspaceId'],\n    environment = environment,\n    repository_directory = \"assets\\scripts\\\\0027-fabric-cicd\\\\fabricItems\",\n    item_type_in_scope = [\"SemanticModel\", \"Report\"],\n    token_credential = credential,\n)\n\n# Publish all items defined in item_type_in_scope\npublish_all_items(target_workspace)\n</code></pre>"},{"location":"posts/fabric-cicd/#deployment","title":"Deployment","text":"<p>We can run <code>deployment.py</code> to deploy our fabric items.</p> <p></p> <p>And we can see our Semantic Model and Report have been successfully deployed.</p> <p></p>"},{"location":"posts/fabric-cicd/#conclusions","title":"Conclusions","text":"<p>I would recommend using this library over a homebrew solution, it is simple to use and seems to be well supported with frequent updates. Looking forward to future developments, particularly if git diffs become supported, and support for gateway connections and refreshes. As a side note, I followed Kevin Chants blogs and managed to painlessly setup a Azure Dev Ops pipeline in a couple hours, so big shout out there.</p>"},{"location":"posts/RealTime-Intelligence/","title":"From Wrist to Dashboard - Real-Time Dashboards in Fabric","text":"<p>I was recently inspired by the real-time data capabilities showcased in a Guy In a Cube video (Harvesting Insights, Block by Block (Minecraft + Microsoft Fabric). In it, Simon Nuss demonstrated how to stream telemetry, in real-time, from Minecraft directly into Microsoft Fabric. Looking around at my desk I saw my Garmin watch. I decided I wanted to try streaming my heart rate into Microsoft Fabric, and visualize it dynamically within Power BI.</p>"},{"location":"posts/RealTime-Intelligence/#architecture","title":"Architecture","text":"<p>Here\u2019s the flow I designed:</p> <ul> <li>Connecting the Source: My Garmin watch initiates the data flow by connecting to my laptop via Bluetooth Low Energy (BLE).</li> <li>Streaming to Azure: A custom Python script acts as an Event Producer, capturing the heart rate data from the watch and streaming it to an Azure Event Hub.</li> <li>Ingesting with Fabric: A Fabric Eventstream is configured to pull the continuous data stream directly from the Azure Event Hub.</li> <li>Storing for Analysis: The ingested data lands in a KQL database within a Fabric Eventhouse.</li> <li>Real-Time Visualization: A Power BI report connects to the KQL database using Direct Query mode and is set up for automatic page refresh to display the heart rate data with minimal latency.</li> </ul> <p></p>"},{"location":"posts/RealTime-Intelligence/#event-producer-from-wrist-to-stream","title":"Event Producer: From Wrist to Stream","text":"<p>My Garmin Forerunner watch is capable of broadcasting heart rate data using the standard Bluetooth Low Energy (BLE) Heart Rate Service. I used the <code>bleak</code> library to connect to the Garmin watch via BLE and subscribe to heart rate measurement notifications. To send this captured data to Azure, I leveraged the Azure Event Hub Python SDK. This effectively turns my laptop into an Event Hub Producer, sending heart rate readings as discrete events.</p> <p>The destination for this data stream is an Azure Event Hub, a highly scalable, real-time data ingestion service designed to handle millions of events per second.</p> <pre><code>import asyncio\nimport json\nimport datetime\nfrom functools import partial # For passing extra arguments to handlers\n\nfrom bleak import BleakClient, BleakScanner\nfrom bleak.backends.device import BLEDevice\nfrom bleak.exc import BleakError # Import BleakError for more specific exception handling\n\n# Azure Event Hubs\nfrom azure.eventhub.aio import EventHubProducerClient\nfrom azure.eventhub import EventData\nfrom azure.identity.aio import AzureCliCredential\n\nimport traceback\n\n# --- Azure Event Hub Configuration ---\nEVENT_HUB_FQDN = \"Garmin.servicebus.windows.net\"\nEVENT_HUB_NAME = \"garmin\"\n\n# --- BLE Configuration ---\nHR_SERVICE_UUID = \"0000180d-0000-1000-8000-00805f9b34fb\"\nHR_MEASUREMENT_CHAR_UUID = \"00002a37-0000-1000-8000-00805f9b34fb\"\n\n# --- Global Queue and Producer for Event Hubs ---\nevent_data_queue = asyncio.Queue(maxsize=1000) # Optional: Set a maxsize to prevent runaway memory usage\nproducer_client = None # Will be initialized in discover_and_connect\n\nasync def send_events_to_hub(producer: EventHubProducerClient, queue: asyncio.Queue, batch_interval_seconds=5):\n    \"\"\"\n    Asynchronously collects events from a queue and sends them as batches to Event Hubs.\n    \"\"\"\n\n    print(f\"[{datetime.datetime.now()}] Starting Event Hubs sender task (batching every {batch_interval_seconds} seconds)...\")\n    while True:\n        await asyncio.sleep(batch_interval_seconds) # Wait first, then collect. Or collect then wait.\n        events_to_send = []\n        try:\n            # Collect all available events from the queue without blocking for new ones\n            while not queue.empty():\n                try:\n                    event_data = queue.get_nowait()\n                    events_to_send.append(event_data)\n                    queue.task_done() # Notify queue that item processing is done\n                except asyncio.QueueEmpty:\n                    break # Should not happen if we check queue.empty() first but good practice\n\n            if events_to_send:\n                print(f\"[{datetime.datetime.now()}] Attempting to send {len(events_to_send)} events to Event Hubs.\")\n                event_data_batch = await producer.create_batch()\n                for event_idx, event in enumerate(events_to_send):\n                    try:\n                        event_data_batch.add(event)\n                    except ValueError:\n                        # Batch is full, send current batch and start a new one\n                        if len(event_data_batch) &gt; 0: # Ensure there's something to send\n                            print(f\"[{datetime.datetime.now()}] Batch full. Sending {len(event_data_batch)} events (chunk {event_idx // len(event_data_batch) +1}).\")\n                            await producer.send_batch(event_data_batch)\n                        event_data_batch = await producer.create_batch()\n                        event_data_batch.add(event) # Add the event that didn't fit\n\n                if len(event_data_batch) &gt; 0: # Send any remaining events in the last batch\n                    print(f\"[{datetime.datetime.now()}] Sending final batch of {len(event_data_batch)} events.\")\n                    await producer.send_batch(event_data_batch)\n                print(f\"[{datetime.datetime.now()}] Successfully sent a total of {len(events_to_send)} events to Event Hubs.\")\n            else:\n                print(f\"[{datetime.datetime.now()}] No events in queue to send.\")\n\n        except Exception as e:\n            print(f\"[{datetime.datetime.now()}] Error sending events to Event Hub: {e}\")\n            traceback.print_exc()\n\n# Modified notification_handler to accept device_address\ndef notification_handler(ble_device_address: str, sender_characteristic_handle: int, data: bytearray):\n    \"\"\"\n    Handles BLE Heart Rate Measurement notifications and puts data into a queue.\n    'sender_characteristic_handle' is the handle of the characteristic sending the notification.\n    'data' is the bytearray of the notification.\n    'ble_device_address' is the MAC address of the BLE device.\n    \"\"\"\n    print(f\"[{datetime.datetime.now()}] NOTIFICATION HANDLER: Received data from handle {sender_characteristic_handle} on device {ble_device_address}. Data: {data.hex()}\")\n    flags = data[0]\n    hr_format_is_uint16 = (flags &amp; 0x01)\n\n    hr_value_offset = 1\n    heart_rate = 0\n\n    if hr_format_is_uint16:\n        if len(data) &gt;= 3: # Ensure data is long enough\n            heart_rate = int.from_bytes(data[hr_value_offset:hr_value_offset+2], byteorder='little')\n            hr_value_offset += 2\n        else:\n            print(f\"[{datetime.datetime.now()}] NOTIFICATION HANDLER: Data too short for UINT16 HR. Data: {data.hex()}\")\n            return # Invalid data\n    else:\n        if len(data) &gt;= 2: # Ensure data is long enough\n            heart_rate = data[hr_value_offset]\n            hr_value_offset += 1\n        else:\n            print(f\"[{datetime.datetime.now()}] NOTIFICATION HANDLER: Data too short for UINT8 HR. Data: {data.hex()}\")\n            return # Invalid data\n\n    timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()\n\n    hr_event_data = {\n        \"timestamp\": timestamp,\n        \"heart_rate_bpm\": heart_rate,\n        \"source\": \"Garmin Forerunner BLE\",\n        \"device_address\": ble_device_address\n    }\n    event_body_str = json.dumps(hr_event_data)\n    event_data = EventData(body=event_body_str) # Pass as string to EventData\n\n    try:\n        event_data_queue.put_nowait(event_data)\n        print(f\"[{datetime.datetime.now()}] Queued HR: {heart_rate} bpm from {ble_device_address} at {timestamp}. Queue size: ~{event_data_queue.qsize()}\")\n    except asyncio.QueueFull:\n        print(f\"[{datetime.datetime.now()}] Event queue is full ({event_data_queue.qsize()}), dropping HR event from {ble_device_address}.\")\n    except Exception as e:\n        print(f\"[{datetime.datetime.now()}] Error queuing HR event from {ble_device_address}: {e}\")\n\nasync def discover_and_connect_garmin_hr():\n    print(f\"[{datetime.datetime.now()}] Scanning for Garmin Heart Rate Broadcast...\")\n    garmin_device: BLEDevice = None\n\n    try:\n        # Increased timeout for discovery, ensure device is actively broadcasting\n        devices = await BleakScanner.discover(service_uuids=[HR_SERVICE_UUID], timeout=20.0)\n    except BleakError as e:\n        print(f\"[{datetime.datetime.now()}] BleakScanner error during discovery: {e}\")\n        return # Cannot proceed\n\n    if not devices:\n        print(f\"[{datetime.datetime.now()}] No BLE devices found advertising the HR service UUID: {HR_SERVICE_UUID}.\")\n    else:\n        for device in devices:\n            print(f\"[{datetime.datetime.now()}] Found device: {device.name} ({device.address}), RSSI: {device.rssi}, Services: {device.metadata.get('uuids', [])}\")\n            # Prioritize devices explicitly named \"Garmin\" but also consider any device broadcasting the HR service\n            if \"Garmin\" in (device.name or \"\"):\n                print(f\"[{datetime.datetime.now()}] Found potential Garmin device by name: {device.name} ({device.address})\")\n                garmin_device = device\n                break\n            # Fallback: if no \"Garmin\" named device, take the first one advertising the HR service (already filtered by discover)\n            if not garmin_device and HR_SERVICE_UUID in device.metadata.get('uuids', []):\n                print(f\"[{datetime.datetime.now()}] Found device broadcasting HR service (fallback): {device.name} ({device.address})\")\n                garmin_device = device\n\n    if not garmin_device:\n        print(f\"[{datetime.datetime.now()}] No Garmin device or device broadcasting Heart Rate service ({HR_SERVICE_UUID}) found. Make sure your watch is actively broadcasting HR and is discoverable.\")\n        return\n\n    print(f\"[{datetime.datetime.now()}] Selected device: {garmin_device.name} ({garmin_device.address})\")\n\n    client = BleakClient(garmin_device.address)\n    sender_task = None # Initialize sender_task\n    global producer_client # Declare that we are using the global producer_client\n\n    try:\n        print(f\"[{datetime.datetime.now()}] Initializing Event Hub Producer Client...\")\n        credential = AzureCliCredential()\n        producer_client = EventHubProducerClient(\n            fully_qualified_namespace=EVENT_HUB_FQDN,\n            eventhub_name=EVENT_HUB_NAME,\n            credential=credential\n        )\n        print(f\"[{datetime.datetime.now()}] Event Hub Producer Client initialized for {EVENT_HUB_FQDN}/{EVENT_HUB_NAME}.\")\n\n        # Start the Event Hub sender task in the background\n        sender_task = asyncio.create_task(send_events_to_hub(producer_client, event_data_queue))\n        print(f\"[{datetime.datetime.now()}] Event Hub sender task created.\")\n\n        print(f\"[{datetime.datetime.now()}] Connecting to {garmin_device.name} ({garmin_device.address})...\")\n        await client.connect(timeout=20.0) # Increased connection timeout\n\n        if not client.is_connected:\n            print(f\"[{datetime.datetime.now()}] Failed to connect to {garmin_device.name}.\")\n            return\n\n        print(f\"[{datetime.datetime.now()}] Connected to {garmin_device.name} ({garmin_device.address}). Discovering services...\")\n\n        hr_char_found = False\n        for service in client.services:\n            if service.uuid.lower() == HR_SERVICE_UUID.lower():\n                for char in service.characteristics:\n                    if char.uuid.lower() == HR_MEASUREMENT_CHAR_UUID.lower():\n                        if \"notify\" in char.properties:\n                            hr_char_found = True\n                            print(f\"[{datetime.datetime.now()}] Found HR Measurement characteristic ({char.uuid}) with notify property.\")\n                        else:\n                            print(f\"[{datetime.datetime.now()}] HR Measurement characteristic ({char.uuid}) found, but does NOT support notifications. Properties: {char.properties}\")\n                        break\n                if hr_char_found:\n                    break\n\n        if not hr_char_found:\n            print(f\"[{datetime.datetime.now()}] Heart Rate Measurement characteristic ({HR_MEASUREMENT_CHAR_UUID}) not found or does not support notifications.\")\n            # Attempt to list all characteristics for debugging\n            print(f\"[{datetime.datetime.now()}] Listing all services and characteristics found on {garmin_device.name}:\")\n            for s in client.services:\n                print(f\"  Service UUID: {s.uuid}\")\n                for c in s.characteristics:\n                    print(f\"    Characteristic UUID: {c.uuid}, Properties: {c.properties}\")\n            return\n\n        print(f\"[{datetime.datetime.now()}] Subscribing to Heart Rate notifications ({HR_MEASUREMENT_CHAR_UUID})...\")\n\n        # Use functools.partial to pass the device address to the handler\n        # The handler signature for start_notify is callback(sender_handle_or_char_obj, data_bytearray)\n        # So, our `notification_handler` should be `def notification_handler(ble_device_address, sender_handle, data)`\n        # And partial should be `partial(notification_handler, garmin_device.address)`\n        # This means `garmin_device.address` will be the *first* argument to `notification_handler`.\n\n        # Bleak's start_notify calls the callback with (BleakGATTCharacteristic, bytearray) or (int, bytearray)\n        # So, our `notification_handler` will receive `ble_device_address` as its first arg from partial,\n        # then `sender_handle` and `data` from bleak.\n        handler_with_address = partial(notification_handler, garmin_device.address)\n\n        await client.start_notify(HR_MEASUREMENT_CHAR_UUID, handler_with_address)\n\n        print(f\"[{datetime.datetime.now()}] Successfully subscribed to HR notifications. Receiving data and queuing for Event Hubs. Press Ctrl+C to stop.\")\n        while client.is_connected:\n            await asyncio.sleep(1) # Keep the connection alive and allow other tasks to run\n        print(f\"[{datetime.datetime.now()}] Client disconnected.\")\n\n    except BleakError as e: # Catch specific Bleak errors\n        print(f\"[{datetime.datetime.now()}] A BLEAK error occurred: {e}\")\n    except asyncio.TimeoutError:\n        print(f\"[{datetime.datetime.now()}] Operation timed out (e.g., connection or discovery).\")\n    except Exception as e:\n        print(f\"[{datetime.datetime.now()}] An unexpected error occurred in discover_and_connect: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        print(f\"[{datetime.datetime.now()}] Cleaning up...\")\n        if client and client.is_connected:\n            try:\n                print(f\"[{datetime.datetime.now()}] Attempting to stop notifications...\")\n                await client.stop_notify(HR_MEASUREMENT_CHAR_UUID)\n                print(f\"[{datetime.datetime.now()}] Stopped notifications.\")\n            except Exception as e_notify:\n                print(f\"[{datetime.datetime.now()}] Error stopping notifications (may not have been started or char not found): {e_notify}\")\n\n            print(f\"[{datetime.datetime.now()}] Attempting to disconnect client...\")\n            await client.disconnect()\n            print(f\"[{datetime.datetime.now()}] Disconnected from Heart Rate monitor.\")\n\n        if sender_task and not sender_task.done():\n            print(f\"[{datetime.datetime.now()}] Cancelling Event Hub sender task...\")\n            sender_task.cancel()\n            try:\n                await sender_task # Await its cancellation\n            except asyncio.CancelledError:\n                print(f\"[{datetime.datetime.now()}] Event Hub sender task successfully cancelled.\")\n            except Exception as e_task_cancel: # Catch any other error during task cleanup\n                print(f\"[{datetime.datetime.now()}] Error during sender task cancellation: {e_task_cancel}\")\n\n        if producer_client: # Ensure producer_client was initialized\n            print(f\"[{datetime.datetime.now()}] Closing Event Hub Producer Client...\")\n            await producer_client.close()\n            print(f\"[{datetime.datetime.now()}] Event Hub Producer Client closed.\")\n\n        print(f\"[{datetime.datetime.now()}] Cleanup finished.\")\n\nasync def main(): # Wrap the main execution in an async function\n    print(f\"[{datetime.datetime.now()}] Starting Garmin HR to Azure Event Hub Streamer...\")\n    try:\n        await discover_and_connect_garmin_hr()\n    except KeyboardInterrupt:\n        print(f\"\\n[{datetime.datetime.now()}] Exiting due to user interrupt (Ctrl+C).\")\n    except Exception as e: # Catch-all for fatal errors in main startup\n        print(f\"[{datetime.datetime.now()}] Fatal application error in main: {e}\")\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    await main()\n</code></pre> <ul> <li>The <code>bleak</code> library handles the Bluetooth connection and receiving notifications.</li> <li><code>notification_handler</code> is the callback function that receives raw heart rate data bytes from the watch. It parses the data according to the BLE Heart Rate Measurement Characteristic specification.</li> <li>The parsed heart rate value, along with a timestamp and device info, is formatted into a JSON string.</li> <li>An <code>asyncio.Queue</code> (<code>event_data_queue</code>) is used to buffer the incoming heart rate events. This decouples the BLE notification handler from the Event Hub sending logic.</li> <li>The <code>send_events_to_hub</code> task runs concurrently, pulling messages from the queue and sending them to Azure Event Hub in batches, every 5 seconds.</li> <li>The <code>azure.identity.aio.AzureCliCredential</code> allows the script to authenticate using the logged-in Azure account via the Azure CLI, simplifying setup.</li> </ul>"},{"location":"posts/RealTime-Intelligence/#setting-up-azure-event-hub","title":"Setting up Azure Event Hub","text":"<p>Before running the Python producer, you need an Azure Event Hub instance to receive the data. First, create an Event Hub Namespace in the Azure portal, which serves as a container for your Event Hub(s).</p> <p></p> <p>Within the namespace, create a new Event Hub.</p> <p></p> <p>For your Python application to send data to this Event Hub, you need to grant your Azure identity (the one you logged into with Azure CLI) the necessary permissions. Assign the <code>Azure Event Hubs Data Sender</code> role to your account on the Event Hub Namespace.</p>"},{"location":"posts/RealTime-Intelligence/#ingesting-with-fabric-real-time-intelligence","title":"Ingesting with Fabric Real-Time Intelligence","text":"<p>Now we have setup the pipeline for data to flow into the Azure Event Hub, the next step is to bring it into Microsoft Fabric. The data will be stored in a KQL Database (within an Eventhouse), passing through a Eventstream.</p> <p>I started by putting my watch into heart rate broadcast mode and running the Python Event Producer script. This ensures there's a live data stream flowing into the Event Hub, which helps Fabric Eventstream to infer a schema.</p> <p>The process within Fabric involves:</p> <ul> <li>Creating a Fabric workspace: A container space for Fabric items.</li> <li>Creating an Eventhouse: This is where the container for the KQL database.</li> <li>Creating an Eventstream: This is Fabric's feature for ingesting, transforming, and routing real-time data streams.</li> <li>Add Azure Event Hub as a Source: Configure the Eventstream to connect to your Azure Event Hub.      </li> <li>Add Eventhouse KQL Database as a Destination: Configure the Eventstream to route the ingested data to the KQL database.     </li> <li>Publish the Eventstream: Activate the Eventstream to start the data ingestion process.</li> <li>Create Eventhouse table </li> <li>Verify Data Ingestion: Check the KQL database to ensure data is flowing in as expected with a simple KQL query.   </li> </ul>"},{"location":"posts/RealTime-Intelligence/#visualizing-real-time-data-in-power-bi","title":"Visualizing Real-Time Data in Power BI","text":"<p>With the heart rate data now stored and queryable in the KQL database within Fabric, the final step is visualization. To achieve a \"real-time\" dashboard experience in Power BI, we can connect to the KQL database using a DirectQuery connection. To make the report feel live, we can configure the Power BI page to refresh automatically at a set interval, such as every second.</p> <p></p> <p>We can then create simple Linechart and watch the dataflow in.</p> <p></p> <p>Note: As mentioned in the Python script section, the data is batched and sent to Event Hubs every 5 seconds. This means the updates in Power BI won't be instantaneous per individual beat but will refresh with the latest batch of data every 5 seconds, resulting in slightly jumpy updates on the graph.</p>"},{"location":"posts/RealTime-Intelligence/#real-time-dashboard","title":"Real-Time Dashboard","text":"<p>Fabric also offers native Real-Time Dashboards, which provide another way to visualize data directly from a KQL database. Creating visuals here was straightforward, by defining a KQL query for each visual element.</p> <p></p> <p>This provides an alternative, often lower-latency, visualization experience compared to Power BI for scenarios requiring highly dynamic updates.</p>"},{"location":"posts/RealTime-Intelligence/#conclusion","title":"Conclusion","text":"<p>Setting up this end-to-end pipeline to stream real-time heart rate data from a Garmin watch to Power BI via Microsoft Fabric was a surprisingly straightforward process. While bringing a project like this to production would involve considerations around scalability, monitoring, and more sophisticated data transformation within Fabric, this exercise provided a fantastic hands-on introduction to Fabric's real-time intelligence capabilities. </p> <p>PS. After finishing this project I found a another Guy in a Cube Video (Creating your first EVENTSTREAM in Microsoft Fabric) which does a good job of visually going through this process.</p>"},{"location":"posts/one-pipeline/","title":"One Pipeline to Rule Them All","text":"<p>In the complex landscapes of modern data analytics, managing Fabric deployments can sometimes feel like a daunting quest. If you've ever found yourself creating and defining a Azure DevOps pipeline per repo you know the pain: vast number of pipelines, repetitive YAML, spiraling maintenance, and a general sense of being \"bound\" by pipeline sprawl.</p> <p>Just like the One Ring brought power and control, we seek One Pipeline to Rule Them All, One Pipeline to Find Them, One Pipeline to Bring Them All, and in the darkness bind them; In the Land of Fabric where the data lies. This isn't just about reducing pipeline count; it's about establishing a centralized, efficient, and auditable CI/CD strategy for your Fabric projects.</p>"},{"location":"posts/one-pipeline/#in-the-land-of-fabric-where-the-data-lies-the-challenge","title":"In the Land of Fabric Where the Data Lies: The Challenge","text":"<p>Many organizations adopting Azure DevOps for Fabric version control and deployments following some common patterns:</p> <ul> <li>Single Git repo for each workspace or related set of Fabric Items, with each repo having its own pipeline</li> <li>Large multi-repo</li> </ul> <p>In these cases it can be assumed that you will have a repo (<code>shared-fabric-pipeline</code>) that will define the core logic of the pipeline (<code>shared-pipeline.yml</code>), plus any CICD scripts (i.e. fabric-cicd). This will be extended (<code>extends:</code>) in the Fabric Items repo(s), passing any repo specific configuration.</p> <p></p> <p>There are pros and cons to both of these approaches.</p>"},{"location":"posts/one-pipeline/#pipeline-per-repo","title":"Pipeline Per Repo","text":"<p>\u2714\ufe0f Pipeline Versions: Each repo can have a pipeline definition that extends a specific version of a shared pipeline definition. If a breaking change is introduced in a new major release of a pipeline, individual repos can transitioned as required</p> <p>\u2714\ufe0f Security: You can limit access and approvals for specific repos, to specific people per repo</p> <p>\u2714\ufe0f Limited Scope: Focused git history with a narrow scope, limited to a small number of items. Can easily be used to auto generate release notes. If repo only contains related items, the scope of changes can be easily understood</p> <p>\u274c Pipeline Proliferation: Large number of duplicate pipelines</p>"},{"location":"posts/one-pipeline/#multi-repo","title":"Multi-Repo","text":"<p>\u2714\ufe0f Pipeline Reusability: Single Pipeline to deploy any Fabric Item</p> <p>\u2714\ufe0f Easier Collaboration: All Fabric Items are in one place meaning developer don't have to swap between repositories</p> <p>\u274c Deployment: The pipeline has to detect and deploy only updated items, else there will be alot of unnecessary deployments</p> <p>\u274c Slower Cloning: Initial cloning of the repo will be will be time consuming</p> <p>\u274c Upgrading Tooling: Deployments will all use the same single version of the pipeline definition. Updates to the pipeline must not introduce breaking changes, otherwise the structure of the entire repo must be updated to accommodate the change</p> <p>\u274c Security: You are not able to limit access and approvals for specific items</p>"},{"location":"posts/one-pipeline/#one-pipeline-to-rule-them-all","title":"One Pipeline to Rule Them All","text":"<p>We want we really want is the best of both worlds, the flexibility of many repos, and a consolidation of pipelines.</p> <p></p> <p>There are a couple of approaches that can achieve this:</p> <ul> <li> <p>Repository Resource Definition: Developer's register each of their Fabric Item repo as a <code>Resources:</code> in the <code>shared-fabric-pipeline</code> repo's <code>shared-pipeline.yml</code>. Commits on defined branches of the Fabric Item Repos will trigger the pipeline. The pipeline will be configured to only checkout the triggering repo, for a targeted deployment</p> </li> <li> <p>Build Validation: Define a build validation policy for each branch in each Fabric Item repo, that uses the <code>shared-fabric-pipeline</code> repo's pipeline. When a merge request is created into a branch with a build policy the defined pipeline will run and must succeed for the Merge Request to be accepted</p> </li> </ul> <p>Personally I prefer the build validation approach since developers can focus on configuring their Fabric Item repo without having to update or worry about the <code>shared-fabric-pipeline</code> repo. The rest of this post will be focused on this approach.</p>"},{"location":"posts/one-pipeline/#parameterizedtemplated-yaml-pipelines","title":"Parameterized/Templated YAML Pipelines","text":"<p>Firstly we need to define the <code>shared-pipeline.yml</code> within the <code>shared-fabric-pipeline</code> repo. This template will accepts parameters like the <code>path</code> to Fabric Items, <code>target workspace ID</code>, and <code>environment</code>.  Deployment is performed with fabric-cicd. </p> <p>For this repo using the Microsoft Release Flow makes sense. Semantic versioning would be used, and each Major release will spin off a new release branch (i.e. <code>v1</code>, <code>v2</code>), and a corresponding pipeline created. Patches and minor release will be cherry picked to the release branch. The developer configuring a Fabric Item repo can then pick a specific Major release version of the pipeline for their repo, and would benefit for Minor and Patch updates. They could then upgrade to a new Major release when convenient.</p> <p>The pipeline will assume Git Flow is being used, whereby branches in the Fabric Items repos maps to a deployment environment: </p> <ul> <li><code>dev</code> -&gt; <code>dev</code></li> <li><code>release</code> -&gt; <code>uat</code></li> <li><code>main</code> -&gt; <code>prod</code></li> </ul> <p></p>"},{"location":"posts/one-pipeline/#build-validation","title":"Build Validation","text":"<p>Instead of each Power BI project repo having its own pipeline definition. A build validation policy is create for each branch that maps to a deployment environment (<code>dev</code>, <code>release</code> and <code>main</code>).</p>"},{"location":"posts/one-pipeline/#environments","title":"Environments","text":"<p>With a build validation policy, the pipeline will be trigger when a merge request is created, where the target branch has the policy defined. This is fine for <code>dev</code> and <code>uat</code>, but we wouldn't want to automatically deploy into <code>prod</code>. A Environment will be defined for <code>prod</code> so that a pre-approval check can be set, to ensure approval is granted prior to pipeline execution. Additionally, having environment defined allows for parameter overrides, using <code>parameter.yml</code> in fabric-cicd for example.</p>"},{"location":"posts/one-pipeline/#one-pipeline-to-bring-them-all-the-azure-devops-orchestration","title":"One Pipeline to Bring Them All: The Azure DevOps Orchestration","text":"<p>Now, let's craft the central <code>shared-pipeline.yml</code> (residing in the dedicated <code>shared-fabric-pipeline</code> repository). This single YAML file will manage deployments for all the fabric repos.</p>"},{"location":"posts/one-pipeline/#fabric-items-repos","title":"Fabric Items Repos","text":"<p>Firstly lets have a look at the structure of one of the <code>Fabric Items repos</code>. Note this only defines Fabric items, no pipeline. There is expected to be many of these.</p> StructuredeploymentManifest.json <pre><code>\u251c\u2500\u2500 \ud83d\udcc1 fabricItems\n\u2502    \u251c\u2500\u2500 \ud83d\udcc1 foo.SemanticModel\n\u2502    \u251c\u2500\u2500 \ud83d\udcc1 foo.Report\n\u2502    \u2514\u2500\u2500 \ud83d\udcc4 parameter.yml           # *optional* find and replace text in files\n\u2514\u2500\u2500 \ud83d\udcc4 deploymentManifest.json\n\u2514\u2500\u2500 \ud83d\udcc4 .gitignore\n</code></pre> <p>Defines which workspaces the Fabric items will be deployed, depending on the environment.</p> <pre><code>{\n  \"repo\": {\n    \"environments\": {\n      \"dev\": {\"workspaceId\": \"000-000-0000-0001\"},\n      \"uat\": {\"workspaceId\": \"000-000-0000-0002\"},\n      \"prod\": {\"workspaceId\": \"000-000-0000-0003\"}\n    }\n  }\n}\n</code></pre>"},{"location":"posts/one-pipeline/#shared-fabric-pipeline","title":"shared-fabric-pipeline","text":"<p>Then we can setup the <code>shared-fabric-pipeline</code>.</p> Structurefabric-cicd.pyshared-fabric-pipeline.yml <pre><code>\u251c\u2500\u2500 \ud83d\udcc4 fabric-cicd.py\n\u2514\u2500\u2500 \ud83d\udcc4 shared-fabric-pipeline.yml\n</code></pre> <pre><code>from fabric_cicd import FabricWorkspace, publish_all_items\nimport argparse\nimport json\n\nparser = argparse.ArgumentParser(description='Process some variables')\nparser.add_argument('--Environment', type=str)\nparser.add_argument('--RepositoryDirectory', type=str)\nparser.add_argument('--ItemsInScope', type=str)\nargs = parser.parse_args()\n\nallItems = args.ItemsInScope\nitem_type_in_scope = allItems.split(\",\")\n\nwith open(f\"{args.RepositoryDirectory}/deploymentManifest.json\") as json_data:\n  manifest = json.load(json_data)\nworkspace_id = manifest['repo']['environment'][args.Environment]['workspaceID']\n\ntarget_workspace = FabricWorkspace(\n  workspace_id = workspace_id,\n  environment = args.Environment,\n  repository_directory = f\"{args.RepositoryDirectory}/fabricItems\",\n  item_type_in_scope = items_type_in_scope\n)\n\npublish_all_items(target_workspace)\n</code></pre> <pre><code>parameters:\n  - name: ITEMS_IN_SCOPE\n    type: string\n    default: 'Report,SemanticModel'\n  - name: POOL_NAME\n    type: string\n    default: '***'\n  - name: SVC_CONNECTION\n    type: string\n    default: '***'\n  - name: KV\n    type: string\n    default: '***'\n  - name: SECRETS_FILTER\n    type: string\n    default: '***'\n  - name: TENANT_ID\n    type: string\n    default: '***'\n\nvariables:\n  # This variable will hold the determined environment (e.g., 'prod', 'uat', 'dev')\n  Environment: ''\n  - ${{ if eq(variables['System.PullRequest.TargetBranchName'], 'main') }}:\n    Environment: 'prod'\n  - ${{ if eq(variables['System.PullRequest.TargetBranchName'], 'release') }}:\n    Environment: 'uat'\n  - ${{ if eq(variables['System.PullRequest.TargetBranchName'], 'dev') }}:\n    Environment: 'dev'\n  # Fallback for unexpected target branches, or if not a PR build\n  - ${{ if not(or(\n          eq(variables['System.PullRequest.TargetBranchName'], 'main'),\n          eq(variables['System.PullRequest.TargetBranchName'], 'release'),\n          eq(variables['System.PullRequest.TargetBranchName'], 'dev'))\n        ) }}:\n    targetEnvironmentName: 'dev'\n\nstages:\n- stage: Deploy\n  displayName: 'Deploy Fabric Items'\n  pool:\n    vmImage: 'windows-latest'\n\n  jobs:\n  - deployment: DeployFabricItems\n    displayName: 'Deploy to ${{ variables.targetEnvironmentName }}'\n    environment: ${{ variables.Environment }}\n\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - checkout: self\n            path: \"s/ToDeploy\"\n          - checkout: shared-fabric-pipeline\n            path: \"s/shared-fabric-pipeline\"\n\n          - task: UsePythonVersion@0\n            displayName: 'Use Python 3.11'\n            inputs:\n              versionSpec: '3.11'\n\n          - task: PowerShell@2\n            displayName: 'Install fabric-cicd'\n            inputs:\n              targetType: 'inline'\n              script: |\n                python -m pip install --upgrade pip\n                pip install fabric-cicd\n\n          - task: AzureKeyVault@1\n            displayName: 'Get Service Principal Credentials'\n            inputs:\n              azureSubscription: $(SVC_CONNECTION)\n              keyVaultName: $(KV)\n              secretsFilter: $(SECRETS_FILTER)\n\n          - task: PowerShell@2\n            displayName: 'Authenticate as Service Principal'\n            inputs:\n              targetType: 'inline'\n              script: |\n                Install-Module -Name Az.Accounts -AllowClobber -Force\n                $SecureStringPwd = ConvertTo-SecureString $env:ARMCLIENTSECRET -AsPlainText -Force\n                $pscredential = New-Object -TypeName System.Management.Automation.PSCredential - ArgumentList $env:ARMCLIENTID, $SecureStringPwd\n                Connect-AzAccount -ServicePrincipal -Credential $pscredential -Tenant $env:TENANTID\n              env:\n                ARMCLIENTID: ${ARMCLIENTID}\n                ARMCLIENTSECRET: ${ARMCLIENTSECRET}\n                TENANTID: ${TENANT_ID}\n\n          - task: PythonScript@2\n            displayName: 'Deploy Fabric Items'\n            inputs:\n              scriptPath: $(Agent.BuildDirectory)/s/shared-fabric-pipeline/fabric-cicd.py\n              arguments: '--Environment ${{parameters.ENVIRONMENT}} --RepositoryDirectory \"$(Build.SourcesDirectory)/ToDeploy\" --ItemsInScope ${{parameters.ITEMS_IN_SCOPE}}\n</code></pre>"},{"location":"posts/one-pipeline/#pipeline-environment-and-prod-approval","title":"Pipeline, Environment and Prod Approval","text":"<p>Once this is all setup we then need to:</p> <ul> <li>Create environments for <code>dev</code>, <code>uat</code> and <code>prod</code></li> <li>Create approval in the <code>prod</code> environment</li> <li>Create the pipeline</li> <li>Create Build Validation on <code>Fabric Items Repos</code> for <code>dev</code>, <code>release</code> and <code>prod</code> branches, linking the shared pipeline</li> </ul>"},{"location":"posts/one-pipeline/#and-in-the-darkness-bind-them-why-this-method-rules-them-all","title":"And in the Darkness Bind Them: Why This Method Rules Them All","text":"<ul> <li>Ultimate Consolidation: Your deployment logic lives in one central pipeline, eliminating redundant YAML across dozens of repos.</li> <li>Effortless Scalability: Adding a new Fabric project simply involves setting Build Validation Policies on <code>dev</code>, <code>release</code> and <code>main</code> - no new pipelines needed!</li> <li>Targeted Deployments: Deployments are scoped to the items within a single repo, be that a workspace or a set of related Fabric Items, saving build minutes and minimizing deployment risk.</li> <li>Robust Auditing: Injecting commit SHA and timestamp directly into the semantic model provides clear, in-model lineage, invaluable for troubleshooting and governance.</li> </ul> <p>By embracing this consolidated pipeline approach, you transform your Fabric deployment strategy from a chaotic free-for-all into a streamlined, automated, and highly auditable process.</p>"},{"location":"posts/KQLGraph/","title":"Smarter Subgraphs - Kusto as a Engine For Power BI","text":"<p>In a couple of previous posts, I detailed how to get permission data into a graph structure with Graphframes and visualize it in Power BI using a force-directed Deneb visual. This approach is great for understanding permission inheritance from the perspective of Fabric Items, but it has a significant drawback when you try to view the graph in reverse, from the User's perspective.</p> <p>This post introduces a better way. By swapping the VertiPaq engine for a Fabric Kusto database, we can leverage its native graph processing capabilities to solve the subgraph filtering problem.</p>"},{"location":"posts/KQLGraph/#the-problem-why-vertipaq-falls-short","title":"The Problem: Why VertiPaq Falls Short","text":"<p>My original approach has a limitation! Filtering to sub-graph relies on pre-baked grouping of paths, represented by a field with item-permission grouping. This works perfectly when you filter by a permission or item.</p> <p>But what if you want to filter by a user and see all the items they can access? Because the VertiPaq engine lacks native recursion, it can't traverse the graph \"backwards\" or discover multi-step paths on the fly. The result is a cluttered visual, polluted with irrelevant user groups that don't directly answer the user's question. You can see below, if I filter to a specific user, in the old solution (top), it is littered with irrelevant groups and users, what we want in the bottom visual, providing a clean and clear answer.</p> <p></p> <p>You could try to solve this by calculating even more pre-baked paths, but that just adds complexity and bloats the model. </p> <p>But there has to be a better way... maybe another engine... in Fabric... with built in graph processing... with motif-matching.... Kusto Database?!</p>"},{"location":"posts/KQLGraph/#the-solution-the-right-engine-for-the-job","title":"The Solution: The Right Engine for the Job","text":"<p>Instead of importing data into VertiPaq, we can use a Kusto Database in Direct Query mode. This approach lets us leverage a engine designed for graph processing.</p> <p>By connecting Power BI to Kusto, we can use dynamic M parameters to pass user selections from our report directly into a KQL query. This query filters the graph on the server side and sends only the small, relevant subgraph back to Power BI for visualization.</p>"},{"location":"posts/KQLGraph/#kustos-graph-semantics-in-action","title":"Kusto's Graph Semantics in Action","text":"<p>Kusto is the blazing-fast analytics engine behind Azure Data Explore and Fabric's Real-Time Intelligence. Crucially for us, it offers built-in graph semantics, including operators for creating and matching patterns within a graph.</p> <p>First, I loaded the same triplet-form permission data from my previous post into a Kusto database (see below). </p> accessToItemGroupId srcId srcType srcName dstId dstType dstName ATG-WKS-5b6962ef-Contributor WKS-5b6962ef Workspace USS-Odyssey UG-D5CE28 Group Medical Officer ATG-WKS-5b6962ef-Contributor WKS-5b6962ef Workspace USS-Odyssey UG-2C73C1 Group Commander ATG-WKS-5b6962ef-Admin WKS-5b6962ef Workspace USS-Odyssey UG-5A5753 Group Security Officer ATG-WKS-5b6962ef-Viewer WKS-5b6962ef Workspace USS-Odyssey SF-4E1026 User Aaron Gonzalez ATG-WKS-5b6962ef-Viewer WKS-5b6962ef Workspace USS-Odyssey SF-6FD6FF User Kylie Coleman ATG-WKS-5b6962ef-Member WKS-5b6962ef Workspace USS-Odyssey SF-6D835A User Tyler Miles ATG-WKS-d6a39ae5-Viewer WKS-d6a39ae5 Workspace Starbase-Intrepid UG-756300 Group Engineering Officer ATG-WKS-d6a39ae5-Admin WKS-d6a39ae5 Workspace Starbase-Intrepid SF-500B65 User Robin Jones ATG-WKS-a8083ba1-Contributor WKS-a8083ba1 Workspace USS-Odyssey UG-5A5753 Group Security Officer ATG-WKS-a8083ba1-Viewer WKS-a8083ba1 Workspace USS-Odyssey SF-FEE711 User Amanda Reynolds ... ... ... ... ... ... ... <p>Then, I crafted a single KQL query to do all the work.</p> <p>The core of this query is the <code>make-graph</code> operator, which builds a graph in memory, and the <code>graph-match</code> operator, which performs motif matching. Motif matching is a powerful technique for finding specific structural patterns. For example, finding all paths that start at a specific item, pass through a user group, and end at a particular user.</p> <p>My KQL query is designed to find all paths that satisfy these criteria:</p> <ul> <li>Starts with a node from a user-selected list (<code>_StartingNodes</code>)</li> <li>Ends with a node from another user-selected list (<code>_EndingNodes</code>)</li> <li>Follows a path between 1 and 5 steps long</li> <li>Obeys the direction of the relationship (e.g., Item -&gt; Group -&gt; User)</li> </ul> <p>This query takes the user's input and returns a perfectly trimmed table containing only the relevant edges for the visualization, in the triplet form the Deneb visual expects.</p> CodeOutput <pre><code>// Parameters passed via dynamic M parameter\nlet _StartingNodes = dynamic(['Excelsior ZetaExcelsior 1']);\nlet _EndingNodes = dynamic(['Vincent Bates\"']);\nlet _Permissions = dynamic(['Read', 'ReadWrite']);\n// Triplet to Vertices and Edges\nlet data = permissions | extend permission = tostring(split(accessToItemGroupId, '-')[-1]);\nlet nodes = \n    union\n        (data | distinct srcId, srcType, srcName | project Id = srcId, Type = srcType, Name = srcName),\n        (data | distinct dstId, dstType, dstName | project Id = dstId, Type = dstType, Name = dstName)\n    | distinct Id, Type, Name;\n// Filter edges by Permission    \nlet edges = data | distinct source = srcId, destination = dstId, permission\n| where isempty(_Permissions) or permission in (_Permissions);\n// Generate Graph\nedges\n| make-graph source --&gt; destination with nodes on Id\n// Motif-matching and filtering by Starting and Ending Node\n| graph-match (start_node)-[path_edge*1..5]-&gt;(end_node)\nwhere\n    (isempty(_StartingNodes) or start_node.Name in (_StartingNodes))\n    and (isempty(_EndingNodes) or end_node.Name in (_EndingNodes))\nproject\n    srcId = start_node.Id,\n    srcName = start_node.Name,\n    srcType = start_node.Type,\n    dstId = end_node.Id,\n    dstName = end_node.Name,\n    dstType = end_node.Type,\n    Path = path_edge\n// Unpack nested structure to individual rows\n| mv-expand Path\n// Edges to triplet\n| project\n    srcId = tostring(Path.source),\n    dstId = tostring(Path.destination),\n    permission = tostring(Path.permission)\n| distinct \n    srcId,\n    dstId,\n    permission\n| join kind=inner nodes on\n    $left.srcId == $right.Id\n| project \n    srcId, \n    srcType = Type, \n    srcName = Name, \n    dstId,\n    permission\n| join kind=inner nodes on\n    $left.dstId == $right.Id\n| project \n    srcId,\n    srcName,\n    srcType, \n    dstId, \n    dstType = Type, \n    dstName = Name,\n    permission\n</code></pre> srcId srcName srcType dstId dstType dstName permission REP-d3dbd2d3 Excelsior ZetaExcelsior 1 Report UG-2C73C1 Group Commander ReadWrite REP-d3dbd2d3 Excelsior ZetaExcelsior 1 Report SF-D1D5EF User Vincent Bates Read UG-2C73C1 Commander Group SF-D1D5EF User Vincent Bates Read UG-2C73C1 Commander Group SF-D1D5EF User Vincent Bates ReadWrite"},{"location":"posts/KQLGraph/#wiring-it-up-in-power-bi","title":"Wiring It Up in Power BI","text":"<p>With the KQL query ready, the setup in Power BI is straightforward. I only required minimal changes to the Semantic Model to make this all work.</p> <p></p> <p>Here's the process:</p> <ul> <li>Create M Parameters: In Power Query, create three text parameters: <code>_StartingNodes</code>, <code>_EndingNodes</code>, and <code>_Permissions</code></li> <li>Create Parameter Tables: Create three separate tables (<code>DP_items</code>, <code>DP_Permissions</code>, <code>DP_Users</code>). These will populate our slicers. I found that I needed to create separate table for the dynamic M parameter, otherwise I got some errors.</li> <li>Set Up the Direct Query Source: Create a new query using the AzureDataExplorer.Contents connector, using the KQL script from above.</li> <li>Bind Parameters: In the Power BI data model view, select each disconnected table and bind its data column to the corresponding M parameter you created in step 1</li> </ul> <p>To setup the power query code for dynamic M parameter I followed the helpful guide from Chris Webb on arbitrary dynamic M parameters  and the MS Docs on the feature. Of note, some processing is required to convert lists from Power BI to strings that can be incorporated into the query.</p> <pre><code>let\n    StartingNodes = \n        if Type.Is(Value.Type(_StartingNodes), List.Type)\n        then Text.Combine({\"'\", Text.Combine(_StartingNodes, \"','\"), \"'\"})\n        else Text.Combine({\"'\", _StartingNodes, \"'\"}),\n    endingNodes = \n        if Type.Is(Value.Type(_EndingNodes), List.Type)\n        then Text.Combine({\"'\", Text.Combine(_EndingNodes, \"','\"), \"'\"})\n        else Text.Combine({\"'\", _EndingNodes, \"'\"}),\n    permissions =\n        if Type.Is(Value.Type(_Permissions), List.Type)\n        then Text.Combine({\"'\", Text.Combine(_Permissions, \"','\"), \"'\"})\n        else Text.Combine({\"'\", _Permissions, \"'\"}),\n    Source = AzureDataExplorer.Contents(\n        \"https://trd-y4vwg29chydm6jmuga.z0.kusto.fabric.microsoft.com\", \n        \"Graph\", \n        \"let _StartingNodes = dynamic([\" &amp; StartingNodes &amp; \"]); //dynamic(['Excelsior ZetaExcelsior 1']);\n        let _EndingNodes = dynamic([\" &amp; endingNodes &amp; \"]); //dynamic(['Vincent Bates']);\n        let _Permissions = dynamic([\" &amp; permissions &amp; \"]); //dynamic(['Read', 'ReadWrite']);\n        let data = permissions | extend permission = tostring(split(accessToItemGroupId, '-')[-1]);\n        let nodes = \n            union\n                (data | distinct srcId, srcType, srcName | project Id = srcId, Type = srcType, Name = srcName),\n                (data | distinct dstId, dstType, dstName | project Id = dstId, Type = dstType, Name = dstName)\n            | distinct Id, Type, Name;\n        let edges = data | distinct source = srcId, destination = dstId, permission\n        | where isempty(_Permissions) or permission in (_Permissions);\n        edges\n        | make-graph source --&gt; destination with nodes on Id\n        | graph-match (start_node)-[path_edge*1..5]-&gt;(end_node)\n        where\n            (isempty(_StartingNodes) or start_node.Name in (_StartingNodes))\n            and (isempty(_EndingNodes) or end_node.Name in (_EndingNodes))\n        project\n            srcId = start_node.Id,\n            srcName = start_node.Name,\n            srcType = start_node.Type,\n            dstId = end_node.Id,\n            dstName = end_node.Name,\n            dstType = end_node.Type,\n            Path = path_edge\n        | mv-expand Path\n        | project\n            srcId = tostring(Path.source),\n            dstId = tostring(Path.destination),\n            permission = tostring(Path.permission)\n        | distinct \n            srcId,\n            dstId,\n            permission\n        | join kind=inner nodes on\n            $left.srcId == $right.Id\n        | project \n            srcId, \n            srcType = Type, \n            srcName = Name, \n            dstId,\n            permission\n        | join kind=inner nodes on\n            $left.dstId == $right.Id\n        | project \n            srcId,\n            srcName,\n            srcType, \n            dstId, \n            dstType = Type, \n            dstName = Name,\n            permission \n        \", \n        [MaxRows=null, MaxSize=null, NoTruncate=null, AdditionalSetStatements=null]\n    )\nin\n    Source\n</code></pre> <p>The parameters can easily to bound to fields in model view.</p> <p></p> <p>That's it! No changes are needed to the Deneb visual specification. We just swap the fields from the old import-mode table with the fields from our new <code>Permission Triplets (DQ)</code> table. The result is a clean, focused graph showing only the nodes and edges that matter.</p> <p></p>"},{"location":"posts/KQLGraph/#conclusion","title":"Conclusion","text":"<p>By offloading the graph traversal logic to Kusto, we achieve three key benefits:</p> <ul> <li>Performance: Kusto is purpose-built for this kind of query and returns results incredibly quickly</li> <li>Scalability: The Power BI model remains lightweight and responsive, as the full graph dataset is never imported</li> <li>Clarity: Users get immediate, uncluttered answers to their questions, free from the visual noise of the old approach</li> </ul> <p>While this solution is powerful, the graph is transient, and is rebuilt in memory for each query. In the future, it would be interesting to explore a persistent graph solution using a dedicated graph database, like Cosmos DB, which was recently announced to be coming to Fabric. But for now, using Kusto as a graph engine is a massive leap forward for interactive graph analysis in Power BI.</p>"},{"location":"posts/sudoku/","title":"Translytical Sudoku","text":"<p>Microsoft's recent announcement of Translytical Task Flows for Power BI has opened up a world of practical applications. But for me, the first thought was how can I use this to do something impractical and build a game? I previously inspired by Phil Seamark impressive collection Power BI games, especially his innovative Sudoku implementation that maintained game state using slicers! With the full power of Microsoft Fabric and a backend database, we can now truly manage game state, making the game simpler to build and play, whilst also making it more feature rich.</p>"},{"location":"posts/sudoku/#understanding-translytical-task-flows","title":"Understanding Translytical Task Flows","text":"<p>Translytical Task Flows, while still in preview, are a game-changer for Power BI users. To enable them, you'll need to go to Options &gt; Preview features &gt; Translytical Task Flows in Power BI Desktop.</p> <p>At their core, they allow buttons within your Power BI reports to trigger Fabric User Data Functions (UDF). UDFs are python function that can perform an action, such as initiate changes in a Fabric SQL database. Within the button definition you map a button, list, or text slicer; a data field; or measures from the report to the input parameters of the UDF.</p>"},{"location":"posts/sudoku/#design-considerations","title":"Design Considerations","text":"<p>Sudoku is a logic puzzle played on a 9x9 grid. Your goal is to fill the grid with digits from 1 to 9, ensuring that each column, each row, and each of the nine 3x3 subgrids contains all the digits from 1 to 9.</p> <p></p> <p>Our approach leverages a Fabric SQL database to store the game's initial state and, crucially, the player's progress. Players will interact with the Power BI report by selecting a cell on the grid and a candidate value. Pressing a button will then activate a UDF, which in turn updates the database.</p> <p>A key feature for serious Sudoku players is pencil marks. These small numbers in the corners of a cell represent all possible valid digits for that cell. Our game will support this by allowing players to toggle pencil marks using a dedicated button.</p> <p>Considering that multiple users might access the report, we'll implement Row Level Security (RLS) using <code>USERPRINCIPALNAME()</code> to ensure each player can only access their own games.</p> <p>Here's an architectural overview of how all these components fit together:</p> <p></p>"},{"location":"posts/sudoku/#creating-the-sql-database","title":"Creating The SQL Database","text":"<p>The first step is to set up our Fabric SQL database with the necessary tables to manage game data:</p> <ul> <li>PuzzleTemplates: Stores predefined Sudoku puzzles</li> <li>PuzzleInitialState: Records the starting values for each puzzle template</li> <li>Games: Tracks individual game instances initiated by players</li> <li>GameState: Holds the current values entered by players for a specific game</li> <li>PencilMarks: Manages the pencil marks added by players</li> </ul> <pre><code>CREATE TABLE PuzzleTemplates (\n    PuzzleId INT PRIMARY KEY IDENTITY(1,1),\n    PuzzleName NVARCHAR(100),\n    Difficulty NVARCHAR(20),\n    CreatedDate DATETIME2 DEFAULT GETDATE()\n);\n\nCREATE TABLE PuzzleInitialState (\n    PuzzleId INT,\n    Position INT CHECK (Position BETWEEN 1 AND 81),\n    InitialValue INT CHECK (InitialValue BETWEEN 1 AND 9),\n    PRIMARY KEY (PuzzleId, Position),\n    FOREIGN KEY (PuzzleId) REFERENCES PuzzleTemplates(PuzzleId)\n);\n\nCREATE TABLE Games (\n    GameId INT PRIMARY KEY IDENTITY(1,1),\n    PuzzleId INT,\n    GameName NVARCHAR(100),\n    CreatedDate DATETIME2 DEFAULT GETDATE(),\n    LastModifiedDate DATETIME2 DEFAULT GETDATE(),\n    FOREIGN KEY (PuzzleId) REFERENCES PuzzleTemplates(PuzzleId)\n);\n\nCREATE TABLE GameState (\n    GameId INT,\n    Position INT CHECK (Position BETWEEN 1 AND 81),\n    CurrentValue INT CHECK (CurrentValue BETWEEN 1 AND 9),\n    IsInitialValue BIT DEFAULT 0,\n    LastModifiedDate DATETIME2 DEFAULT GETDATE(),\n    PRIMARY KEY (GameId, Position),\n    FOREIGN KEY (GameId) REFERENCES Games(GameId)\n);\n\nCREATE TABLE PencilMarks (\n    GameId INT,\n    Position INT CHECK (Position BETWEEN 1 AND 81),\n    PencilValue INT CHECK (PencilValue BETWEEN 1 AND 9),\n    IsActive BIT DEFAULT 1,\n    CreatedDate DATETIME2 DEFAULT GETDATE(),\n    LastModifiedDate DATETIME2 DEFAULT GETDATE(),\n    PRIMARY KEY (GameId, Position, PencilValue),\n    FOREIGN KEY (GameId) REFERENCES Games(GameId)\n);\n</code></pre> <p>To efficiently populate our Sudoku grid in Power BI via Direct Query, we'll create a view that combines all relevant game state information:</p> <pre><code>CREATE VIEW vw_GameState AS\nSELECT\n    g.GameId,\n    g.GameName,\n    g.PuzzleId,\n    coord.Position,\n    gs.CurrentValue,\n    gs.IsInitialValue,\n    STRING_AGG(CAST(pm.PencilValue AS VARCHAR), '') AS PencilMarks\nFROM Games g\nCROSS JOIN (\n    -- Generate all possible positions (1-81)\n    SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS Position\n    FROM (VALUES (1),(2),(3),(4),(5),(6),(7),(8),(9)) X(n)\n    CROSS JOIN (VALUES (1),(2),(3),(4),(5),(6),(7),(8),(9)) Y(n)\n) coord\nLEFT JOIN GameState gs ON g.GameId = gs.GameId \n    AND coord.Position = gs.Position\nLEFT JOIN PencilMarks pm ON g.GameId = pm.GameId \n    AND coord.Position = pm.Position\n    AND pm.IsActive = 1\nGROUP BY g.GameId, g.GameName, g.PuzzleId,\n         coord.Position, gs.CurrentValue, gs.IsInitialValue;\n</code></pre> <p>We'll use stored procedures to encapsulate the core game actions, making them callable from our UDFs:</p> <ul> <li><code>sp_InitializeGame</code>: Creates a new instance of a puzzle for a player</li> <li><code>sp_SetCellValue</code>: Adds or removes a selected digit from a selected cell</li> <li><code>sp_TogglePencilMark</code> Adds or removes a pencil mark for a selected cell</li> <li><code>sp_ClearUserEntries</code> Resets all user-entered digits and pencil marks for the current game</li> </ul> <pre><code>CREATE PROCEDURE sp_InitializeGame\n    @GameName NVARCHAR(100),\n    @PuzzleId INT\nAS\nBEGIN\n    DECLARE @GameId INT;\n\n    -- Check if game with same name and puzzle already exists\n    IF EXISTS (SELECT 1 FROM Games WHERE GameName = @GameName AND PuzzleId = @PuzzleId)\n    BEGIN\n        RAISERROR('A game with this name already exists for this puzzle.', 16, 1);\n        RETURN;\n    END;\n\n    -- Create new game\n    INSERT INTO Games (GameName, PuzzleId)\n    VALUES (@GameName, @PuzzleId);\n\n    SET @GameId = SCOPE_IDENTITY();\n\n    -- Copy initial state from puzzle template\n    INSERT INTO GameState (GameId, Position, CurrentValue, IsInitialValue)\n    SELECT @GameId, pis.Position, pis.InitialValue, 1\n    FROM PuzzleInitialState pis\n    WHERE pis.PuzzleId = @PuzzleId;\n\n    SELECT @GameId AS GameId;\nEND;\nGO\n\nCREATE PROCEDURE sp_SetCellValue\n    @GameId INT,\n    @Position INT,\n    @Value INT\nAS\nBEGIN\n    -- Check if cell is an initial value (cannot be changed)\n    IF EXISTS (SELECT 1 FROM GameState \n               WHERE GameId = @GameId AND Position = @Position AND IsInitialValue = 1)\n    BEGIN\n        RAISERROR('Cannot modify initial puzzle values.', 16, 1);\n        RETURN;\n    END;\n\n    -- Check if trying to set the same value that's already there (toggle off)\n    IF EXISTS (SELECT 1 FROM GameState \n               WHERE GameId = @GameId AND Position = @Position \n               AND CurrentValue = @Value AND IsInitialValue = 0)\n    BEGIN\n        -- Remove the value (toggle off)\n        DELETE FROM GameState \n        WHERE GameId = @GameId AND Position = @Position;\n\n        -- Clear pencil marks for this cell\n        DELETE FROM PencilMarks \n        WHERE GameId = @GameId AND Position = @Position;\n\n        RETURN;\n    END;\n\n    -- Insert or update the cell value\n    MERGE GameState AS target\n    USING (VALUES (@GameId, @Position, @Value)) AS source (GameId, Position, CurrentValue)\n    ON target.GameId = source.GameId \n       AND target.Position = source.Position\n    WHEN MATCHED THEN\n        UPDATE SET CurrentValue = source.CurrentValue, LastModifiedDate = GETDATE()\n    WHEN NOT MATCHED THEN\n        INSERT (GameId, Position, CurrentValue, IsInitialValue)\n        VALUES (source.GameId, source.Position, source.CurrentValue, 0);\n\n    -- Clear pencil marks for this cell\n    DELETE FROM PencilMarks \n    WHERE GameId = @GameId AND Position = @Position;\nEND;\nGO\n\nCREATE PROCEDURE sp_TogglePencilMark\n    @GameId INT,\n    @Position INT,\n    @PencilValue INT\nAS\nBEGIN\n    -- Only allow pencil marks if cell is empty\n    IF EXISTS (SELECT 1 FROM GameState \n               WHERE GameId = @GameId AND Position = @Position AND CurrentValue IS NOT NULL)\n    BEGIN\n        RAISERROR('Cannot add pencil marks to filled cells.', 16, 1);\n        RETURN;\n    END;\n\n    -- Toggle the pencil mark\n    IF EXISTS (SELECT 1 FROM PencilMarks \n               WHERE GameId = @GameId AND Position = @Position \n               AND PencilValue = @PencilValue AND IsActive = 1)\n    BEGIN\n        -- Remove the pencil mark\n        UPDATE PencilMarks \n        SET IsActive = 0, LastModifiedDate = GETDATE()\n        WHERE GameId = @GameId AND Position = @Position \n              AND PencilValue = @PencilValue;\n    END\n    ELSE\n    BEGIN\n        -- Add or reactivate the pencil mark\n        MERGE PencilMarks AS target\n        USING (VALUES (@GameId, @Position, @PencilValue)) AS source (GameId, Position, PencilValue)\n        ON target.GameId = source.GameId \n           AND target.Position = source.Position\n           AND target.PencilValue = source.PencilValue\n        WHEN MATCHED THEN\n            UPDATE SET IsActive = 1, LastModifiedDate = GETDATE()\n        WHEN NOT MATCHED THEN\n            INSERT (GameId, Position, PencilValue, IsActive)\n            VALUES (source.GameId, source.Position, source.PencilValue, 1);\n    END;\nEND;\nGO\n\nCREATE PROCEDURE sp_ClearUserEntries\n    @GameId INT\nAS\nBEGIN\n    -- Delete all user-entered values (keep only initial values)\n    DELETE FROM GameState \n    WHERE GameId = @GameId AND IsInitialValue = 0;\n\n    -- Clear all pencil marks\n    DELETE FROM PencilMarks \n    WHERE GameId = @GameId;\n\n    -- Update game's last modified date\n    UPDATE Games \n    SET LastModifiedDate = GETDATE()\n    WHERE GameId = @GameId;\nEND;\nGO\n</code></pre> <p>Finally, let's add some sample Sudoku puzzles to our <code>PuzzleTemplates</code> and <code>PuzzleInitialState</code> tables:</p> <pre><code>INSERT INTO PuzzleTemplates (PuzzleName, Difficulty) VALUES \n('Sample Easy Puzzle', 'Easy'),\n('Sample Medium Puzzle', 'Medium'),\n('Sample Hard Puzzle', 'Hard'),\n('Sample Expert Puzzle', 'Expert');\n\n-- EASY PUZZLE (PuzzleId = 1) - converted to positions\n-- Position mapping: Position = (Row-1) * 9 + Column\n-- Row 1: positions 1-9, Row 2: positions 10-18, etc.\nINSERT INTO PuzzleInitialState (PuzzleId, Position, InitialValue) VALUES\n-- Row 1: 5 3 . | . 7 . | . . .\n(1, 1, 5), (1, 2, 3), (1, 5, 7),\n-- Row 2: 6 . . | 1 9 5 | . . .\n(1, 10, 6), (1, 13, 1), (1, 14, 9), (1, 15, 5),\n-- Row 3: . 9 8 | . . . | . 6 .\n(1, 20, 9), (1, 21, 8), (1, 26, 6),\n-- Row 4: 8 . . | . 6 . | . . 3\n(1, 28, 8), (1, 32, 6), (1, 36, 3),\n-- Row 5: 4 . . | 8 . 3 | . . 1\n(1, 37, 4), (1, 40, 8), (1, 42, 3), (1, 45, 1),\n-- Row 6: 7 . . | . 2 . | . . 6\n(1, 46, 7), (1, 50, 2), (1, 54, 6),\n-- Row 7: . 6 . | . . . | 2 8 .\n(1, 56, 6), (1, 61, 2), (1, 62, 8),\n-- Row 8: . . . | 4 1 9 | . . 5\n(1, 67, 4), (1, 68, 1), (1, 69, 9), (1, 72, 5),\n-- Row 9: . . . | . 8 . | . 7 9\n(1, 77, 8), (1, 80, 7), (1, 81, 9);\n\n-- MEDIUM PUZZLE (PuzzleId = 2) - converted to positions\nINSERT INTO PuzzleInitialState (PuzzleId, Position, InitialValue) VALUES\n-- Row 1: . . . | 6 . . | . . 3\n(2, 4, 6), (2, 9, 3),\n-- Row 2: . 7 . | . 9 . | 2 . .\n(2, 11, 7), (2, 14, 9), (2, 16, 2),\n-- Row 3: 5 . . | . . . | . . .\n(2, 19, 5),\n-- Row 4: . . . | . . . | . 1 .\n(2, 35, 1),\n-- Row 5: . . 1 | . 8 . | 4 . .\n(2, 39, 1), (2, 41, 8), (2, 43, 4),\n-- Row 6: . 6 . | . . . | . . .\n(2, 47, 6),\n-- Row 7: . . . | . . . | . . 7\n(2, 63, 7),\n-- Row 8: . . 4 | . 2 . | . 9 .\n(2, 66, 4), (2, 68, 2), (2, 71, 9),\n-- Row 9: 9 . . | . . 8 | . . .\n(2, 73, 9), (2, 78, 8);\n\n-- HARD PUZZLE (PuzzleId = 3) - converted to positions\nINSERT INTO PuzzleInitialState (PuzzleId, Position, InitialValue) VALUES\n-- Row 1: . . . | . . . | . 1 .\n(3, 8, 1),\n-- Row 2: 4 . . | . . . | . . .\n(3, 10, 4),\n-- Row 3: . . . | . . . | 6 . 2\n(3, 25, 6), (3, 27, 2),\n-- Row 4: . . . | . . . | . 7 .\n(3, 35, 7),\n-- Row 5: . 8 . | . . . | . 4 .\n(3, 38, 8), (3, 44, 4),\n-- Row 6: . 3 . | . . . | . . .\n(3, 47, 3),\n-- Row 7: 7 . 9 | . . . | . . .\n(3, 55, 7), (3, 57, 9),\n-- Row 8: . . . | . . . | . . 5\n(3, 72, 5),\n-- Row 9: . 2 . | . . . | . . .\n(3, 74, 2);\n\n-- EXPERT PUZZLE (PuzzleId = 4) - converted to positions\nINSERT INTO PuzzleInitialState (PuzzleId, Position, InitialValue) VALUES\n-- Row 1: . . . | . . . | . . 9\n(4, 9, 9),\n-- Row 2: . . . | . . 3 | . 8 .\n(4, 15, 3), (4, 17, 8),\n-- Row 3: . . 1 | . 2 . | . . .\n(4, 21, 1), (4, 23, 2),\n-- Row 4: . . . | . . . | . . 2\n(4, 36, 2),\n-- Row 5: . . . | 2 . . | . . .\n(4, 40, 2),\n-- Row 6: 6 . . | . . . | . . .\n(4, 46, 6),\n-- Row 7: . . . | . 5 . | 7 . .\n(4, 59, 5), (4, 61, 7),\n-- Row 8: . 4 . | 9 . . | . . .\n(4, 65, 4), (4, 67, 9),\n-- Row 9: 1 . . | . . . | . . .\n(4, 73, 1);\n</code></pre>"},{"location":"posts/sudoku/#creating-the-user-data-functions","title":"Creating The User Data Functions","text":"<p>With our database set up, we now need to create the UDFs that will act as the bridge between the Power BI report and our SQL stored procedures.</p> <p>First, you'll need to set up a connection to your Fabric SQL server within your UDF environment:</p> <p></p> <p>Next, we define our Python UDFs, each calling a specific stored procedure:</p> <ul> <li><code>initialize_game</code>: Calls <code>sp_InitializeGame</code></li> <li><code>set_cell_value</code>: Calls <code>sp_SetCellValue</code></li> <li><code>toggle_pencil_mark</code>: Calls <code>sp_TogglePencilMark</code></li> <li><code>clear_user_entries</code>: Calls <code>sp_ClearUserEntries</code></li> </ul> <pre><code>import fabric.functions as fn\nimport json\nfrom datetime import datetime\n\nudf = fn.UserDataFunctions()\n\n@udf.connection(argName=\"sqlDB\", alias=\"Game\") \n@udf.function()\ndef initialize_game(sqlDB: fn.FabricSqlConnection, gameName: str, puzzleId: int) -&gt; str:\n    \"\"\"\n    Initialize a new Sudoku game from a puzzle template\n    Prevents duplicate game names for the same puzzle\n\n    Args:\n        gameName: Name for the new game\n        puzzleId: ID of the puzzle template to use\n\n    Returns:\n        Str with game initialization result\n    \"\"\"\n    try:\n        conn = sqlDB.connect()\n        cursor = conn.cursor()\n\n        # Check if game with same name and puzzle already exists\n        cursor.execute(\"\"\"\n            SELECT COUNT(*) FROM Games \n            WHERE GameName = ? AND PuzzleId = ?\n        \"\"\", (gameName, puzzleId))\n\n        if cursor.fetchone()[0] &gt; 0:\n            return json.dumps(\n                {\n                    \"status\": \"ERROR\",\n                    \"message\": \"A game with this name already exists for this puzzle. Please choose a different name.\",\n                    \"game_id\": None\n                }\n            )\n\n        # Execute the stored procedure\n        cursor.execute(\"EXEC sp_InitializeGame ?, ?\", (gameName, puzzleId))\n\n        # Commit the transaction\n        conn.commit()\n\n        # Get the new game ID\n        cursor.execute(\"\"\"\n            SELECT TOP 1 GameId, GameName, CreatedDate\n            FROM Games \n            WHERE GameName = ? AND PuzzleId = ?\n            ORDER BY CreatedDate DESC\n        \"\"\", (gameName, puzzleId))\n\n        game_details = cursor.fetchone()\n\n        if game_details:\n            return json.dumps(\n                {\n                    \"status\": \"SUCCESS\",\n                    \"message\": \"Game initialized successfully\",\n                    \"game_id\": game_details[0],\n                    \"gameName\": game_details[1],\n                    \"puzzleId\": puzzleId,\n                    \"created_date\": game_details[2].isoformat(),\n                }\n            )\n        else:\n            return json.dumps(\n                {\n                    \"status\": \"ERROR\",\n                    \"message\": \"Failed to initialize game - game not found after creation\",\n                    \"game_id\": None\n                }\n            )\n\n    except Exception as e:\n        return json.dumps(\n            {\n                \"status\": \"ERROR\",\n                \"message\": f\"Database error: {str(e)}\",\n                \"game_id\": None\n            }\n        )\n    finally:\n        cursor.close() \n        conn.close()\n\n@udf.connection(argName=\"sqlDB\", alias=\"Game\")\n@udf.function()\ndef set_cell_value(sqlDB: fn.FabricSqlConnection, gameId: int, position: int, value: int) -&gt; str:\n    \"\"\"\n    Set a value in a Sudoku cell - now supports toggling off user-entered values\n\n    Args:\n        gameId: The game ID\n        position: Cell position (1-81)\n        value: Value to set (1-9)\n\n    Returns:\n        Str with operation result\n    \"\"\"\n    try:\n        # Validate inputs\n        if not (1 &lt;= position &lt;= 81 and 1 &lt;= value &lt;= 9):\n            return json.dumps(\n                {\n                    \"status\": \"ERROR\",\n                    \"message\": \"Invalid position or value. Position must be 1-81, value must be 1-9.\",\n                    \"action_time\": datetime.now().isoformat()\n                }\n            )\n\n        conn = sqlDB.connect()\n        cursor = conn.cursor()\n\n        # Check current state of the cell\n        cursor.execute(\"\"\"\n            SELECT CurrentValue, IsInitialValue FROM GameState \n            WHERE GameId = ? AND Position = ?\n        \"\"\", (gameId, position))\n\n        result = cursor.fetchone()\n\n        # Determine the action that will be taken\n        action_type = \"set\"\n        if result:\n            if result[1] == 1:  # IsInitialValue = 1\n                return json.dumps(\n                    {\n                        \"status\": \"ERROR\",\n                        \"message\": \"Cannot modify initial puzzle values\",\n                        \"action_time\": datetime.now().isoformat()\n                    }\n                )\n            elif result[0] == value:  # Same value already there\n                action_type = \"toggled_off\"\n\n        # Execute the stored procedure\n        cursor.execute(\"EXEC sp_SetCellValue ?, ?, ?\", \n                      (gameId, position, value))\n\n        conn.commit()\n\n        # Check final state to confirm action\n        cursor.execute(\"\"\"\n            SELECT CurrentValue FROM GameState \n            WHERE GameId = ? AND Position = ?\n        \"\"\", (gameId, position))\n\n        final_result = cursor.fetchone()\n        final_value = final_result[0] if final_result else None\n\n        if action_type == \"toggled_off\":\n            message = f\"Value {value} toggled off at position {position}\"\n        else:\n            message = f\"Value {value} set at position {position}\"\n\n        return json.dumps(\n            {\n                \"status\": \"SUCCESS\",\n                \"message\": message,\n                \"game_id\": gameId,\n                \"position\": position,\n                \"requested_value\": value,\n                \"final_value\": final_value,\n                \"action_type\": action_type,\n                \"action_time\": datetime.now().isoformat()\n            }\n        )\n\n    except Exception as e:\n        return json.dumps(\n            {\n                \"status\": \"ERROR\",\n                \"message\": f\"Database error: {str(e)}\",\n                \"action_time\": datetime.now().isoformat()\n            }\n        )\n    finally:\n        cursor.close() \n        conn.close()\n\n@udf.connection(argName=\"sqlDB\", alias=\"Game\")\n@udf.function()\ndef toggle_pencil_mark(sqlDB: fn.FabricSqlConnection, gameId: int, position: int, pencilValue: int) -&gt; str:\n    \"\"\"\n    Toggle a pencil mark in a Sudoku cell\n\n    Args:\n        gameId: The game ID\n        position: Cell position (1-81)\n        pencilValue: Pencil mark value (1-9)\n\n    Returns:\n        Str with operation result\n    \"\"\"\n    try:\n        # Validate inputs\n        if not (1 &lt;= position &lt;= 81 and 1 &lt;= pencilValue &lt;= 9):\n            return json.dumps(\n                {\n                    \"status\": \"ERROR\",\n                    \"message\": \"Invalid position or pencil value. Position must be 1-81, value must be 1-9.\",\n                    \"action_time\": datetime.now().isoformat()\n                }\n            )\n\n        conn = sqlDB.connect()\n        cursor = conn.cursor()\n\n        # Check if cell has a value (cannot add pencil marks)\n        cursor.execute(\"\"\"\n            SELECT CurrentValue FROM GameState \n            WHERE GameId = ? AND Position = ?\n        \"\"\", (gameId, position))\n\n        result = cursor.fetchone()\n        if result and result[0] is not None:\n            return json.dumps(\n                {\n                    \"status\": \"ERROR\",\n                    \"message\": \"Cannot add pencil marks to filled cells\",\n                    \"action_time\": datetime.now().isoformat()\n                }\n            )\n\n        # Execute the stored procedure\n        cursor.execute(\"EXEC sp_TogglePencilMark ?, ?, ?\", \n                      (gameId, position, pencilValue))\n\n        conn.commit()\n\n        # Check if pencil mark was added or removed\n        cursor.execute(\"\"\"\n            SELECT IsActive FROM PencilMarks \n            WHERE GameId = ? AND Position = ? AND PencilValue = ?\n        \"\"\", (gameId, position, pencilValue))\n\n        pencil_result = cursor.fetchone()\n        is_active = pencil_result[0] if pencil_result else False\n        action_type = \"added\" if is_active else \"removed\"\n\n        return json.dumps(\n            {\n                \"status\": \"SUCCESS\",\n                \"message\": f\"Pencil mark {pencilValue} {action_type} at position {position}\",\n                \"game_id\": gameId,\n                \"position\": position,\n                \"pencil_value\": pencilValue,\n                \"is_active\": is_active,\n                \"action_type\": action_type,\n                \"action_time\": datetime.now().isoformat()\n            }\n        )\n\n    except Exception as e:\n        return json.dumps(\n            {\n                \"status\": \"ERROR\",\n                \"message\": f\"Database error: {str(e)}\",\n                \"action_time\": datetime.now().isoformat()\n            }\n        )\n    finally:\n        cursor.close() \n        conn.close()\n\n@udf.connection(argName=\"sqlDB\", alias=\"Game\")\n@udf.function()\ndef clear_user_entries(sqlDB: fn.FabricSqlConnection, gameId: int) -&gt; str:\n    \"\"\"\n    Clear all user-entered values from a Sudoku game, keeping only initial puzzle values\n\n    Args:\n        gameId: The game ID\n\n    Returns:\n        Str with operation result\n    \"\"\"\n    try:\n        conn = sqlDB.connect()\n        cursor = conn.cursor()\n\n        # Verify game exists\n        cursor.execute(\"SELECT GameName FROM Games WHERE GameId = ?\", (gameId,))\n        game_result = cursor.fetchone()\n\n        if not game_result:\n            return json.dumps(\n                {\n                    \"status\": \"ERROR\",\n                    \"message\": \"Game not found\",\n                    \"action_time\": datetime.now().isoformat()\n                }\n            )\n\n        # Count user entries before clearing\n        cursor.execute(\"\"\"\n            SELECT COUNT(*) FROM GameState \n            WHERE GameId = ? AND IsInitialValue = 0\n        \"\"\", (gameId,))\n\n        user_entries_count = cursor.fetchone()[0]\n\n        # Count pencil marks before clearing\n        cursor.execute(\"\"\"\n            SELECT COUNT(*) FROM PencilMarks \n            WHERE GameId = ? AND IsActive = 1\n        \"\"\", (gameId,))\n\n        pencil_marks_count = cursor.fetchone()[0]\n\n        # Execute the stored procedure\n        cursor.execute(\"EXEC sp_ClearUserEntries ?\", (gameId,))\n\n        conn.commit()\n\n        return json.dumps(\n            {\n                \"status\": \"SUCCESS\",\n                \"message\": f\"Cleared {user_entries_count} user entries and {pencil_marks_count} pencil marks\",\n                \"game_id\": gameId,\n                \"game_name\": game_result[0],\n                \"cleared_entries\": user_entries_count,\n                \"cleared_pencil_marks\": pencil_marks_count,\n                \"action_time\": datetime.now().isoformat()\n            }\n        )\n\n    except Exception as e:\n        return json.dumps(\n            {\n                \"status\": \"ERROR\",\n                \"message\": f\"Database error: {str(e)}\",\n                \"action_time\": datetime.now().isoformat()\n            }\n        )\n    finally:\n        cursor.close() \n        conn.close()\n</code></pre>"},{"location":"posts/sudoku/#creating-the-power-bi-report","title":"Creating The Power BI Report","text":"<p>Now, let's bring it all together in the Power BI report, following our architectural design.</p>"},{"location":"posts/sudoku/#report-background","title":"Report Background","text":"<p>We'll start by setting a background image that includes the Sudoku grid:</p> <p></p>"},{"location":"posts/sudoku/#puzzle-selector","title":"Puzzle Selector","text":"<p>To allow players to choose a puzzle, we'll use the <code>PuzzleTemplates</code> table and add the <code>PuzzleName</code> field to a slicer so that the player can select a template that they want to play.</p> <p></p> <p>We'll also need a DAX measure to pass the <code>PuzzleId</code> to the UDFs:</p> <pre><code>Selected PuzzleId = SELECTEDVALUE( PuzzleTemplates[PuzzleId] )\n</code></pre>"},{"location":"posts/sudoku/#number-selector","title":"Number Selector","text":"<p>Players need to select a digit (1-9) to enter into the grid. A simple DAX table and a button slicer will do the trick:</p> <pre><code>SelectValue = GENERATESERIES( 1, 9, 1 )\n</code></pre> <p></p>"},{"location":"posts/sudoku/#sudoku-grid","title":"Sudoku Grid","text":"<p>Instead of a standard Matrix visual, which has limitations in terms of visual customization, I opted for another button slicer, which has extensive formatting options and control. We'll create a DAX table with 81 positions, one for each cell:</p> <pre><code>Grid Position = \nvar tbl = GENERATESERIES( 1, 81, 1 )\nreturn\nSELECTCOLUMNS(\n    tbl,\n    \"Position\", [Value]\n)\n</code></pre> <p>These button slicers will then be aligned with the Sudoku grid image in our background.</p> <p>To display both the main cell values and the smaller pencil marks, we'll use an SVG measure. This allows for dynamic and highly customizable cell content.</p> <p>Measure: Data Category</p> <p>For SVG to render as a image instead of text the Data Category of the measure needs to be set to \"Image URL\"</p> <p></p> <pre><code>Cell = \nvar width = 40\nvar height = 40\nvar mainFontSize = 16\nvar pencilFontSize = 8\nvar isInitial = \n    IF(\n        CALCULATE(\n            MAXX( GameState, INT( GameState[IsInitialValue] ) )\n            ,TREATAS( VALUES( 'Grid Position'[Position] ), GameState[Position] )\n        ),\n        \" font-weight='bold'\"\n    )\nvar val = \n    CALCULATE(\n        MAX( GameState[CurrentValue] )\n        ,TREATAS( VALUES( 'Grid Position'[Position] ), GameState[Position] )\n    )\nvar pencil = \n    CALCULATE(\n        MAX( GameState[PencilMarks] )\n        ,TREATAS( VALUES( 'Grid Position'[Position] ), GameState[Position] )\n    )\nreturn\n\n\"data:image/svg+xml;charset=utf8,\n&lt;svg xmlns='http://www.w3.org/2000/svg' width='\"&amp; width &amp; \"' height='\"&amp; height &amp; \"'&gt;\n    &lt;text x='\"&amp; width / 2 &amp; \"' y='\"&amp; height /2 &amp; \"' font-size='\" &amp; mainFontSize &amp; \"' font-family='Segoe UI' dominant-baseline='middle' text-anchor='middle' fill='#333'\" &amp; isInitial &amp; \"&gt;\" &amp; val &amp; \"&lt;/text&gt;\n    &lt;text x='\"&amp; width / 2 &amp; \"' y='5' font-size='\" &amp; pencilFontSize &amp; \"' font-family='Segoe UI' dominant-baseline='middle' text-anchor='right' fill='#575555'&gt;\" &amp; pencil &amp; \"&lt;/text&gt;\n&lt;/svg&gt;\"\n</code></pre>"},{"location":"posts/sudoku/#buttons","title":"Buttons","text":"<p>Finally, we'll add four buttons, each linked to one of our UDFs. We'll also need a couple of additional DAX measures to pass the current user's name and selected game ID:</p> <pre><code>Selected GameName = USERPRINCIPALNAME()\n</code></pre> <pre><code>Selected GameId = SELECTEDVALUE( GameState[GameId] )\n</code></pre> <p>The crucial step is to map the appropriate slicer fields and measures to the input parameters of each UDF:</p> <p></p>"},{"location":"posts/sudoku/#row-level-security","title":"Row Level Security","text":"<p>To ensure players only interact with their own games, we'll apply RLS on the <code>GameState</code> table. This rule filters the data based on the logged-in user's principal name:</p> <pre><code>vw_GameState[GameName] = USERPRINCIPALNAME()\n</code></pre> <p>RLS</p> <p>For the role to apply to a player, after the report has been publish to service you need to go to the workspace and select ... &gt; security for the semantic model, and assign the player's to the role.</p> <p>Keep in mind that RLS doesn't apply to workspace Admins. While I won't be subject to it, other players will experience an isolated experience from other players.</p> <p></p>"},{"location":"posts/sudoku/#playing-the-game","title":"Playing the Game!","text":"<p>With everything configured and published to the Power BI service we can play the game.</p> <p></p>"},{"location":"posts/sudoku/#conclusion","title":"Conclusion","text":"<p>This project serves as a compelling proof-of-concept, demonstrating the power and flexibility of Translytical Task Flows in Power BI and Microsoft Fabric.Hopefully this lives up to being a spiritual successor to Phil's initial work. While I'm pleased with the current state, there's always room for improvement. Future enhancements could include:</p> <ul> <li>Highlighting cells with digits that match the currently selected cell</li> <li>Highlighting the row and column of the selected cell for better guidance</li> <li>Implementing a solver or hint feature</li> <li>Providing an indicator when a player enters an incorrect value</li> <li>A celebratory animation or message upon game completion</li> <li>Allowing players to enter pencil marks into multiple cells simultaneously</li> </ul>"},{"location":"posts/translytical-rows/","title":"Handling Multi-Row Inputs for Translytical Task Flows","text":"<p>In a previous post I explored using Translytical Task Flows to create Sudoku in Power BI. One potential improvement I mentioned was the ability for players to enter \"pencil marks\" into multiple cells simultaneously. This seemingly simple enhancement presents a technical challenge: how do we pass multiple row selections into a User-Defined Function (UDF) at the same time?</p> <p>This post dives into the solution, demonstrating how to leverage the seldomly used DAX function, <code>TOJSON()</code>, to achieve this multi-row input capability.</p>"},{"location":"posts/translytical-rows/#translytical-task-flows-in-power-bi","title":"Translytical Task Flows in Power BI","text":"<p>Before we delve into the solution, let's briefly revisit the basics of Translytical Task Flows in Power BI, as highlighted previously:</p> <p>Translytical Sudoku</p> <p>Translytical Task Flows, while still in preview, are a game-changer for Power BI users. To enable them, you'll need to go to Options &gt; Preview features &gt; Translytical Task Flows in Power BI Desktop.</p> <p>At their core, they allow buttons within your Power BI reports to trigger Fabric User Data Functions (UDF). UDFs are python function that can perform an action such as initiate changes in a Fabric SQL database. Within the button definition you map a button, list, or text slicer; a data field; or measures from the report to the input parameters of the UDF.</p> <p>-- Evaluation Context - Translytical Sudoku</p>"},{"location":"posts/translytical-rows/#the-challenge-handling-multiple-selections","title":"The Challenge: Handling Multiple Selections","text":"<p>The existing <code>toggle_pencil_mark()</code> function, which handles player input, currently maps the GameId from a measure <code>Selected GameId = SELECTEDVALUE( GameState[GameId] )</code>, and the selected grid position and input value from two separate button slicers. This setup works perfectly for entering a single value into a single cell.</p> <p></p> <p>When a player selects just one cell in the Sudoku grid via a button slicer, all of our action buttons remain active, as expected:</p> <p></p> <p>However, the problem arises when a player attempts to select two or more cells simultaneously. The \"pencil\" button, designed for single-cell input, becomes inactive:</p> <p></p> <p>The cause of this limitation is that Power BI can only pass a single value to the UDF. To enable multi-cell input, we need a method to generate a single string that encapsulates information about multiple selected positions. For those familiar with REST APIs, calling the UDF in this context is analogous to a <code>POST</code> request, where the JSON format is the preferred payload.</p>"},{"location":"posts/translytical-rows/#the-solution-tojson","title":"The Solution: TOJSON()","text":"<p>Fortunately, DAX provides a perfect solution: the built-in <code>TOJSON()</code> function. This function is generates a JSON string from a given table.</p> <p>A crucial consideration is to incorporate error handling for scenarios where the grid might not be filtered, potentially leading to all cells being selected. Let's create the DAX measure with this in mind:</p> <pre><code>Selected Positions = \nIF(\n    ISFILTERED( 'Grid Position'[Position] ),\n    TOJSON( VALUES( 'Grid Position'[Position] ) )\n)\n</code></pre> <p>This provides the following output:</p> <p></p> <p>With our DAX measure ready, the next step is to adapt our Python UDF to correctly parse and process this new JSON input.</p>"},{"location":"posts/translytical-rows/#updating-the-udf","title":"Updating the UDF","text":"<p>Previously, our UDF expected an integer for the <code>position</code> variable. Now that we're sending a JSON string, we need to make the following adjustments:</p> <ul> <li>Variable Type Change: Modify the <code>position</code> variable from <code>int</code> to <code>str</code></li> <li>JSON String to Python Dictionary: Convert the JSON string to a python dictionary with <code>json.loads()</code></li> <li>Iterate and Process: Loop through each position extracted from the JSON and call the stored procedure for each individual position to write the selected value</li> </ul> <pre><code>import fabric.functions as fn\nimport json\nfrom datetime import datetime\n\nudf = fn.UserDataFunctions()\n\n@udf.connection(argName=\"sqlDB\", alias=\"Game\")\n@udf.function()\ndef toggle_pencil_mark(sqlDB: fn.FabricSqlConnection, gameId: int, position: str, pencilValue: int) -&gt; str:\n    \"\"\"\n    Toggle a pencil mark in a Sudoku cell\n\n    Args:\n        gameId: The game ID\n        position: JSON string with cell positions\n        pencilValue: Pencil mark value (1-9)\n\n    Returns:\n        JSON string with operation result\n    \"\"\"\n    try:\n        # Parse the JSON string to get positions\n        positions_data = json.loads(position)\n        positions = [pos[0] for pos in positions_data[\"data\"]]\n\n        # Validate pencil value\n        if not (1 &lt;= pencilValue &lt;= 9):\n            return json.dumps({\n                \"status\": \"ERROR\",\n                \"message\": \"Invalid pencil value. Value must be 1-9.\",\n                \"action_time\": datetime.now().isoformat()\n            })\n\n        conn = sqlDB.connect()\n        cursor = conn.cursor()\n\n        processed_positions = []\n        filled_cells = []\n        last_action_type = None\n        last_is_active = False\n\n        for pos in positions:\n            # Validate position\n            if not (1 &lt;= pos &lt;= 81):\n                continue  # Skip invalid positions\n\n            # Check if cell has a value (cannot add pencil marks)\n            cursor.execute(\"\"\"\n                SELECT CurrentValue FROM GameState \n                WHERE GameId = ? AND Position = ?\n            \"\"\", (gameId, pos))\n\n            result = cursor.fetchone()\n            if result and result[0] is not None:\n                filled_cells.append(pos)\n                continue  # Skip filled cells but continue with others\n\n            # Execute the stored procedure\n            cursor.execute(\"EXEC sp_TogglePencilMark ?, ?, ?\", \n                          (gameId, pos, pencilValue))\n\n            conn.commit()\n\n            # Check if pencil mark was added or removed\n            cursor.execute(\"\"\"\n                SELECT IsActive FROM PencilMarks \n                WHERE GameId = ? AND Position = ? AND PencilValue = ?\n            \"\"\", (gameId, pos, pencilValue))\n\n            pencil_result = cursor.fetchone()\n            is_active = pencil_result[0] if pencil_result else False\n            last_action_type = \"added\" if is_active else \"removed\"\n            last_is_active = is_active\n\n            processed_positions.append(pos)\n\n        # Prepare response message\n        if not processed_positions and filled_cells:\n            return json.dumps({\n                \"status\": \"ERROR\",\n                \"message\": f\"Cannot add pencil marks to filled cells at positions: {filled_cells}\",\n                \"action_time\": datetime.now().isoformat()\n            })\n        elif not processed_positions:\n            return json.dumps({\n                \"status\": \"ERROR\",\n                \"message\": \"No valid positions to process\",\n                \"action_time\": datetime.now().isoformat()\n            })\n\n        # Build success message\n        message = f\"Pencil mark {pencilValue} {last_action_type} at position(s) {processed_positions}\"\n        if filled_cells:\n            message += f\". Skipped filled cells at positions: {filled_cells}\"\n\n        return json.dumps({\n            \"status\": \"SUCCESS\",\n            \"message\": message,\n            \"game_id\": gameId,\n            \"positions_processed\": processed_positions,\n            \"positions_skipped\": filled_cells,\n            \"pencil_value\": pencilValue,\n            \"is_active\": last_is_active,\n            \"action_type\": last_action_type,\n            \"action_time\": datetime.now().isoformat()\n        })\n\n    except json.JSONDecodeError:\n        return json.dumps({\n            \"status\": \"ERROR\",\n            \"message\": \"Invalid JSON format for position parameter\",\n            \"action_time\": datetime.now().isoformat()\n        })\n    except Exception as e:\n        return json.dumps({\n            \"status\": \"ERROR\",\n            \"message\": f\"Database error: {str(e)}\",\n            \"action_time\": datetime.now().isoformat()\n        })\n    finally:\n        if 'cursor' in locals():\n            cursor.close()\n        if 'conn' in locals():\n            conn.close()\n</code></pre> <p>Once these changes are published to your Fabric workspace, we can update the mapping in the Power BI report. Instead of mapping the single position, we now map our new <code>Selected Positions</code> measure to the UDF's <code>position</code> parameter.</p> <p></p> <p>And with that, we can now see it in action, allowing players to enter pencil marks into multiple cells simultaneously.</p> <p></p>"},{"location":"posts/translytical-rows/#conclusion","title":"Conclusion","text":"<p>By combining DAX's <code>TOJSON()</code> function with a refined UDF, we can successfully pass multiple row selections using Translytical Task Flows. This capability significantly enhances the interactivity and user experience of our Sudoku game, allowing for more intuitive and efficient gameplay, and of course for more meaningful reporting solutions.</p>"},{"location":"posts/UDF-Logs/","title":"Logging User Data Functions (UDFs) in Fabric","text":"<p>Fabric User Data Functions (UDFs) are a great way to create reusable and encapsulated logic. While Fabric provides basic invocation logs, they often lack the detail needed for proper debugging and monitoring. This post will show you how to enhance your UDFs with detailed logging by sending them to a Fabric Eventstream and a KQL database which can support advanced analysis and alerting.</p>"},{"location":"posts/UDF-Logs/#basic-udf-logging","title":"Basic UDF Logging","text":"<p>Before we dive into advanced logging, let's understand the built-in capabilities.</p> <p>Preview Feature</p> <p>You will need to enable <code>User Data functions (preview)</code> in the Fabric Admin portal</p> <p>First lets create a simple UDF. Publish and refresh it to make it available.</p> <pre><code>import fabric.functions as fn\n\nudf = fn.UserDataFunctions()\n\n@udf.function()\ndef test( )-&gt; str:\n\n    return f\"hello world\"\n</code></pre> <p>You can test the UDF by hovering over its name in the Function explorer pane and clicking the <code>\u25b7</code> button. This will open a <code>Run</code> pane where we can select <code>run</code> to test the function. The function will run and return \"hello world\".</p> <p></p> <p>To see the invocation log, click <code>...</code> from the Function explorer pane and select <code>View historical log</code>.</p> <p>Tip</p> <p>UDF invocation logs can take a few minutes to appear</p> <p>We are able to see the successful run:</p> <p></p> <p>If we click on the date we can see the message \"There are no logs to show\".</p> <p></p> <p>This highlights the limitation of basic UDF logging, which only records success or failure without capturing internal details.</p>"},{"location":"posts/UDF-Logs/#enhancing-udfs-with-detailed-python-logging","title":"Enhancing UDFs with Detailed Python Logging","text":"<p>To get more insight into what our UDF is doing, we can use Python's built-in <code>logging</code> library.</p> <p>Python's <code>logging</code> library provides five standard levels of severity, from least to most severe: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, and <code>CRITICAL</code>. You can configure the logging level to filter which messages get displayed.</p> <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO) # DEBUG, INFO, WARNING, ERROR, CRITICAL\n\nlogging.debug('This message will NOT be shown.')\nlogging.info('This message will be shown.')\nlogging.warning('This message will be shown.')\n</code></pre> <p>Let's modify our UDF to include some log messages at different levels. We'll set the logging level to <code>INFO</code>, so only messages of <code>INFO</code> or higher severity will be outputted.</p> <pre><code>import fabric.functions as fn\nimport logging\n\nlogging.basicConfig(level=logging.INFO) \n\nudf = fn.UserDataFunctions()\n\n@udf.function()\ndef test( )-&gt; str:\n\n    logging.debug(f\"debug\")\n    logging.info(f\"info\")\n    logging.warning(f\"warning\")\n\n    return f\"hello world\"\n</code></pre> <p>As expected we see our <code>INFO</code> and <code>WARNING</code> logs, but not our <code>DEBUG</code>.</p> <p></p>"},{"location":"posts/UDF-Logs/#storing-logs-for-analysis-with-eventstream","title":"Storing Logs for Analysis with Eventstream","text":"<p>While seeing logs in the invocation pane is useful, it's not ideal for long-term storage, analysis, or alerting. A more robust solution is to send these logs to a Fabric Eventstream, which can then feed them into a KQL database for querying.</p> <p>At the time of writing UDFs logs are not captured in Workspace Monitoring. Hopefully this will come in the future.</p> <p>In the meantime a good option would be to write the logs to a eventhouse, since it is designed for this workload.</p>"},{"location":"posts/UDF-Logs/#setting-up-the-eventstream","title":"Setting up the Eventstream","text":"<ul> <li>Create an Eventstream: In your Fabric workspace, create a new Eventstream</li> <li>Add a Custom App Source: Add a New source and select Custom App. This source acts as an endpoint for external applications (like our UDF) to send data</li> <li>Add a KQL Database Destination: Add a New destination and select your KQL database. This will ingest the data from the eventstream into a new table</li> <li>Publish the Eventstream</li> </ul> <p>If we look at the published Eventstream we can grab the <code>Connection string-primary key</code>, which will we need for our producer in the UDF.</p> <p></p>"},{"location":"posts/UDF-Logs/#adding-the-custom-logging-handler","title":"Adding the Custom Logging Handler","text":"<p>To send logs to the Eventstream, we'll create a custom Python logging handler. This handler will use the <code>azure-eventhub</code> library to connect to the Eventstream and send each log message as a JSON object.</p> <p>First, ensure the <code>azure-eventhub</code> library is installed in your Fabric environment. You can do this by managing your library dependencies.</p> <p></p> <p>Now we can update the UDF:</p> <pre><code>import logging\nimport json\nfrom azure.eventhub import EventHubProducerClient, EventData\nimport fabric.functions as fn\n\nEVENTHUB_CONNECTION_STR = \"Endpoint=sb://xxx.servicebus.windows.net/;SharedAccessKeyName=key_xxx;SharedAccessKey=xxx;EntityPath=xxx9\"\nEVENTHUB_NAME = \"xxx\"\n\nclass FabricEventStreamHandler(logging.Handler):\n    \"\"\"\n    A custom logging handler to send log records to a Fabric Eventstream.\n    \"\"\"\n    def __init__(self, connection_str, eventhub_name):\n        super().__init__()\n        # Initialize the Event Hubs producer client\n        self.producer = EventHubProducerClient.from_connection_string(\n            conn_str=connection_str,\n            eventhub_name=eventhub_name\n        )\n        self.formatter = logging.Formatter(\"{asctime} - {levelname} - {message}\", style=\"{\", datefmt=\"%Y-%m-%d %H:%M\")\n\n    def emit(self, record):\n        \"\"\"\n        Formats the log record and sends it as an event to the Eventstream.\n        \"\"\"\n        try:\n            # Create a log entry with the formatted message and other details\n            log_entry = {\n                \"timestamp\": self.formatter.format(record).split(' - ')[0],\n                \"level\": record.levelname,\n                \"message\": record.message,\n                \"logger_name\": record.name\n            }\n            event_data = EventData(json.dumps(log_entry))\n            # Send the log entry as a batch of one event\n            with self.producer:\n                self.producer.send_batch([event_data])\n        except Exception as e:\n            # Print to standard error if the log sending fails\n            self.handleError(record)\n            print(f\"Failed to send log to Eventstream: {e}\")\n\n# Logger and UDF Setup\nlogger = logging.getLogger(\"logger\")\nlogger.setLevel(logging.INFO)\n\n# Create and add the custom Eventstream handler\neventstream_handler = FabricEventStreamHandler(EVENTHUB_CONNECTION_STR, EVENTHUB_NAME)\nlogger.addHandler(eventstream_handler)\n\nudf = fn.UserDataFunctions()\n\n@udf.function()\ndef test() -&gt; str:\n    # These logs will now be sent to the Fabric Eventstream\n    logger.debug(f\"debug\")\n    logger.info(f\"info\")\n    logger.warning(f\"warning\")\n\n    return \"hello world\"\n</code></pre>"},{"location":"posts/UDF-Logs/#querying-logs-in-the-kql-database","title":"Querying Logs in the KQL Database","text":"<p>Now we can publish and run the UDF. Now, if we go to the Logs Eventhouse QuerySet, we can query our database, and see our logs. </p> <p>I ran the UDF twice, so have two sets of logs below.</p> <p></p> <p>This solution helps ensuring the health and reliability of your Fabric environment. You could even use Activator to trigger an action, like sending a Teams message, when an ERROR or CRITICAL log is detected.</p>"},{"location":"posts/UDF-Logs/#conclusion","title":"Conclusion","text":"<p>Logging is an essential part of building robust applications. While Fabric's built-in UDF logs are a good starting point, integrating them with an Eventstream and KQL database provides a scalable solution for in-depth analysis and proactive monitoring.</p>"},{"location":"posts/udf-colours/","title":"DAX UDF Colour Library","text":"<p>Microsoft have just released User Defined Functions (UDFs). This game-changing feature allows developers to create and reuse their own custom functions within a semantic model. Once defined, they are treated like any of the pre-built functions, andy can be invoked anywhere in your model, centralizing and simplifying your DAX code. This makes measures less verbose and easier to manage</p> <p>Additionally SQLBI has released DAX Lib, a centralized, community-driven library for sharing and discovering UDFs. With DAX Lib, you can easily find, export, and import libraries using TMDL. This is a massive step for community collaboration, and it's fantastic to see it already populated with libraries for conversions, filtering, SVG, and color manipulation.</p>"},{"location":"posts/udf-colours/#dax-user-defined-functions-udf","title":"DAX User Defined Functions (UDF)","text":"<p>I am not going to dive too deep on how to define UDFs, as the docs and SQLBIs video already do a good job. </p> <p>But we can have a quick look at a simple UDF:</p> <pre><code>function\n    HelloWorld() =&gt; \"Hello World\"\n</code></pre> <p>UDFs are incredibly versatile. Although the example above doesn't accept any parameters, UDF can accept parameters like strings, numbers, columns, measures, or tables, and return either scalar values or entire tables. The parameters can have hints to provide implicit casting (i.e. int64, double), and define the evaluation mode (val, expr) to determine if the parameter is evaluated before entering the function or within it.</p>"},{"location":"posts/udf-colours/#dax-lib","title":"DAX Lib","text":"<p>DAX Lib is a centralized repository of UDF libraries. You can search for existing library, easily export the library as a TMDL script and import into your model with TMDL view. As of launch there are already libraries for conversions, filtering, SVG and colour manipulation. Anyone can contribute to this platform via github. </p> <p>Contribute To DAX Lib</p> <p>Please read and follow the contribute to DAX Lib guide if you want to contribute a library.</p>"},{"location":"posts/udf-colours/#evaluationcontextcolour-library","title":"EvaluationContext.Colour Library","text":"<p> Docs  Package</p> <p>My recent contribution to DAX Lib was EvaluationContext.Colour, This library provides a comprehensive set of functions to manipulate HEX color codes, making it easier to manage colors for tasks like SVG visualizations and conditional formatting. The library includes a range of functions, for adjusting properties like hue, saturation, and luminance, using pre-built themes, and determining the best text color for contrast with a given background.</p> Functions <ul> <li>Colour Conversion<ul> <li><code>EvaluationContext.Colour.Int.ToHex</code></li> <li><code>EvaluationContext.Colour.Hex.ToInt</code></li> <li><code>EvaluationContext.Colour.RGB.ToHex</code></li> <li><code>EvaluationContext.Colour.HSL.ToHex</code></li> </ul> </li> <li>Hex Colour Manipulation<ul> <li><code>EvaluationContext.Colour.HEX.Hue</code></li> <li><code>EvaluationContext.Colour.HEX.Saturation</code></li> <li><code>EvaluationContext.Colour.HEX.Luminance</code></li> <li><code>EvaluationContext.Colour.HEX.Alpha</code></li> <li><code>EvaluationContext.Colour.HEX.AdjustHue</code></li> <li><code>EvaluationContext.Colour.HEX.AdjustSaturation</code></li> <li><code>EvaluationContext.Colour.HEX.AdjustLuminance</code></li> <li><code>EvaluationContext.Colour.HEX.AdjustAlpha</code></li> <li><code>EvaluationContext.Colour.HEX.TextColour</code></li> </ul> </li> <li>Colour Theming<ul> <li><code>EvaluationContext.Colour.Hex.Theme</code></li> <li><code>EvaluationContext.Colour.HEX.LinearTheme</code></li> <li><code>EvaluationContext.Colour.Hex.Interpolate</code></li> </ul> </li> </ul> <p>You can see these functions in action in Power BI:</p> <p></p>"},{"location":"posts/udf-colours/#conclusion","title":"Conclusion","text":"<p>Major updates to DAX don't happen often, but UDFs are a game-changer. They will fundamentally transform the way Power BI developers write, share, and use DAX, opening up new possibilities for creating robust, reusable, and maintainable semantic models. I encourage everyone to check out DAX Lib and consider contributing to this incredible community effort!</p>"},{"location":"posts/DaxLibContribute/","title":"Contributing to DaxLib","text":"<p>Dax Lib, a centralized repository of DAX UDF libraries, has just announced a new type of library - Medium/Large. I will show the processes of how to submit a Small library, then how to move from a Small to Medium/Large library, and explore why you might want to do this.</p>"},{"location":"posts/DaxLibContribute/#small-library","title":"Small library","text":"<p>In this mode you develop your library on a personal fork of  <code>daxlib/daxlib</code> (i.e  <code>evaluationcontext/daxlib</code>), then submit a  pull request to  <code>daxlib/daxlib</code> to submit your library, and your  library will be released to <code>Dax lib</code>. There is a guide on how to contribute a small library to DAX Lib.</p> <p>The process is as follows:</p> <pre><code>sequenceDiagram\n    participant DaxLib as daxlib.org\n    participant Main as daxlib/daxlib\n    participant Fork as evaluationcontext/daxlib\n\n    Main-&gt;&gt;Fork: 1. Fork repository\n    Fork-&gt;&gt;Fork: 2. Develop library\n    Fork-&gt;&gt;Main: 3. Submit pull request\n    Main-&gt;&gt;Main: 4. Review &amp; merge\n    Main-&gt;&gt;DaxLib: 5. Package deployed</code></pre> <ul> <li>Go to  daxlib/daxlib github repo, and create a fork </li> <li>Create a folder within packages, for your library, and add/update the relevant files</li> </ul> daxlib/daxlib package daxlib/daxlib repo structurelib/functions.tmdlmanifest.daxlibicon.pngREADME.md <p>The example below shows the structure of the  <code>daxlib/daxlib</code> repo, showing only the relevant items</p> <pre><code>\u2514\u2500\u2500 \ud83d\udcc1 packages\n    |\u2500\u2500 \ud83d\udcc1 a   \n    \u251c\u2500\u2500 \ud83d\udcc1 ...\n    \u251c\u2500\u2500 \ud83d\udcc1 e\n    \u2502    \u2514\u2500\u2500 \ud83d\udcc1 evaluationcontext.colour // (1)!\n    \u2502         \u251c\u2500\u2500 \ud83d\udcc1 0.1.0\n    \u2502         \u2514\u2500\u2500 \ud83d\udcc1 0.1.1\n    \u2502              \u251c\u2500\u2500 \ud83d\udcc1 lib\n    \u2502              \u2502    \u2514\u2500\u2500 functions.tmdl // (2)!\n    \u2502              \u251c\u2500\u2500 \ud83d\udcc4 icon.png // (3)!\n    \u2502              \u251c\u2500\u2500 \ud83d\udcc4 README.md // (4)!\n    \u2502              \u2514\u2500\u2500 \ud83d\udcc4 manifest.daxlib // (5)!\n    \u251c\u2500\u2500 \ud83d\udcc1 ...\n    \u2514\u2500\u2500 \ud83d\udcc1 z\n</code></pre> <ol> <li>Your library</li> <li>Required - Your DAX UDF functions </li> <li>Optional - Icon for your library</li> <li>Optional - Docs for your library</li> <li>Required - Declares package properties</li> </ol> <p>Required: Contains TMDL definition of the functions within your library</p> Naming Convention <p>There are some guidelines on DAX UDF naming conventions</p> <ul> <li>DAX Lib Naming Conventions</li> <li>SQLBI DAX Naming Conventions</li> </ul> Annotation <p>TDML script must not have <code>CreateOrReplace</code> keyword</p> <p>Functions must have the following annotations:</p> <pre><code>annotation DAXLIB_PackageId = EvaluationContext.Colour\nannotation DAXLIB_PackageVersion = 0.1.2-beta\n</code></pre> <pre><code>/// Int to Hex conversion\n/// number  INT64   The integer to convert\n/// padTo   INT64   Optional: Minimum number of characters in result\nfunction 'EvaluationContext.Colour.Int.ToHex' =\n    (\n      number: INT64,\n      padTo: INT64\n    ) =&gt;\n\n      VAR MinPadding = IF( number = 0, 1, CEILING( LOG( number + 1, 16 ), 1 ) )\n      VAR ActualPadding = MAX( MinPadding, IF( ISBLANK( padTo ), MinPadding, padTo ) )\n      VAR BitTable = GENERATESERIES( 1, ActualPadding )\n      VAR Hex =\n        CONCATENATEX(\n          BitTable,\n          VAR c = MOD( TRUNC( number / POWER( 16, [Value] - 1 ) ), 16 )\n          RETURN\n            SWITCH( c, 10, \"A\", 11, \"B\", 12, \"C\", 13, \"D\", 14, \"E\", 15, \"F\", c ),\n          \"\",\n          [Value],\n          DESC\n        )\n\n      RETURN Hex\n\n  annotation DAXLIB_PackageId = EvaluationContext.Colour\n  annotation DAXLIB_PackageVersion = 0.1.2-beta\n...\n</code></pre> <p>Required: The package properties in JSON format</p> <pre><code>{\n  \"$schema\": \"https://raw.githubusercontent.com/sql-bi/daxlib/refs/heads/main/schemas/manifest/1.0.0/manifest.1.0.0.schema.json\",\n  \"id\": \"EvaluationContext.Colour\",\n  \"version\": \"0.1.2-beta\",\n  \"authors\": \"Jake Duddy\",\n  \"description\": \"A comprehensive set of User-Defined Functions designed to enable easy manipulation of hex colours\",\n  \"tags\": \"DAX,UDF,colour\",\n  \"releaseNotes\": \"Added Documentation page and `EvaluationContext.Colour.Hex.Interpolate` function\",\n  \"projectUrl\": \"https://evaluationcontext.github.io/EvaluationContext.Colour/\",\n  \"repositoryUrl\": \"https://github.com/sql-bi/daxlib/tree/main/packages/e/evaluationcontext.colour\",\n  \"icon\": \"/icon.png\", // (1)!\n  \"readme\": \"/README.md\" // (2)!\n}\n</code></pre> <ol> <li>Required if you include a <code>icon.png</code> file</li> <li>Required if you include a <code>README.md</code> file</li> </ol> <p>Optional: icon for library</p> <p>Remarks</p> <p>The icon file must be in PNG format (.PNG), with a maximum size of 100 KB</p> <p>Optional: Markdown docs file, with general information about the library, usage instructions, examples, and any notes for users</p> <p>Remarks</p> <p>The file must be in Markdown format (.MD), with a maximum size of 100 KB</p> <p>For security reasons, only a limited set of Markdown features are supported, and external links may be restricted to trusted domains</p> <ul> <li>When you are ready to publish a version of the  library, submit a  pull request from your  <code>fork</code> to  <code>daxlib/daxlib</code></li> <li>After approval your  library will appear on DAX Lib for other to download and use</li> </ul> <p>Packages are immutable</p> <p>After a pull request has been accepted, Packages are immutable. If you want to submit any changes to functions or files, a new version (i.e. v0.1.0  v0.2.0 ) must be created, and function annotations and <code>manifest.daxlib</code> must be updated with the new version number. Changes can then be submitted with a new pull request.</p>"},{"location":"posts/DaxLibContribute/#mediumlarge-library","title":"Medium/large Library","text":"<p>The process for Medium/Large libraries uses a extended workflow.</p> <p>We still need a fork of  <code>daxlib/daxlib</code> (i.e.  <code>evaluationscontext/daxlib</code>), and will still submit a  pull request to  <code>daxlib/daxlib</code> to publish a library.</p> <p>The difference is that development of the library will occur on a fork of  <code>daxlib/lib-quickstart-template</code> (one per library i.e.  <code>evaluationcontext/evaluationcontext.colour</code>). A github workflow can be run on  <code>evaluationcontext/evaluationcontext.colour</code>, which will push the library to a new, version specific, branch of  <code>evaluationcontext/daxlib</code>, which can then be submitted via a  pull request to  <code>daxlib/daxlib</code>.</p> <pre><code>sequenceDiagram\n    participant DaxLib as daxlib.org\n    participant Main as daxlib/daxlib\n    participant Fork as evaluationcontext/daxlib\n    participant LibRepo1 as evaluationcontext/evaluationcontext.colour\n\n    Main-&gt;&gt;Fork: 1. Fork repository\n    LibRepo1-&gt;&gt;LibRepo1: 2. Develop library\n    LibRepo1-&gt;&gt;Fork: 3. Workflow creates branch\n    Fork-&gt;&gt;Main: 4. Submit pull request\n    Main-&gt;&gt;Main: 5. Review &amp; merge\n    Main-&gt;&gt;DaxLib: 6. Package deployed</code></pre>"},{"location":"posts/DaxLibContribute/#why-create-a-mediumlarge-library","title":"Why Create a Medium/large Library?","text":"<p>Since you have a specific repo dedicated to your library you are able to:</p> <ul> <li>Connect with your users with GitHub issues</li> <li>Collaborate with others to develop the library</li> <li>Add documentation site and host (for example) on GitHub Pages</li> <li>Opens the door for auto-documentation generation</li> </ul>"},{"location":"posts/DaxLibContribute/#creating-a-mediumlarge-library","title":"Creating a Medium/large Library","text":""},{"location":"posts/DaxLibContribute/#creating-a-library-repo","title":"Creating a Library Repo","text":"<p>Let's start by creating a development repo ( evaluationcontext/evaluationcontext.colour) from  daxlib/lib-quickstart-template. This is where we can develop our library.</p> <p></p> <p></p>"},{"location":"posts/DaxLibContribute/#modifying-repo","title":"Modifying Repo","text":"<p>Now we have a development repo we need to update it's content.</p> <ul> <li>Go to vscode and run <code>git clone https://github.com/EvaluationContext/evaluationcontext.colour.git</code></li> </ul> <p>daxlib/lib-quickstart-template structure</p> <p>We can see the structure is slightly different, but the required content remain the same, except your library now lives in the <code>src</code> folder</p> <pre><code>\u251c\u2500\u2500 \ud83d\udcc1 .github\n\u2502    \u2514\u2500\u2500 \ud83d\udcc1 workflows\n\u2502        \u2514\u2500\u2500 \ud83d\udcc4 publish-package.yml // (1)!\n\u2514\u2500\u2500 \ud83d\udcc1 src // (2)!\n     \u251c\u2500\u2500 \ud83d\udcc1 lib\n     \u2502    \u2514\u2500\u2500 functions.tmdl // (3)!\n     \u251c\u2500\u2500 \ud83d\udcc4 icon.png // (4)!\n     \u251c\u2500\u2500 \ud83d\udcc4 README.md // (5)!\n     \u2514\u2500\u2500 \ud83d\udcc4 manifest.daxlib // (6)!\n</code></pre> <ol> <li>Workflow to create a pull request to daxlib/daxlib repo</li> <li>Your library</li> <li>Required - Your DAX UDF functions</li> <li>Optional - Icon for your library</li> <li>Optional - Docs for your library</li> <li>Required - Declares package properties</li> </ol> <ul> <li>Since I already have a Small Library I can copy the files to <code>src</code> folder</li> <li>Since this will be a new version I need to update the version number in <code>manifest.daxlib</code> (0.1.2-beta  0.1.3-beta)</li> <li>I can also update the main repo <code>README.md</code> to give specific info about this library</li> </ul> <p>Annotation Placeholders</p> <p>We are able to replace the annotation values in <code>functions.tmdl</code> with placeholders. These will be overwritten by the version specified in <code>manifest.daxlib</code> when running the <code>publish-package.yml</code> workflow. </p> <pre><code>- annotation DAXLIB_PackageId = EvaluationContext.Colour\n+ annotation DAXLIB_PackageId = __PLACEHOLDER_PACKAGE_ID__\n\n- annotation DAXLIB_PackageVersion = 0.1.3-beta\n+ annotation DAXLIB_PackageVersion = __PLACEHOLDER_PACKAGE_VERSION__\n</code></pre>"},{"location":"posts/DaxLibContribute/#adding-a-docs-site","title":"Adding a Docs Site","text":"<p>For my version of the library docs I used  Jekyll and the Just the Docs theme, primarily because I have been using the chirpy Material for my blog. But I have recently transitioned my blog over to the  Material Theme for MKDocs, as it has some really nice functionality, and faster build times. So lets quickly add a Material for MKDocs site definition to the repo.</p> Material for MKDocs <p>The Material for MKDocs docs site give really good documentation for how to create a site and clear explanation of all the features and how to use them. Additionally this video series is very good at getting you started.</p> <p>I am going to some files to define the site to the repo:</p> <pre><code>\u251c\u2500\u2500 \ud83d\udcc1 .devcontainer // (1)!\n\u2502    \u251c\u2500\u2500 \ud83d\udcc4 devcontainer.json\n\u2502    \u2514\u2500\u2500 \ud83d\udcc4 Dockerfile\n\u251c\u2500\u2500 \ud83d\udcc1 .github\n\u2502    \u2514\u2500\u2500 \ud83d\udcc1 workflows\n\u2502        \u251c\u2500\u2500 \ud83d\udcc4 publish-package.yml\n\u2502        \u2514\u2500\u2500 \ud83d\udcc4 ci.yml // (3)!\n\u251c\u2500\u2500 \ud83d\udcc1 src\n\u2502    \u251c\u2500\u2500 \ud83d\udcc1 lib\n\u2502    \u2502    \u2514\u2500\u2500 functions.tmdl\n\u2502    \u251c\u2500\u2500 \ud83d\udcc4 icon.png\n\u2502    \u251c\u2500\u2500 \ud83d\udcc4 README.md\n\u2502    \u2514\u2500\u2500 \ud83d\udcc4 manifest.daxlib\n\u251c\u2500\u2500 \ud83d\udcc1 docs // (4)!\n\u2502    \u251c\u2500\u2500 \ud83d\udcc4 index.md\n\u2502    \u2514\u2500\u2500 \ud83d\udcc4 ...\n\u251c\u2500\u2500 \ud83d\udcc1 PowerBI // (5)!\n\u2502    \u251c\u2500\u2500 \ud83d\udcc4 Model.pbip\n\u2502    \u2514\u2500\u2500 \ud83d\udcc4 ...\n\u251c\u2500\u2500 \ud83d\udcc4 mkdocs.yml // (7)!\n\u251c\u2500\u2500 \ud83d\udcc4 requirements.txt // (2)!\n\u2514\u2500\u2500 \ud83d\udcc4 .gitignore // (6)!\n</code></pre> <ol> <li>Dev Container configuration for containerized development</li> <li>Python dependencies for MKDocs and Material theme</li> <li>GitHub workflow to build and deploy site to <code>gh-pages</code> branch</li> <li>Markdown files for all site pages</li> <li>Power BI Project file for function testing and examples</li> <li>Ignore PBIP and MKDocs specific files</li> <li>Site configuration</li> </ol> <p>Once I have everything setup I can run the run the site in the Dev Container.</p> Dev Container dependencies <p>To run dev containers you need Docker Desktop installed. Additionally if you are running on Windows you will need to install Windows Subsystem for Linux (WSL). </p> <p>Since MKdocs runs on python, you could could install python, and pip install the dependencies, on your machine without a container.</p> <p>Once the container is built I can go to the terminal and serve the site on localhost.</p> <pre><code>mkdocs serve --livereload\n</code></pre> <p>Then view the site at <code>http://localhost:8000/</code> and test to make sure the site is functioning correctly.</p> <p></p> <p>Then we can <code>push</code> to GitHub, this will run the <code>ci</code> job, which will execute <code>mkdocs build</code>, generating the html pages, and saving the result to a  <code>gh-pages</code> branch. </p> <p>We can now setup GitHub pages to use the  <code>gh-pages</code> branch as a source to deploy our site, by selecting <code>Setting</code> from the top nav bar of the Git Hub repo, select <code>pages</code> and set <code>Deploy from a branch</code> and set the branch to  <code>gh-pages</code>, and select save.</p> <p></p> <p>We will see a <code>pages build and deployment</code> action kick off, which will deploy the site.</p> <p></p> <p>Then we can navigate to our site URL and confirm it deployed successfully.</p> <p></p>"},{"location":"posts/DaxLibContribute/#deploying-library","title":"Deploying Library","text":"<p>After that brief detour, we can deploy the library by following the Publish Your Library guidance.</p> <p>We first need to create a Personal Access Token, granting <code>read/write</code> permissions on  <code>evaluationcontext/daxlib</code>.</p> <p></p> <p>We add the token as a secret on  <code>evaluationcontext/evaluationcontext.colour</code>, granting these permission to  <code>evaluationcontext/daxlib</code>. So that the workflow run from  <code>evaluationcontext/evaluationcontext.colour</code> can create a new  branch in  <code>evaluationcontext/daxlib</code></p> <p></p> <p>Navigate to <code>actions</code>, select <code>publish-package</code> and <code>Run workflow</code>.</p> <p></p> <p>Once this has run we can navigate to our  <code>evaluationcontext/daxlib</code>, and we confirm new branch ( <code>evaluationcontext.colour/publish-EvaluationContext.Colour-0.1.3-beta</code>) has been created.</p> <p></p> <p>If we go back the actions section of  <code>evaluationcontext/evaluationcontext.colour</code> and open the completed <code>publish-package</code> job, you can see we get the option to open a  pull request.</p> <p></p> <p>Then we can select <code>Open Pull Request</code> and <code>Create pull request</code> to create a  pull request to  <code>daxlib/daxlib</code>.</p> <p></p> <p>The  pull request will then be reviewed by the DAX Lib maintainers. If changes are requested during the review:</p> <ul> <li>Apply the requested fixes to your code in the development repo, and commit them to your repository</li> <li>Re-run the <code>publish-package</code> workflow</li> <li>The  pull request will be automatically updated</li> </ul> <p>Once your  pull request is approved and merged, your  library will be automatically published on daxlib.org.</p>"},{"location":"posts/DAXLibSVG-0-2-0/","title":"DaxLib.SVG","text":"<p>I am happy to announce the release of <code>DaxLib.SVG v0.2.0-beta</code>, designed to help make creating SVGs in Power BI easier for everyone.</p> <p> Docs  Package</p> <p></p>"},{"location":"posts/DAXLibSVG-0-2-0/#what-are-svgs","title":"What are SVGs?","text":"<p>SVGs are <code>Vector</code> images and defined using <code>XML</code>. You can define elements like <code>&lt;circle&gt;</code>, <code>&lt;path&gt;</code>, <code>&lt;rect&gt;</code> which are rendered into images.</p> <p>For example you can define a simple circle like this:</p> <pre><code>&lt;svg width='500' height='100' viewbox= '0 0 100 20' xmlns='http://www.w3.org/2000/svg'&gt;\n    &lt;circle cx='50' cy='10' r='10%' fill='#EC008C' fill-opacity='0.5' stroke='#EC008C' stroke-width='1'/&gt;\n&lt;/svg&gt;\n</code></pre> <p></p> <p>Intro to SVGs</p> <p>If you want to know more about SVGs, Joshua Comeau has a fantastic blog post on SVGs: A Friendly Introduction to SVG</p>"},{"location":"posts/DAXLibSVG-0-2-0/#daxlibsvg-function-categories","title":"DaxLib.SVG Function Categories","text":"<p>DaxLib.SVG has the following function categories: </p> <ul> <li> <p> SVG Wrapper </p> <p>Wraps one or more <code>Elements</code>, <code>Defs</code>, <code>Compounds</code>, applying required metadata and <code>&lt;SVG&gt;</code> tags</p> </li> <li> <p> Elements</p> <p>SVG primitives (i.e. <code>&lt;Rect&gt;</code>, <code>&lt;Circle&gt;</code>, <code>&lt;line&gt;</code>)</p> </li> <li> <p> Compounds </p> <p>Complex <code>Compound</code> components (i.e. boxplot, violin) made from <code>Elements</code></p> </li> <li> <p> Defs</p> <p>Define reusable SVG elements (i.e. <code>&lt;linearGradient&gt;</code>, <code>&lt;clipPath&gt;</code>, and shapes). </p> </li> <li> <p> Attr</p> <p>Create Attributes (i.e.fill, stroke, opacity) string that can applied to <code>Elements</code> and <code>Defs</code></p> </li> <li> <p> Transforms</p> <p>Create Transform strings</p> </li> <li> <p> Scales</p> <p>Mapping values between scales</p> </li> <li> <p> Colors </p> <p>Themes and functions for colour manipulation.</p> </li> </ul>"},{"location":"posts/DAXLibSVG-0-2-0/#how-do-i-use-daxlibsvg","title":"How do I use DaxLib.SVG?","text":"<p>For detailed examples and complete API reference, check out the documentation site or download the example PBIP file. But lets have a quick tour of what DaxLib.SVG offers.</p> <p>Data Category</p> <p>Any measures or column that use these UDFs must have <code>DataCategory = \"ImageUrl\"</code>, otherwise you will see a string output instead of the image.</p>"},{"location":"posts/DAXLibSVG-0-2-0/#svg-wrapper","title":"SVG Wrapper","text":"<p><code>DaxLib.SVG.SVG()</code> is a wrapper function. This wraps <code>daxlib.svg.Element.*</code>, <code>daxlib.svg.Compound.*</code> and <code>daxlib.svg.Def.*</code> with <code>&lt;SVG&gt;</code> tags plus the metadata Power BI need to interpret the SVG definition correctly (<code>data:image/svg+xml;utf8</code>)</p> ExampleSyntaxDefinition <pre><code>DaxLib.SVG.SVG(\n    \"100\",\n    \"100\",\n    \"viewBox='0 0 100 100' \",\n    DaxLib.SVG.Element.Circle(\"50\", \"50\", \"40\", \"fill:blue;\", BLANK(), BLANK()),\n    BLANK()\n)\n</code></pre> <pre><code>DaxLib.SVG.SVG( width, height, viewbox, contents, sortValue)\n</code></pre> Name Type Required Description width STRING Width (pixels or percentage) height STRING Height (pixels or percentage) viewbox STRING viewBox (e.g., \"0 0 100 100\") contents STRING SVG elements to include (e.g., from DaxLib.SVG.Element functions) sortValue EXPR Sort value for ordering in tables <p>STRING SVG string</p> <pre><code>function 'DaxLib.SVG.SVG' =\n    (\n        width : STRING,\n        height : STRING,\n        viewbox : STRING,\n        contents : STRING,\n        sortValue : EXPR\n    ) =&gt;\n\n        VAR _Canvas =   IF( NOT ISBLANK( width ) &amp;&amp; NOT ISBLANK( height ), \"width='\" &amp; width &amp; \"' height='\" &amp; height &amp; \"' \" )\n        VAR _SortDesc = IF( NOT ISBLANK( sortValue ), \"&lt;desc&gt;\" &amp; FORMAT( sortValue, \"000000000000\" ) &amp; \"&lt;/desc&gt;\" )\n\n        RETURN \n\n            \"data:image/svg+xml;utf8,\" &amp;\n            \"&lt;svg \" &amp;\n            _Canvas &amp;\n            viewbox &amp;\n            \"xmlns='http://www.w3.org/2000/svg'&gt;\" &amp;\n            _SortDesc &amp;\n            contents &amp;\n            \"&lt;/svg&gt;\"\n</code></pre>"},{"location":"posts/DAXLibSVG-0-2-0/#elements","title":"Elements","text":"<p>These functions generate SVG primitives such as <code>&lt;circle&gt;</code>, <code>&lt;rect&gt;</code>, <code>&lt;line&gt;</code>, <code>&lt;polygon&gt;</code>, <code>&lt;polyline&gt;</code>, <code>&lt;text&gt;</code>, and <code>&lt;&lt;g&gt;</code>. Use these together to build more complex SVG graphics.</p> CirclelinePathsRectTxtUseDef <p></p> <p></p> <p></p> <p></p> <p>DaxLib</p> <p></p> <p>For example we can use <code>DaxLib.SVG.Element.Circle()</code> to draw the circle above in Power BI like this:</p> ExampleSyntaxDefinition <pre><code>DaxLib.SVG.SVG(\n    500,                // width\n    100,                // height\n    \"0 0 100 20\",       // viewbox\n    DaxLib.SVG.Element.Circle(\n        50,             // cx\n        10,             // cy\n        \"10%\",          // r\n        DaxLib.SVG.Attr.Shapes(\n            DaxLib.SVG.Colour.Theme(\n                \"Power BI\",\n                25\n            ),              // fill\n            0.5,            // fillOpacity\n            BLANK(),        // fillRule   \n            DaxLib.SVG.Colour.Theme(\n                \"Power BI\",\n                25\n            ),              // stroke\n            1,              // strokeWidth\n            BLANK(),        // strokeOpacity\n            BLANK()         // opacity\n        ),              // attributes\n        BLANK()         // transforms\n    ),                  // contents\n    BLANK()             // sortValue\n)\n</code></pre> <pre><code>DaxLib.SVG.Element.Circle( cx, cy, r, attributes, transforms )\n</code></pre> Name Type Required Description cx STRING The x position of the center cy STRING The y position of the center r STRING The radius attributes STRING Direct SVG attributes to apply (e.g., \"fill='red' stroke-width='2'\"), can generate with <code>DaxLib.SVG.Attr.*</code> or manually transforms STRING Transformation to apply (can be generated with <code>DaxLib.SVG.Transforms</code>) <p>STRING <code>&lt;circle&gt;</code> element</p> <pre><code>function 'DaxLib.SVG.Element.Circle' = \n    (\n        cx: STRING,\n        cy: STRING,\n        r: STRING,\n        attributes: STRING,\n        transforms: STRING\n    ) =&gt;\n\n        \"&lt;circle\" &amp;\n        \" cx='\" &amp; cx &amp; \"'\" &amp;\n        \" cy='\" &amp; cy &amp; \"'\" &amp;\n        \" r='\" &amp; r &amp; \"'\" &amp;\n        IF( NOT ISBLANK( attributes ), \" \" &amp; attributes &amp; \" \" ) &amp;\n        IF( NOT ISBLANK( transforms ), \" transform='\" &amp; transforms &amp; \"'\" ) &amp; \n        \"/&gt;\"\n</code></pre>"},{"location":"posts/DAXLibSVG-0-2-0/#compounds","title":"Compounds","text":"<p>Compound functions combine multiple <code>DaxLib.SVG.Element.*</code> functions into higher-level reusable components and complete chart visualizations. These simplify the creation of complex visuals by providing ready-made chart types.</p> AreaBoxPlotHeatmapJitterLineViolinPill <p></p> <p></p> <p></p> <p> </p> <p></p> <p></p> <p>DaxLib</p> <p>Each compound function accepts <code>x</code>, <code>y</code>, <code>width</code>, and <code>height</code> parameters, making it easy to combine multiple compounds together or mix them with basic elements. Here is how to create a violin plot.</p> ExampleSyntaxDefinition <pre><code>DaxLib.SVG.SVG(\n    500,\n    100,\n    BLANK(),\n    DaxLib.SVG.Compound.Violin(\n        0,                  // x\n        0,                  // y\n        500,                // width\n        100,                // height\n        0.05,               // paddingX\n        0.02,               // paddingY\n        Dates[Date],        // axisRef\n        [Total Cost],       // measureVal\n        MAX( Samples[Samples] ), // samples\n        MAX( Bandwidth[Bandwidth] ), // bandwidth\n        DaxLib.SVG.Colour.Theme(\n            \"Power BI\",\n            25\n        )                   // color\n    ),\n    BLANK()\n)\n</code></pre> <pre><code>DaxLib.SVG.Compound.Violin( x, y, width, height, paddingX, paddingY, axisRef, measureRef, samples, bandwidth, color )\n</code></pre> Parameter Type Required Description x INT64 The x position of the compound y INT64 The y position of the compound width INT64 The width of the compound height INT64 The height of the compound paddingX DOUBLE The horizontal padding percentage (0.0-1.0, e.g., 0.1 = 10% padding) paddingY DOUBLE The vertical padding percentage (0.0-1.0, e.g., 0.1 = 10% padding) axisRef ANYREF EXPR The column that the measure will be evaluated against measureRef NUMERIC EXPR The measure to evaluate samples INT64 Number of density calculation points bandwidth NUMERIC Kernel bandwidth for smoothing color STRING Fill color for the violin shape <p>STRING An SVG violin plot showing the probability density of data using kernel density estimation</p> <pre><code>function 'DaxLib.SVG.Compound.Violin' =\n    (\n        x: INT64,\n        y: INT64,\n        width: INT64,\n        height: INT64,\n        paddingX: DOUBLE,\n        paddingY: DOUBLE,\n        axisRef: ANYREF EXPR,\n        measureRef: NUMERIC EXPR,\n        samples: INT64,\n        bandwidth: NUMERIC,\n        color: STRING\n    ) =&gt;\n\n        // Apply padding to dimensions\n        VAR _X =            x + (width * (IF(ISBLANK(paddingX), 0, paddingX) / 2))\n        VAR _Y =            y + (height * (IF(ISBLANK(paddingY), 0, paddingY) / 2))\n        VAR _Width =        width * (1 - IF(ISBLANK(paddingX), 0, paddingX))\n        VAR _Height =       height * (1 - IF(ISBLANK(paddingY), 0, paddingY))\n\n        // Check if Axis is numeric\n        VAR axisSample =    MAX( axisRef )\n        VAR axisIsNumeric = ISNUMERIC( axisSample ) || ISDATETIME( axisSample )\n\n        // For totals\n        VAR _Data = \n            ADDCOLUMNS(\n                FILTER(\n                    VALUES( axisRef ),\n                    NOT ISBLANK( measureRef )\n                ),\n                \"@AxisIndex\",   \n                    IF(\n                        axisIsNumeric,\n                        axisRef,\n                        RANK( DENSE, CALCULATETABLE( VALUES( axisRef ), ALLSELECTED() ) )\n                    ),\n                \"@Value\", measureRef\n            )\n\n        VAR _NumValues =        COUNTROWS( _Data )\n        VAR _Min =              MINX( _Data, [@Value] )\n        VAR _Max =              MAXX( _Data, [@Value] )\n        VAR _Range =            _Max - _Min\n        VAR _RangePerSample =   _Range / samples\n\n        // Calculate Kernel Density Estimation using Normal distribution\n        VAR _KDE = \n            ADDCOLUMNS(\n                GENERATESERIES( 0, samples + 1, 1 ),\n                \"@InputX\", _Min + _RangePerSample * [Value],\n                \"@KDE\", \n                    ( 1 / _NumValues ) * \n                    SUMX(\n                        _Data, \n                        NORM.DIST( \n                            _Min + _RangePerSample * [Value], \n                            [@Value], \n                            bandwidth, \n                            FALSE \n                        ) \n                    )\n            )\n\n        VAR _MaxKDE =       MAXX( _KDE, [@KDE] )\n\n        // Map KDE values to SVG coordinates using normalize function\n        VAR _Points = \n            ADDCOLUMNS(\n                _KDE,\n                \"@X\", DaxLib.SVG.Scale.Normalize( [@InputX], _Min, _Max, _X, _X + _Width),\n                \"@Y\", DaxLib.SVG.Scale.Normalize( [@KDE], 0, _MaxKDE, _Y + _Height * 0.5, _Y )\n            )\n\n        // Create control points for smooth B\u00e9zier curves\n        VAR _PointsWithPrev = \n            NATURALLEFTOUTERJOIN(\n                _Points,\n                SELECTCOLUMNS(\n                    _Points,\n                    \"Value\", [Value] + 1,\n                    \"@PrevX\", [@X],\n                    \"@PrevY\", [@Y]\n                )\n            )\n\n        VAR _WithControlPoints = \n            ADDCOLUMNS(\n                _PointsWithPrev,\n                \"@CX\", [@prevX] + ( ( [@x] - [@prevX] ) / 2 ),\n                \"@CY\", [@y]\n            )\n\n        // Create the violin shape as a single closed path\n        // Start at the center-left, go up the top curve, then down the bottom curve, and close\n        VAR _FirstPoint = MINX( _Points, [@X] )\n        VAR _LastPoint = MAXX( _Points, [@X] )\n        VAR _CenterY = _Y + (_Height * 0.5)\n\n        // Top half curve (from left to right)\n        VAR _TopCurve = \n            CONCATENATEX(\n                _WithControlPoints,\n                IF(\n                    [Value] = 0,\n                    \"M \" &amp; [@X] &amp; \" \" &amp; _CenterY &amp; \" L \" &amp; [@X] &amp; \" \" &amp; [@Y],\n                    \"S \" &amp; [@CX] &amp; \" \" &amp; [@CY] &amp; \", \" &amp; [@X] &amp; \" \" &amp; [@Y]\n                ),\n                \" \",\n                [Value],\n                ASC\n            )\n\n        // Bottom half curve (from right to left, mirrored)\n        VAR _BottomCurve = \n            CONCATENATEX(\n                _WithControlPoints,\n                VAR _MirroredY = _CenterY + (_CenterY - [@Y])\n                VAR _MirroredCY = _CenterY + (_CenterY - [@CY])\n                RETURN\n                    \"S \" &amp; [@CX] &amp; \" \" &amp; _MirroredCY &amp; \", \" &amp; [@X] &amp; \" \" &amp; _MirroredY,\n                \" \",\n                [Value],\n                DESC\n            )\n\n        // Create a single closed path for the violin shape\n        VAR _ViolinPath = \n            _TopCurve &amp; \n            \" \" &amp; _BottomCurve &amp; \n            \" Z\" // Close the path\n\n        // Combined Elements\n        VAR _CombinedElements = \n            DaxLib.SVG.Element.Paths(\n                _ViolinPath, // d\n                DaxLib.SVG.Attr.Shapes(\n                    color,          // fill\n                    0.5,            // fillOpacity\n                    BLANK(),        // fillRule\n                    color,          // stroke\n                    1,              // strokeWidth\n                    BLANK(),        // strokeOpacity\n                    BLANK()         // opacity\n                ),\n                BLANK()             // transforms\n            )\n\n        RETURN\n\n            IF( NOT ISEMPTY( _Data ), _CombinedElements )\n</code></pre>"},{"location":"posts/DAXLibSVG-0-2-0/#defs","title":"Defs","text":"<p>Defs functions allow you to define reusable SVG elements that can be referenced multiple times throughout your visual.</p> Defining Defs <p><code>DaxLib.SVG.Def.*</code> should be wrapped in <code>DaxLib.SVG.Element.Defs()</code>] to add <code>&lt;defs&gt;...&lt;/defs&gt;</code> tags.</p> <pre><code>DaxLib.SVG.Element.Defs(\n        DaxLib.SVG.Def.Circle(...) &amp;\n        DaxLib.SVG.Def.Rect(...)\n)\n// Returns: &lt;defs&gt;...&lt;/defs&gt;\n</code></pre> SVG Performance <p>Using Defs can reduce the string length of the SVG reducing the chance of hitting memory errors in Power BI Minimizing SVG Strings to Avoid Hitting Power BI Memory Limits</p> ClipPathLinearGradientRadialGradient <p></p> <p></p> <p></p> <p>We can define a linear gradient to be applied to a <code>rect</code>.</p> ExampleSyntaxDefinition <pre><code>DaxLib.SVG.SVG(\n    500,\n    100,\n    \"0 0 100 20\",\n    DaxLib.SVG.Element.Defs(\n        DaxLib.SVG.Def.LinearGradient(\n            \"myGradient\",   // id\n            DaxLib.SVG.Def.GradientStop(\n                \"20%\",      // offset\n                DaxLib.SVG.Colour.Theme(\n                    \"Power BI\",\n                    25\n                ),          // colour\n                BLANK()     // opacity\n            ) &amp;\n            DaxLib.SVG.Def.GradientStop( \n                \"80%\",      // offset\n                DaxLib.SVG.Colour.Theme(\n                    \"Power BI\",\n                    26\n                ),           // colour\n                BLANK()     // opacity\n            ),              // stops\n            BLANK(),        // x1\n            BLANK(),        // y1\n            BLANK(),        // x2\n            BLANK()         // y2\n        )                   // contents\n    ) &amp;\n    DaxLib.SVG.Element.Rect(\n        2,                  // x\n        2,                  // y\n        \"80%\",              // width\n        \"80%\",              // height\n        BLANK(),            // rx\n        BLANK(),            // ry\n        DaxLib.SVG.Attr.Shapes(\n            \"url(\"\"\" &amp; \"#myGradient\" &amp; \"\"\")\", // fill\n            BLANK(),        // fillOpacity\n            BLANK(),        // fillRule   \n            BLANK(),        // stroke\n            BLANK(),        // strokeWidth\n            BLANK(),        // strokeOpacity\n            BLANK()         // opacity\n        ),                  // attributes\n        BLANK()             // transforms\n    ),\n    BLANK()                 // sortValue               \n)\n</code></pre> <pre><code>DaxLib.SVG.Def.LinearGradient( defId, stops, x1, y1, x2, y2 )\n</code></pre> Name Type Required Description defId STRING The unique identifier for the gradient stops STRING Concatenated list of one or more <code>DaxLib.SVG.Def.GradientStop</code> elements x1 STRING Start X position y1 STRING Start Y position x2 STRING End X position y2 STRING End Y position <p>STRING <code>&lt;linearGradient&gt;</code> definition</p> <pre><code>function 'DaxLib.SVG.Def.LinearGradient' =\n    (\n        defId: STRING,\n        stops: STRING,\n        x1: STRING,\n        y1: STRING,\n        x2: STRING,\n        y2: STRING\n    ) =&gt;\n\n        \"&lt;linearGradient\" &amp;\n        \" id='\" &amp; defId &amp; \"'\" &amp;\n        IF( NOT ISBLANK( x1 ), \" x1='\" &amp; x1 &amp; \"'\" ) &amp;\n        IF( NOT ISBLANK( y1 ), \" y1='\" &amp; y1 &amp; \"'\" ) &amp;\n        IF( NOT ISBLANK( x2 ), \" x2='\" &amp; x2 &amp; \"'\" ) &amp;\n        IF( NOT ISBLANK( y2 ), \" y2='\" &amp; y2 &amp; \"'\" ) &amp;\n        \"&gt;\" &amp;\n        stops &amp;\n        \"&lt;/linearGradient&gt;\"\n</code></pre>"},{"location":"posts/DAXLibSVG-0-2-0/#attributes","title":"Attributes","text":"<p><code>Attr</code> functions generate attribute strings for <code>fill</code>, <code>stroke</code>, <code>opacity</code>, and text formatting. These wrappers ensure consistent attribute application across your SVG elements.</p> ExampleSyntaxDefinition <pre><code>DaxLib.SVG.Attr.Shapes(\"#FF0000\", 0.8, \"nonzero\", \"#000000\", 2, 1, 1)\n// Returns \"fill='#FF0000' fill-opacity='0.8' fill-rule='nonzero' stroke='#000000' stroke-width='2' stroke-opacity='1' opacity='1' \"\n</code></pre> <pre><code>DaxLib.SVG.Attr.Shapes( fill, fillOpacity, fillRule, stroke, strokeWidth, strokeOpacity, opacity )\n</code></pre> Parameter Type Required Description fill STRING Fill color (e.g., \"#FF0000\", \"red\") fillOpacity DOUBLE Fill opacity value between 0 and 1 fillRule STRING Fill rule (\"nonzero\", \"evenodd\") stroke STRING Stroke color (e.g., \"#000000\", \"black\") strokeWidth INT64 Width of the stroke strokeOpacity DOUBLE Stroke opacity value between 0 and 1 opacity DOUBLE Overall opacity value between 0 and 1 <p>STRING An attribute string that can be used directly in SVG elements</p> <pre><code>function 'DaxLib.SVG.Attr.Shapes' = \n    (\n        fill: STRING,\n        fillOpacity: DOUBLE,\n        fillRule: STRING,\n        stroke: STRING,\n        strokeWidth: INT64,\n        strokeOpacity: DOUBLE,\n        opacity: DOUBLE\n    ) =&gt;\n\n        IF( NOT ISBLANK( fill ),          \"fill='\" &amp; fill &amp; \"' \") &amp;\n        IF( NOT ISBLANK( fillOpacity ),   \"fill-opacity='\" &amp; fillOpacity &amp; \"' \") &amp;\n        IF( NOT ISBLANK( fillRule ),      \"fill-rule='\" &amp; fillRule &amp; \"' \") &amp;\n        IF( NOT ISBLANK( stroke ),        \"stroke='\" &amp; stroke &amp; \"' \") &amp;\n        IF( NOT ISBLANK( strokeWidth ),   \"stroke-width='\" &amp; strokeWidth &amp; \"' \") &amp;\n        IF( NOT ISBLANK( strokeOpacity ), \"stroke-opacity='\" &amp; strokeOpacity &amp; \"' \") &amp;\n        IF( NOT ISBLANK( opacity ),       \"opacity='\" &amp; opacity &amp; \"' \")\n</code></pre>"},{"location":"posts/DAXLibSVG-0-2-0/#transforms","title":"Transforms","text":"<p>Transform functions help you create transformation strings for rotating, scaling, translating, and skewing SVG elements. Like rotating this rect.</p> <p></p> ExampleSyntaxDefinition <pre><code>DaxLib.SVG.Transforms(\"10,20\", \"45\", \"1.5\", \"\", \"\")\n// Returns: \"translate(10,20) rotate(45) scale(1.5)\"\n</code></pre> <pre><code>DaxLib.SVG.Transforms(translate, rotate, scale, skewX, skewY)\n</code></pre> Parameter Type Required Description translate STRING Translation coordinates in the format \"x,y\" rotate STRING Rotation angle in degrees, or \"angle x y\" for rotation around a point scale STRING Scale factor, or \"x,y\" for different scaling in each dimension skewX STRING Horizontal skew angle in degrees skewY STRING Vertical skew angle in degrees <p>STRING A transform attribute value that can be used with the transform attribute of SVG elements.</p> <pre><code>function 'DaxLib.SVG.Transforms' = \n    (\n        translate: STRING,\n        rotate: STRING,\n        scale: STRING,\n        skewX: STRING,\n        skewY: STRING\n    ) =&gt;\n\n        IF(NOT ISBLANK(translate),  \"translate(\" &amp; translate &amp; \") \") &amp;\n        IF(NOT ISBLANK(rotate),     \"rotate(\" &amp; rotate &amp; \") \") &amp;\n        IF(NOT ISBLANK(scale),      \"scale(\" &amp; scale &amp; \") \") &amp;\n        IF(NOT ISBLANK(skewX),      \"skewX(\" &amp; skewX &amp; \") \") &amp;\n        IF(NOT ISBLANK(skewY),      \"skewY(\" &amp; skewY &amp; \") \") \n</code></pre>"},{"location":"posts/DAXLibSVG-0-2-0/#scales","title":"Scales","text":"<p>Scale functions help you map values between different scales - essential for converting data values into SVG coordinates.</p> ExampleSyntaxDefinition <pre><code>DaxLib.SVG.Scale.Normalize(50, 0, 200, 0, 100) \n// Returns 25\n</code></pre> <pre><code>DaxLib.SVG.Scale.Normalize( inputValue, fromMin, fromMax, toMin, toMax )\n</code></pre> Name Type Required Description inputValue NUMERIC VAL The value to map fromMin NUMERIC VAL The minimum value of the original scale fromMax NUMERIC VAL The maximum value of the original scale toMin NUMERIC VAL The minimum value of the new scale toMax NUMERIC VAL The maximum value of the new scale <p>NUMERIC mapped value</p> <pre><code>function 'DaxLib.SVG.Scale.Normalize' =\n    (\n        inputValue: NUMERIC VAL,\n        fromMin: NUMERIC VAL,\n        fromMax: NUMERIC VAL,\n        toMin: NUMERIC VAL,\n        toMax: NUMERIC VAL\n    ) =&gt;\n\n        ( ( inputValue - fromMin ) / ( fromMax - fromMin ) ) * ( toMax - toMin ) + toMin\n</code></pre>"},{"location":"posts/DAXLibSVG-0-2-0/#colors","title":"Colors","text":"<p>Color functions provide theme support and color manipulation utilities, for example we can you <code>DaxLib.SVG.Color.Theme()</code> to return a theme colours.</p> ExampleSyntaxAvailable ThemesDefinition <pre><code>DaxLib.SVG.Color.Theme(\"Power BI\", 1)\n// Returns: \"#118DFF\"\n</code></pre> <pre><code>DaxLib.SVG.Color.Theme(themeName, variant)\n</code></pre> Name Type Required Description themeName STRING Theme name variant INT64 Variant index (1-N, wraps around if exceeds available variants) <p>STRING Hex color code</p> Variant Power BI Modern Corporate Ocean Breeze Sunset Vibes Forest Green Purple Rain Monochrome Vibrant Tech Earth Tones Pastel Dreams Midnight Blue 1 2 3 4 5 6 ... ... ... ... ... ... ... ... ... ... ... ... 41 <pre><code>function 'DaxLib.SVG.Color.Theme' =\n    (\n        themeName: STRING,\n        variant: INT64\n    ) =&gt;\n\n        VAR Themes =\n            DATATABLE(\n            \"ThemeName\", STRING,\n            \"Variant\", INTEGER,\n            \"Color\", STRING,\n            {\n                // Power BI Default\n                {\"Power BI\", 1, \"#118DFF\"},\n                {\"Power BI\", 2, \"#12239E\"},\n                {\"Power BI\", 3, \"#E66C37\"},\n                {\"Power BI\", 4, \"#6B007B\"},\n                {\"Power BI\", 5, \"#E044A7\"},\n                {\"Power BI\", 6, \"#744EC2\"},\n                {\"Power BI\", 7, \"#D9B300\"},\n                {\"Power BI\", 8, \"#D64550\"},\n                {\"Power BI\", 9, \"#197278\"},\n                {\"Power BI\", 10, \"#1AAB40\"},\n                {\"Power BI\", 11, \"#15C6F4\"},\n                {\"Power BI\", 12, \"#4092FF\"},\n                {\"Power BI\", 13, \"#FFA058\"},\n                {\"Power BI\", 14, \"#BE5DC9\"},\n                {\"Power BI\", 15, \"#F472D0\"},\n                {\"Power BI\", 16, \"#B5A1FF\"},\n                {\"Power BI\", 17, \"#C4A200\"},\n                {\"Power BI\", 18, \"#FF8080\"},\n                {\"Power BI\", 19, \"#00DBBC\"},\n                {\"Power BI\", 20, \"#5BD667\"},\n                {\"Power BI\", 21, \"#0091D5\"},\n                {\"Power BI\", 22, \"#4668C5\"},\n                {\"Power BI\", 23, \"#FF6300\"},\n                {\"Power BI\", 24, \"#99008A\"},\n                {\"Power BI\", 25, \"#EC008C\"},\n                {\"Power BI\", 26, \"#533285\"},\n                {\"Power BI\", 27, \"#99700A\"},\n                {\"Power BI\", 28, \"#FF4141\"},\n                {\"Power BI\", 29, \"#1F9A85\"},\n                {\"Power BI\", 30, \"#25891C\"},\n                {\"Power BI\", 31, \"#0057A2\"},\n                {\"Power BI\", 32, \"#002050\"},\n                {\"Power BI\", 33, \"#C94F0F\"},\n                {\"Power BI\", 34, \"#450F54\"},\n                {\"Power BI\", 35, \"#B60064\"},\n                {\"Power BI\", 36, \"#34124F\"},\n                {\"Power BI\", 37, \"#6A5A29\"},\n                {\"Power BI\", 38, \"#1AAB40\"},\n                {\"Power BI\", 39, \"#BA141A\"},\n                {\"Power BI\", 40, \"#0C3D37\"},\n                {\"Power BI\", 41, \"#0B511F\"},\n\n                // Modern Corporate - Professional blues and grays\n                {\"Modern Corporate\", 1, \"#2E3440\"},\n                {\"Modern Corporate\", 2, \"#3B4252\"},\n                {\"Modern Corporate\", 3, \"#434C5E\"},\n                {\"Modern Corporate\", 4, \"#4C566A\"},\n                {\"Modern Corporate\", 5, \"#5E81AC\"},\n                {\"Modern Corporate\", 6, \"#81A1C1\"},\n\n                // Ocean Breeze - Cool blues and teals\n                {\"Ocean Breeze\", 1, \"#0077BE\"},\n                {\"Ocean Breeze\", 2, \"#00A8CC\"},\n                {\"Ocean Breeze\", 3, \"#40E0D0\"},\n                {\"Ocean Breeze\", 4, \"#87CEEB\"},\n                {\"Ocean Breeze\", 5, \"#B0E0E6\"},\n                {\"Ocean Breeze\", 6, \"#E0F6FF\"},\n\n                // Sunset Vibes - Warm oranges and reds\n                {\"Sunset Vibes\", 1, \"#FF6B35\"},\n                {\"Sunset Vibes\", 2, \"#F7931E\"},\n                {\"Sunset Vibes\", 3, \"#FFD23F\"},\n                {\"Sunset Vibes\", 4, \"#EE4B2B\"},\n                {\"Sunset Vibes\", 5, \"#C04000\"},\n                {\"Sunset Vibes\", 6, \"#FFCBA4\"},\n\n                // Forest Green - Natural greens\n                {\"Forest Green\", 1, \"#355E3B\"},\n                {\"Forest Green\", 2, \"#228B22\"},\n                {\"Forest Green\", 3, \"#32CD32\"},\n                {\"Forest Green\", 4, \"#90EE90\"},\n                {\"Forest Green\", 5, \"#98FB98\"},\n                {\"Forest Green\", 6, \"#F0FFF0\"},\n\n                // Purple Rain - Rich purples\n                {\"Purple Rain\", 1, \"#301934\"},\n                {\"Purple Rain\", 2, \"#663399\"},\n                {\"Purple Rain\", 3, \"#9966CC\"},\n                {\"Purple Rain\", 4, \"#BA55D3\"},\n                {\"Purple Rain\", 5, \"#DDA0DD\"},\n                {\"Purple Rain\", 6, \"#E6E6FA\"},\n\n                // Monochrome - Sophisticated grays\n                {\"Monochrome\", 1, \"#1C1C1C\"},\n                {\"Monochrome\", 2, \"#333333\"},\n                {\"Monochrome\", 3, \"#666666\"},\n                {\"Monochrome\", 4, \"#999999\"},\n                {\"Monochrome\", 5, \"#CCCCCC\"},\n                {\"Monochrome\", 6, \"#F5F5F5\"},\n\n                // Vibrant Tech - Bold and energetic\n                {\"Vibrant Tech\", 1, \"#FF0080\"},\n                {\"Vibrant Tech\", 2, \"#00FFFF\"},\n                {\"Vibrant Tech\", 3, \"#FFFF00\"},\n                {\"Vibrant Tech\", 4, \"#FF8000\"},\n                {\"Vibrant Tech\", 5, \"#8000FF\"},\n                {\"Vibrant Tech\", 6, \"#00FF80\"},\n\n                // Earth Tones - Natural browns and beiges\n                {\"Earth Tones\", 1, \"#8B4513\"},\n                {\"Earth Tones\", 2, \"#A0522D\"},\n                {\"Earth Tones\", 3, \"#CD853F\"},\n                {\"Earth Tones\", 4, \"#DEB887\"},\n                {\"Earth Tones\", 5, \"#F4A460\"},\n                {\"Earth Tones\", 6, \"#FFF8DC\"},\n\n                // Pastel Dreams - Soft and gentle\n                {\"Pastel Dreams\", 1, \"#FFB3BA\"},\n                {\"Pastel Dreams\", 2, \"#FFDFBA\"},\n                {\"Pastel Dreams\", 3, \"#FFFFBA\"},\n                {\"Pastel Dreams\", 4, \"#BAFFC9\"},\n                {\"Pastel Dreams\", 5, \"#BAE1FF\"},\n                // {\"Pastel Dreams\", 6, \"#E1BAFF\"},\n\n                // Midnight Blue - Deep blues and navy\n                {\"Midnight Blue\", 1, \"#191970\"},\n                {\"Midnight Blue\", 2, \"#000080\"},\n                {\"Midnight Blue\", 3, \"#0000CD\"},\n                {\"Midnight Blue\", 4, \"#4169E1\"},\n                {\"Midnight Blue\", 5, \"#6495ED\"},\n                {\"Midnight Blue\", 6, \"#B0C4DE\"}\n            }\n        )\n\n        VAR ThemeColors = FILTER(Themes, [ThemeName] = themeName)\n        VAR MaxVariant = MAXX(ThemeColors, [Variant])\n        VAR AdjustedVariant = IF(\n            MaxVariant &gt; 0,\n            MOD( variant - 1, MaxVariant ) + 1,\n            variant\n        )\n        VAR SelectedColor =\n            MAXX(\n                FILTER( ThemeColors, [Variant] = AdjustedVariant),\n                [Color]\n            )\n\n        RETURN SelectedColor\n</code></pre>"},{"location":"posts/DAXLibSVG-0-2-0/#contribute","title":"Contribute","text":"<p>This is a good start at defining the basic structure of the library, but I think a missing category of functions is <code>daxlib.visual.*</code> which uses the existing function, but with a simpler interface, to create even more elaborate SVGs. Look out for upcoming documentation on how to contribute to DAX Lib official libraries.</p>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2025/#2025","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2024/#2024","title":"2024","text":""},{"location":"category/dax/","title":"DAX","text":""},{"location":"category/dax/#dax","title":"DAX","text":""},{"location":"category/user-data-functions/","title":"User Data Functions","text":""},{"location":"category/user-data-functions/#user-data-functions","title":"User Data Functions","text":""},{"location":"category/translytical-task-flow/","title":"Translytical Task Flow","text":""},{"location":"category/translytical-task-flow/#translytical-task-flow","title":"Translytical Task Flow","text":""},{"location":"category/graphs/","title":"Graphs","text":""},{"location":"category/graphs/#graphs","title":"Graphs","text":""},{"location":"category/cicd/","title":"CICD","text":""},{"location":"category/cicd/#cicd","title":"CICD","text":""},{"location":"category/real-time-intelligence/","title":"Real-Time Intelligence","text":""},{"location":"category/real-time-intelligence/#real-time-intelligence","title":"Real-Time Intelligence","text":""},{"location":"category/administration/","title":"Administration","text":""},{"location":"category/administration/#administration","title":"Administration","text":""},{"location":"category/ssas-tabular/","title":"SSAS Tabular","text":""},{"location":"category/ssas-tabular/#ssas-tabular","title":"SSAS Tabular","text":""},{"location":"category/svg/","title":"SVG","text":""},{"location":"category/svg/#svg","title":"SVG","text":""},{"location":"category/deneb/","title":"Deneb","text":""},{"location":"category/deneb/#deneb","title":"Deneb","text":""},{"location":"category/vega/","title":"Vega","text":""},{"location":"category/vega/#vega","title":"Vega","text":""},{"location":"category/pbip/","title":"PBIP","text":""},{"location":"category/pbip/#pbip","title":"PBIP","text":""},{"location":"category/lakehouse/","title":"Lakehouse","text":""},{"location":"category/lakehouse/#lakehouse","title":"Lakehouse","text":""},{"location":"category/vs-code/","title":"VS Code","text":""},{"location":"category/vs-code/#vs-code","title":"VS Code","text":""},{"location":"category/data-modelling/","title":"Data Modelling","text":""},{"location":"category/data-modelling/#data-modelling","title":"Data Modelling","text":""},{"location":"category/syntax-highlighting/","title":"Syntax Highlighting","text":""},{"location":"category/syntax-highlighting/#syntax-highlighting","title":"Syntax Highlighting","text":""},{"location":"page/2/","title":"Index","text":""},{"location":"page/2/#index","title":"Index","text":""},{"location":"page/3/","title":"Index","text":""},{"location":"page/3/#index","title":"Index","text":""},{"location":"page/4/","title":"Index","text":""},{"location":"page/4/#index","title":"Index","text":""},{"location":"archive/2025/page/2/","title":"2025","text":""},{"location":"archive/2025/page/2/#2025","title":"2025","text":""},{"location":"archive/2025/page/3/","title":"2025","text":""},{"location":"archive/2025/page/3/#2025","title":"2025","text":""},{"location":"archive/2024/page/2/","title":"2024","text":""},{"location":"archive/2024/page/2/#2024","title":"2024","text":""}]}